{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating your first agent\n",
    "\n",
    "In this tutorial, we'll build upon the web search agent from [my_first_agent.ipynb](https://github.com/mozilla-ai/any-agent/blob/main/docs/cookbook/your_first_agent.ipynb) and demonstrate how to evaluate its performance using any-agent's evaluation framework. We'll explore different evaluation methods including custom code evaluation, an LLM-based judge, and an agent-based judge. \n",
    "\n",
    "Note: Since we are building on the previous notebook, we encourage you to run that one first to read through details and choices available while building the agent before evaluating it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "any-agent uses the python asyncio module to support async functionality. When running in Jupyter notebooks, this means we need to enable the use of nested event loops. We'll install any-agent and enable this below using nest_asyncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Web Search Agent\n",
    "\n",
    "First, let's recreate the web search agent from [the previous tutorial](https://github.com/mozilla-ai/any-agent/blob/main/docs/cookbook/your_first_agent.ipynb) so we have something to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"MISTRAL_API_KEY\" not in os.environ:\n",
    "    print(\"MISTRAL_API_KEY not found in environment!\")\n",
    "    api_key = getpass(\"Please enter your MISTRAL_API_KEY: \")\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = api_key\n",
    "    print(\"MISTRAL_API_KEY set for this session!\")\n",
    "else:\n",
    "    print(\"MISTRAL_API_KEY found in environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_agent import AgentConfig, AnyAgent\n",
    "from any_agent.tools import search_tavily, visit_webpage\n",
    "\n",
    "agent = AnyAgent.create(\n",
    "    \"tinyagent\",  # See all options in https://mozilla-ai.github.io/any-agent/\n",
    "    AgentConfig(\n",
    "        model_id=\"mistral:mistral-small-latest\", tools=[search_tavily, visit_webpage]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Agent to Generate a Trace\n",
    "\n",
    "Now let's run our agent on a test query to generate a trace that we can evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"What film won a Goya Award for best film in 2024?\n",
    "Please provide the name of the film, the genre, a very brief\n",
    "description of the film - and rotten tomatoes popcornmeter\n",
    "score.\"\"\"\n",
    "\n",
    "agent_trace = agent.run(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Agent Results\n",
    "\n",
    "Let's first see what our agent produced:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_trace.final_output)  # Final answer\n",
    "print(f\"Duration: {agent_trace.duration.total_seconds():.2f} seconds\")\n",
    "print(f\"Usage: {agent_trace.tokens.total_tokens:,}\")\n",
    "print(f\"Cost (USD): {agent_trace.cost.total_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Custom Code Evaluation\n",
    "\n",
    "Before using LLM-based evaluation, let's start with deterministic custom code evaluation. This is often more efficient, reliable, and cost-effective for specific criteria.\n",
    "\n",
    "Some criteria are clearly quantitative: a result exists or it doesn't, it has a measurable length, the number of steps can be counted and a tool was either called or wasn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_agent.tracing.agent_trace import AgentTrace\n",
    "from any_agent.tracing.attributes import GenAI\n",
    "\n",
    "\n",
    "def check_tool_usage(trace: AgentTrace, required_tool: str) -> bool:\n",
    "    \"\"\"Check if a specific tool was used in the trace.\"\"\"\n",
    "    return any(\n",
    "        span.attributes[GenAI.TOOL_NAME] == required_tool\n",
    "        for span in trace.spans\n",
    "        if span.is_tool_execution()\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_web_search_efficiency(trace: AgentTrace) -> dict:\n",
    "    \"\"\"Custom evaluation function for web search agent efficiency criteria.\"\"\"\n",
    "    # Direct access to trace properties\n",
    "    token_count = trace.tokens.total_tokens\n",
    "    step_count = len(trace.spans)\n",
    "    final_output = trace.final_output\n",
    "    duration = trace.duration.total_seconds()\n",
    "    # Check if web search tools were used\n",
    "    used_search = check_tool_usage(trace, \"search_tavily\")\n",
    "    used_visit = check_tool_usage(trace, \"visit_webpage\")\n",
    "    # Apply quantitative criteria\n",
    "    results = {\n",
    "        \"token_efficient\": token_count\n",
    "        < 20000,  # Magic number alert: adjust to what you consider reasonable for your budget\n",
    "        \"step_efficient\": step_count\n",
    "        <= 10,  # A high number of steps would point at problems, but this is also a debatable limit\n",
    "        \"has_output\": final_output is not None and len(str(final_output)) > 5,\n",
    "        \"short_output\": len(str(final_output)) < 10 if final_output else 0,\n",
    "        \"used_web_search\": used_search,\n",
    "        \"used_webpage_visit\": used_visit,\n",
    "        \"reasonable_duration\": duration < 60,\n",
    "    }\n",
    "    # Choose the quantitative criteria you care most about\n",
    "    results[\"passed\"] = all(\n",
    "        [\n",
    "            results[\"token_efficient\"],\n",
    "            results[\"step_efficient\"],\n",
    "            results[\"has_output\"],\n",
    "            results[\"used_web_search\"],\n",
    "            results[\"short_output\"],\n",
    "        ]\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = evaluate_web_search_efficiency(agent_trace)\n",
    "print(\"Custom Code Evaluation Results:\")\n",
    "for key, value in evaluation.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: LLM Judge Evaluation\n",
    "\n",
    "The method above is already useful and can assess quantitative results (how long or how costly answers were, whether a specific tool was present). Programmatic evaluations are less costly, more deterministic, but also less flexible. They can see that a tool was used: but was the result well understood? Was the content actually used to extract an answer? This is a qualitative assessment.\n",
    "\n",
    "For such criteria, you can use the `LlmJudge`. This is great for evaluating response quality, helpfulness, and other subjective criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Good to know: different models\n",
    "\n",
    "Notice we use a different LLM as a judge to the one we used for the original agent, as LLM judges are known to have a [bias towards their own results](https://neurips.cc/virtual/2024/poster/96672)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_agent.evaluation import LlmJudge\n",
    "\n",
    "# Create an LLM judge\n",
    "judge = LlmJudge(model_id=\"gpt-4.1-mini\")\n",
    "\n",
    "# Define evaluation questions - notice the last one is not like the others\n",
    "evaluation_questions = [\n",
    "    \"Did the agent provide a clear and concise answer?\",\n",
    "    \"Did the agent correctly identify the genre?\",\n",
    "    \"Did the agent provide a brief description (under 10 words) of the film?\",\n",
    "]\n",
    "\n",
    "# Run evaluations\n",
    "print(\"LLM Judge Evaluation Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "for i, question in enumerate(evaluation_questions, 1):\n",
    "    result = judge.run(context=str(agent_trace.spans_to_messages()), question=question)\n",
    "    results.append(result)\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"  Passed: {result.passed}\")\n",
    "    print(f\"  Reasoning: {result.reasoning}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Summary\n",
    "passed_count = sum(1 for r in results if r.passed)\n",
    "print(f\"\\nOverall: {passed_count}/{len(results)} criteria passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Good to know: fuzzy criteria \n",
    "\n",
    "Notice Question 3: if you run the evaluation multiple times, it won't pass or fail consistently, since the LLM judge may interpret that only the description should be under 10 words, not necessarily the whole Agent's answer. In the programmatic method, there is nothing to interpret: we check that the final output was under 10 words. \n",
    "\n",
    "This showcases the main downside with using an LLM judge: as with humans, criteria can be misunderstood. \n",
    "\n",
    "On the other hand, using a programmatic approach to assess clarity, for example, would have been rather complex without an LLM judge.\n",
    "\n",
    "A take-home message here is to use custom code when criteria can be counted or measured, and think of using an LLMJudge when your criteria are qualitative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Agent Judge Evaluation\n",
    "\n",
    "For more complex evaluations that require inspecting specific aspects of the trace, we can use the `AgentJudge`. Notice the AgentJudge can:\n",
    "\n",
    "* call built-in tools to get straight to relevant parts of the traces (e.g. final output),\n",
    "* call additional tools that the original agent did not have. For example, you will see below how we give it a second search tool so it can do its own research to check if the original agent's answer was correct.\n",
    "\n",
    "As with the LLMJudge, we choose a different model to the one enabling the original judge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, if you do not have a [Tavily API key](https://www.tavily.com/), you can import and use `search_web` (Duck Duck Go Search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_agent.evaluation import AgentJudge\n",
    "from any_agent.tools import search_tavily\n",
    "\n",
    "# Create an agent judge\n",
    "agent_judge = AgentJudge(model_id=\"gpt-4.1-mini\")\n",
    "\n",
    "# Define a complex evaluation question that requires trace inspection\n",
    "complex_question = \"\"\"\n",
    "Evaluate the agent's performance on this web search task by verifying\n",
    "whether the agent correctly used web search to find relevant information\n",
    "for the winner film of the Goya Award in 2024 and its Rotten Tomatoes rating?\n",
    "\n",
    "Use the available tools to inspect the trace and, specially, make sure\n",
    "the agent visited Rotten Tomatoes and checked the audience score, not\n",
    "the critics score.\n",
    "\"\"\"\n",
    "\n",
    "# Run the agent judge evaluation\n",
    "eval_trace = agent_judge.run(\n",
    "    trace=agent_trace,\n",
    "    question=complex_question,\n",
    "    additional_tools=[\n",
    "        search_tavily\n",
    "    ],  # Give the judge access to web search for verification\n",
    ")\n",
    "\n",
    "# Get the evaluation result\n",
    "result = eval_trace.final_output\n",
    "print(\"Agent Judge Evaluation Result:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Passed: {result.passed}\")\n",
    "print(f\"Reasoning: {result.reasoning}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how giving the judge tools enables it to check **independently** whether the original agent successfully did its job."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "any-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
