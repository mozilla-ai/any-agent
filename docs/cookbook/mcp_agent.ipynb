{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an agent with MCP\n",
    "\n",
    "[![MCP Agent](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mozilla-ai/any-agent/blob/main/docs/cookbook/mcp_agent.ipynb) \n",
    "\n",
    "The [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol) (MCP) introduced by Anthropic has proven to be a popular method for providing an AI agent with access to a variety of tools. [This Huggingface blog post ](https://huggingface.co/blog/Kseniase/mcp) has a nice explanation of MCP.  In this tutorial, we'll build an agent that is able to leverage MCP server provided tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "any-agent uses the python asyncio module to support async functionality. When running in Jupyter notebooks, this means we need to enable the use of nested event loops. We'll install any-agent and enable this below using nest_asyncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'any-agent[all]'\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Agent\n",
    "\n",
    "Now it's time to configure the agent! At this stage you have a few choices:\n",
    "\n",
    "### Pick the framework\n",
    "\n",
    "We support a variety of underlying agent frameworks (OpenAI, Smolagents, Langchain, etc), which all have their own particular agentic AI implementations. For this tutorial's simple use case, any of the frameworks should work just fine, but any-agent makes it easy to try out a different framework later, if we so choose. For this example, we will use the \"openai\" agent framework.\n",
    "\n",
    "### Pick an LLM\n",
    "\n",
    "Regardless of which agent framework you choose, each framework supports LiteLLM, which is a proxy that allows us to use whichever LLM inside the framework, hosted on by any provider. For example, we could use a local model via llama.cpp or llamafile, a google hosted gemini model, or a AWS bedrock hosted Llama model. For this example, let's use OpenAI's gpt-4.1-nano.\n",
    "\n",
    "### Pick which tools to use\n",
    "\n",
    " In this example, we'll add a few MCP servers that we host locally, which means we'll use a Stdio MCP server. If an MCP Server is already running and hosted elsewhere, you can use an SSE connection to access it. You can browse some of the officially supported MCP servers [here](https://github.com/modelcontextprotocol/servers/tree/main?tab=readme-ov-file).\n",
    "\n",
    " Lets give use two MCP servers: \n",
    " \n",
    " * [Time](https://github.com/modelcontextprotocol/servers/tree/main/src/time): so the agent can know what time/day it is.\n",
    " * [Airbnb](https://github.com/openbnb-org/mcp-server-airbnb): so the agent can browse airbnb listings\n",
    "\n",
    " I will also add a custom send_message tool, that way it can ask us additional questions before getting its final answer!\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    print(\"OPENAI_API_KEY not found in environment!\")\n",
    "    api_key = getpass(\"Please enter your OPENAI_API_KEY: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    print(\"OPENAI_API_KEY set for this session!\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY found in environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_agent import AgentConfig, AnyAgent\n",
    "from any_agent.config import MCPStdio\n",
    "\n",
    "time_tool = MCPStdio(\n",
    "    command=\"uvx\",\n",
    "    args=[\"mcp-server-time\", \"--local-timezone=America/New_York\"],\n",
    "    tools=[\n",
    "        \"get_current_time\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "airbnb_tool = MCPStdio(\n",
    "    command=\"npx\", args=[\"-y\", \"@openbnb/mcp-server-airbnb\", \"--ignore-robots-txt\"]\n",
    ")\n",
    "\n",
    "\n",
    "def send_message(message: str) -> str:\n",
    "    \"\"\"Display a message to the user and wait for their response.\n",
    "\n",
    "    Args:\n",
    "        message: str\n",
    "            The message to be displayed to the user.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the user.\n",
    "\n",
    "    \"\"\"\n",
    "    return input(message + \" \")\n",
    "\n",
    "\n",
    "agent = AnyAgent.create(\n",
    "    \"openai\",  # See all options in https://mozilla-ai.github.io/any-agent/\n",
    "    AgentConfig(model_id=\"gpt-4o\", tools=[airbnb_tool, time_tool, send_message]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Agent\n",
    "\n",
    "Now we've configured our agent, so it's time to run it! Let's give it a simple task: find 5 trending new TV shows that were released recently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I am looking to book an airbnb next weekend near Ohiopyle, PA. Can you help me plan this? Ask me some questions and lets figure this out together.\n",
    "\"\"\"\n",
    "\n",
    "agent_trace = agent.run(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `agent.run` method returns an AgentTrace object, which has a few convenient attributes for displaying some interesting information about the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_trace.final_output)  # Final answer\n",
    "print(f\"Duration: {agent_trace.duration.total_seconds():.2f} seconds\")\n",
    "print(f\"Usage: {agent_trace.usage}\")\n",
    "print(f\"Cost (USD): {agent_trace.cost.total_cost:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
