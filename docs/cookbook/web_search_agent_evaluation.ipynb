{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating your first agent\n",
    "\n",
    "In this tutorial, we'll build upon the web search agent from [my_first_agent.ipynb](https://github.com/mozilla-ai/any-agent/blob/main/docs/cookbook/your_first_agent.ipynb) and demonstrate how to evaluate its performance using any-agent's evaluation framework. We'll explore different evaluation methods including custom code evaluation, an LLM-based judge, and an agent-based judge. \n",
    "\n",
    "Note: Since we are building on the previous notebook, we encourage you to run that one first to read through details and choices available while building the agent before evaluating it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "any-agent uses the python asyncio module to support async functionality. When running in Jupyter notebooks, this means we need to enable the use of nested event loops. We'll install any-agent and enable this below using nest_asyncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'any-agent'\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Web Search Agent\n",
    "\n",
    "First, let's recreate the web search agent from [the previous tutorial](https://github.com/mozilla-ai/any-agent/blob/main/docs/cookbook/your_first_agent.ipynb) so we have something to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"MISTRAL_API_KEY\" not in os.environ:\n",
    "    print(\"MISTRAL_API_KEY not found in environment!\")\n",
    "    api_key = getpass(\"Please enter your MISTRAL_API_KEY: \")\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = api_key\n",
    "    print(\"MISTRAL_API_KEY set for this session!\")\n",
    "else:\n",
    "    print(\"MISTRAL_API_KEY found in environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_agent import AgentConfig, AnyAgent\n",
    "from any_agent.tools import search_web, visit_webpage\n",
    "\n",
    "agent = AnyAgent.create(\n",
    "    \"tinyagent\",  # See all options in https://mozilla-ai.github.io/any-agent/\n",
    "    AgentConfig(\n",
    "        model_id=\"mistral/mistral-small-latest\", tools=[search_web, visit_webpage]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Agent to Generate a Trace\n",
    "\n",
    "Now let's run our agent on a test query to generate a trace that we can evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What film won a Goya Award for best film in 2024? Please search for the most relevant page, visit it and provide the name of the film, the genre, and a brief description of the film in under 10 words.\"\n",
    "\n",
    "agent_trace = agent.run(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Agent Results\n",
    "\n",
    "Let's first see what our agent produced:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_trace.final_output)  # Final answer\n",
    "print(f\"Duration: {agent_trace.duration.total_seconds():.2f} seconds\")\n",
    "print(f\"Usage: {agent_trace.tokens.total_tokens:,}\")\n",
    "print(f\"Cost (USD): {agent_trace.cost.total_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Custom Code Evaluation\n",
    "\n",
    "Before using LLM-based evaluation, let's start with deterministic custom code evaluation. This is often more efficient, reliable, and cost-effective for specific criteria.\n",
    "\n",
    "Some criteria are clearly quantitative: a result exists or it doesn't, it has a measurable length, the number of steps can be counted and a tool was either called or wasn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_agent.tracing.agent_trace import AgentTrace\n",
    "\n",
    "\n",
    "def evaluate_web_search_efficiency(trace: AgentTrace) -> dict:\n",
    "    \"\"\"Custom evaluation function for web search agent efficiency criteria.\"\"\"\n",
    "    # Direct access to trace properties\n",
    "    token_count = trace.tokens.total_tokens\n",
    "    step_count = len(trace.spans)\n",
    "    final_output = trace.final_output\n",
    "    duration = trace.duration.total_seconds()\n",
    "    # Check if web search tools were used\n",
    "    messages = trace.spans_to_messages()\n",
    "    used_search = any(\n",
    "        message.role == \"assistant\" and \"search_web\" in str(message.content)\n",
    "        for message in messages\n",
    "    )\n",
    "    used_visit = any(\n",
    "        message.role == \"assistant\" and \"visit_webpage\" in str(message.content)\n",
    "        for message in messages\n",
    "    )\n",
    "    # Apply quantitative criteria\n",
    "    results = {\n",
    "        \"token_efficient\": token_count\n",
    "        < 20000,  # Magic number alert: adjust to what you consider reasonable for your budget\n",
    "        \"step_efficient\": step_count\n",
    "        <= 10,  # A high number of steps would point at problems, but this is also a debatable limit\n",
    "        \"has_output\": final_output is not None and len(str(final_output)) > 5,\n",
    "        \"short_output\": len(str(final_output)) < 10 if final_output else 0,\n",
    "        \"used_web_search\": used_search,\n",
    "        \"used_webpage_visit\": used_visit,\n",
    "        \"reasonable_duration\": duration < 60,\n",
    "    }\n",
    "    # Choose the quantitative criteria you care most about\n",
    "    results[\"passed\"] = all(\n",
    "        [\n",
    "            results[\"token_efficient\"],\n",
    "            results[\"step_efficient\"],\n",
    "            results[\"has_output\"],\n",
    "            results[\"used_web_search\"],\n",
    "            results[\"short_output\"],\n",
    "        ]\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = evaluate_web_search_efficiency(agent_trace)\n",
    "print(\"Custom Code Evaluation Results:\")\n",
    "for key, value in evaluation.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: LLM Judge Evaluation\n",
    "\n",
    "The method above is already useful and can assess quantitative results (how long or how costly answers were, whether a specific tool was present). Programmatic evaluations are less costly, more deterministic, but also less flexible.\n",
    "For more qualitative assessments, you can also use the `LlmJudge`. This is great for evaluating response quality, helpfulness, and other subjective criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Good to know: different models\n",
    "\n",
    "Notice we use a different LLM as a judge to the one we used for the original agent, as LLM judges are known to have a [bias towards their own results](https://neurips.cc/virtual/2024/poster/96672)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_agent.evaluation import LlmJudge\n",
    "\n",
    "# Create an LLM judge\n",
    "judge = LlmJudge(model_id=\"gpt-4.1-mini\")\n",
    "\n",
    "# Define evaluation questions\n",
    "evaluation_questions = [\n",
    "    \"Did the agent provide a clear and concise answer?\",\n",
    "    \"Did the agent correctly identify the genre?\",\n",
    "    \"Did the agent provide a brief description (under 10 words) of the film?\",\n",
    "    \"Did the agent use web search tools to gather current information?\",\n",
    "    \"Was the agent's whole output, structured or not, metadata or no, everything added together, shorter than 10 words?\",\n",
    "]\n",
    "\n",
    "# Run evaluations\n",
    "print(\"LLM Judge Evaluation Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "for i, question in enumerate(evaluation_questions, 1):\n",
    "    result = judge.run(context=str(agent_trace.spans_to_messages()), question=question)\n",
    "    results.append(result)\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"  Passed: {result.passed}\")\n",
    "    print(f\"  Reasoning: {result.reasoning}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Summary\n",
    "passed_count = sum(1 for r in results if r.passed)\n",
    "print(f\"\\nOverall: {passed_count}/{len(results)} criteria passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Good to know: fuzzy criteria \n",
    "\n",
    "Notice Question 3: if you run the evaluation multiple times, it won't pass or fail consistently, since the LLMJudge may interpret that only the description should be under 10 words, not necessarily the whole Agent's answer. In the programmatic method, there is nothing to interpret: we check that the final output was under 10 words. \n",
    "\n",
    "For comparison, look at Question 5, where we are even more specific in what our criterion is for the Agent to pass. Results (pass or fail) should be much more consistent in this case.\n",
    "\n",
    "This showcases the main downside with using an LLMJudge: as with humans, criteria can be misunderstood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Agent Judge Evaluation\n",
    "\n",
    "For more complex evaluations that require inspecting specific aspects of the trace, we can use the `AgentJudge`. Notice the AgentJudge can:\n",
    "\n",
    "* call built-in tools to get straight to relevant parts of the traces (e.g. final output),\n",
    "* call additional tools that the original agent did not have. For example, you will see below how we give it a second search tool so it can do its own research to check if the original agent's answer was correct.\n",
    "\n",
    "As with the LLMJudge, we choose a different model to the one enabling the original judge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, if you have a [Tavily API key](https://www.tavily.com/), you can import and use `search_tavily` as an alternative to `search_web`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_agent.evaluation import AgentJudge\n",
    "from any_agent.tools import search_web\n",
    "\n",
    "# Create an agent judge\n",
    "agent_judge = AgentJudge(model_id=\"gpt-4.1-mini\")\n",
    "\n",
    "# Define a complex evaluation question that requires trace inspection\n",
    "complex_question = \"\"\"\n",
    "Evaluate the agent's performance on this web search task by considering:\n",
    "1. Did the agent use the search_web tool to find relevant information?\n",
    "2. Did the agent visit a webpage to get detailed content?\n",
    "3. Did the final answer contain both the film's original name and its genre?\n",
    "4. Was the full output of the agent, not just a specific section or answer, but the whole text it produced as final output, within the requested constraints (under 10 words)?\n",
    "5. Did the agent demonstrate good web search practices?\n",
    "\n",
    "Use the available tools to inspect the trace and provide a comprehensive evaluation, specially for the final output. Additionally, use the search tool to verify the agent's answer to the original question: 'who won the Goya Award for best film in 2024'.\n",
    "\"\"\"\n",
    "\n",
    "# Run the agent judge evaluation\n",
    "eval_trace = agent_judge.run(\n",
    "    trace=agent_trace,\n",
    "    question=complex_question,\n",
    "    additional_tools=[\n",
    "        search_web\n",
    "    ],  # Give the judge access to web search for verification\n",
    ")\n",
    "\n",
    "# Get the evaluation result\n",
    "result = eval_trace.final_output\n",
    "print(\"Agent Judge Evaluation Result:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Passed: {result.passed}\")\n",
    "print(f\"Reasoning: {result.reasoning}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here again how hard it is to make the agent understand the length requirements. Do play around with them and notice how the assessment changes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
