{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully offline Agent!\n",
    "\n",
    "_(no Google Colab for this one since it is meant to be run entirely locally)_\n",
    "\n",
    "This tutorial will guide on how to run agent, fully locally / offline i.e., running with a local LLM and a couple of local tools (Callable Python functions), so no data will leave your machine!\n",
    "\n",
    "This can be especially useful for privacy-sensitive applications or when you want to avoid any cloud dependencies.\n",
    "\n",
    "In this example, we will showcase how to let an agent read and write in your local filesystem! Specifically,\n",
    "we will give read-access to the agent to some files in our codebase and ask it to create a short report of potential vulnerabilities or bugs in the codebase, and write it to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "any-agent uses the python asyncio module to support async functionality. When running in Jupyter notebooks,\n",
    " this means we need to enable the use of nested event loops. We'll install any-agent and enable this below using nest_asyncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T07:21:04.254798Z",
     "start_time": "2025-07-29T07:21:02.421700Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install 'any-agent[smolagents]' --quiet\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your own LLM locally\n",
    "\n",
    "Regardless of which agent framework you choose in any-agent, all of them support LiteLLM, which is a proxy that\n",
    "allows us to use whichever LLM inside the framework, hosted on by any provider. For example,\n",
    "we could use a local model via llama.cpp or [llamafile](https://github.com/Mozilla-Ocho/llamafile), a\n",
    "google hosted gemini model, or a AWS bedrock hosted Llama model. For this example,\n",
    "we will use [Ollama](https://ollama.com/) to run our LLM locally!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama setup\n",
    "\n",
    "First, install Ollama by following their instructions: https://ollama.com/download\n",
    "\n",
    "### Picking an LLM\n",
    "\n",
    "Pick a model that you can run locally based on your hardware and download it from your terminal. For example:\n",
    "\n",
    "16-24GB RAM -> `granite3.3:8b`  or  `gemma3:12b` with ~10â€“20k context length\n",
    "\n",
    "24+GB RAM -> `mistral-small3.2:24b` or `devstral:24b` with ~20k+ context length\n",
    "\n",
    "**_NOTE:_** Smaller models have shown to be more inconsistent and less capable, especially as task complexity increases. If you have the hardware, we highly recommend using the larger models like `mistral-small3.2:24b` for better performance.\n",
    "\n",
    "### Serving the model with the appropriate context length\n",
    "\n",
    "By default, Ollama forces a context length of 8192 tokens to all models, which is not enough for our agent to work properly.\n",
    "We can simply pass to the AgentConfig of `any-agent` as a model argument (`num_ctx` is ollama-specific) our desired value of context length like so:\n",
    "```\n",
    "    AgentConfig(\n",
    "        model_id=\"ollama/granite3.3:8b\",\n",
    "        instructions=\"You must use the available tools to solve the task.\",\n",
    "        tools=[mcp_filesystem, show_plan],\n",
    "        model_args={\"num_ctx\": 14000},\n",
    "```\n",
    "\n",
    "References: [AgentConfig](https://mozilla-ai.github.io/any-agent/api/config/), [num_ctx](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values)\n",
    "\n",
    "All four of the models above have a max context length of 128k tokens, but if you have limited RAM if you\n",
    "set it to 128k it might cause you memory issues. For this example, we will set it to 14.000 tokens and provide a relatively small codebase.\n",
    "\n",
    "### Load the model\n",
    "\n",
    "Firstly, download the Ollama model locally:\n",
    "\n",
    "```\n",
    "ollama pull granite3.3:8b\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Agent and the Tools\n",
    "\n",
    "Then, let's setup which files the agent should access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "codebase_directory = \"../../demo/app.py\"\n",
    "abs_path = str(Path(codebase_directory).resolve())\n",
    "print(f\"File to scan: {abs_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick which tools to use\n",
    "\n",
    "Since we want our agent to work fully locally/offline, we will not add any tools that require communication with remote\n",
    "servers. In this example we are using python callable functions as tools, but we could also have used MCP servers that run fully locally (e.g. [mcp/filesystem](https://hub.docker.com/r/mcp/filesystem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Read a file from the file path given and return its content.\n",
    "\n",
    "    Args:\n",
    "        file_path: The absolute, local path to search for the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "    except Exception as e:\n",
    "        content = f\"Error reading file: {file_path} \\n\\n {e}\"\n",
    "    return content\n",
    "\n",
    "\n",
    "def write_file(file_path_to_write: str, content: str) -> str:\n",
    "    \"\"\"\n",
    "    Write content to a file.\n",
    "    Args:\n",
    "        file_path_to_write: The filepath, including the file extension, to write the file.\n",
    "        content: The content to write into the file.\n",
    "    \"\"\"\n",
    "    with open(file_path_to_write, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "    return f\"File written to {file_path_to_write}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have downloaded your LLM and you have defined your tools, you need to pick your\n",
    "agent framework to build your agent. Note that the agent you'll build with any-agent can be run across multiple\n",
    "agent frameworks (Smolagent, TinyAgent, OpenAI, etc) and across various LLMs (Llama, DeepSeek, Mistral, etc).\n",
    "For this example, we will use the smolagents framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_agent import AgentConfig, AnyAgent\n",
    "\n",
    "# Define the agent\n",
    "agent = AnyAgent.create(\n",
    "    \"smolagents\",\n",
    "    AgentConfig(\n",
    "        model_id=\"ollama/granite3.3\",\n",
    "        instructions=\"\"\"\n",
    "        You must use the available tools to solve the task.\n",
    "        \"\"\",\n",
    "        tools=[write_file, read_file],\n",
    "        model_args={\"num_ctx\": 14000},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_trace = agent.run(\n",
    "    f\"\"\"\n",
    "You are an expert software engineer agent with an eye to detecting vulnerabilities and bugs.\n",
    "Carefully read the file in the path: {abs_path} and write a very short report of potential vulnerabilities or bugs,\n",
    "*if there are any*, in a report.md file in the same directory as the file.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `agent.run` method returns an AgentTrace object, which has a few convenient attributes for displaying some interesting information about the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_trace.final_output)  # Final answer\n",
    "print(f\"Duration: {agent_trace.duration.total_seconds():.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
