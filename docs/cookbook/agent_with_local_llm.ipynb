{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully offline Agent!\n",
    "\n",
    "_(no colab for this one since the point is to be run entirely locally)_\n",
    "\n",
    "This tutorial will guide on how to run agent, fully locally / offline i.e., running with a local LLM and a local MCP server, so no data will leave your machine!\n",
    "\n",
    "This can be especially useful for privacy-sensitive applications or when you want to avoid any cloud dependencies.\n",
    "\n",
    "In this example, we will showcase how to let an agent read and write in your local filesystem! Specifically, we will give read-access to the agent to our codebase so that it can write up and generate a README file to describe the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "any-agent uses the python asyncio module to support async functionality. When running in Jupyter notebooks, this means we need to enable the use of nested event loops. We'll install any-agent and enable this below using nest_asyncio."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install 'any-agent[smolagents]' --quiet\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your own LLM locally\n",
    "\n",
    "Regardless of which agent framework you choose in any-agent, all of them support LiteLLM, which is a proxy that allows us to use whichever LLM inside the framework, hosted on by any provider. For example, we could use a local model via llama.cpp or [llamafile](https://github.com/Mozilla-Ocho/llamafile), a google hosted gemini model, or a AWS bedrock hosted Llama model. For this example, we will use [Ollama](https://ollama.com/) to run our LLM locally!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama setup\n",
    "\n",
    "First, install Ollama by following their instructions: https://ollama.com/download\n",
    "\n",
    "### Picking an LLM\n",
    "\n",
    "Pick a model that you can run locally based on your hardware and running it in your terminal. For example:\n",
    "\n",
    "16-24GB RAM -> `granite3.3`  or  `deepseek-r1:8b` with ~20â€“35k context length\n",
    "\n",
    "24+GB RAM -> `mistral-small3.2` or `devstral:24b` with ~40k+ context length\n",
    "\n",
    "### Serving the model with the appropriate context length\n",
    "\n",
    "By default, Ollama forces a context length of 8192 tokens to all models, which is not enough for our agent to work properly.\n",
    "We can simply pass to the AgentConfig of `any-agent` as a model argument our desired value of context length like so:\n",
    "```\n",
    "    AgentConfig(\n",
    "        model_id=\"ollama/granite3.3\",\n",
    "        instructions=\"You must use the available tools to solve the task.\",\n",
    "        tools=[mcp_filesystem, show_plan],\n",
    "        model_args={\"num_ctx\": 24000},\n",
    "```\n",
    "\n",
    "References: [AgentConfig](https://mozilla-ai.github.io/any-agent/api/config/), [num_ctx](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values)\n",
    "\n",
    "All four of the models above have a max context length of 128k tokens, but if you have limited RAM if you set it to 128k it might cause you memory issues. For this example, we will set it to 24.000 tokens and provide a relatively small codebase.\n",
    "\n",
    "### Load and run the model\n",
    "\n",
    "In this tutorial, we will be running in a terminal, parallel to our notebook, `granite3.3`. In another terminal run:\n",
    "\n",
    "```\n",
    "ollama run granite3.3\n",
    "```\n",
    "\n",
    "For more information on setting environment variables in Ollama, please refer to their [documentation](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Agent and the Tools\n",
    "\n",
    "Instead of giving read-write access to the agent to the whole of our filesystem, we will limit its scope by manually adding which path its allowed to work in by providing it as an argument to the filesystem tool later on."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "codebase_directory = \".\"\n",
    "abs_path = str(Path(codebase_directory).resolve())\n",
    "print(f\"Codebase directory set: {abs_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick which tools to use\n",
    "\n",
    "Since we want our agent to work fully locally/offline, we will not add any tools that require communication with remote servers, in this case a local MCP server for secure file-system operations. We could also simply implement python callable functions that do these operations (e.g. using the os library), but instead we are opting here for an MCP server to showcase how easy it would be to swap or add other MCP servers to this use-case. To enforce that our agent doesn't go rogue and read/write in directories it's not supposed to access we will use the MCP server through a docker container that only has access to the directory above by mounting it. Before running the code below, make sure you have [Docker](https://docs.docker.com/get-started/) running in the background."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from any_agent.config import MCPStdio\n",
    "\n",
    "docker_destination = \"/projects\"\n",
    "\n",
    "mcp_filesystem = MCPStdio(\n",
    "    command=\"docker\",\n",
    "    args=[\n",
    "        \"run\",\n",
    "        \"-i\",\n",
    "        \"--rm\",\n",
    "        \"--mount\",\n",
    "        f\"type=bind,src={abs_path},dst={docker_destination}\",\n",
    "        \"mcp/filesystem\",\n",
    "        docker_destination,\n",
    "    ],\n",
    "    tools=[\n",
    "        \"read_file\",\n",
    "        \"read_multiple_files\",\n",
    "        \"write_file\",\n",
    "        \"list_allowed_directories\",\n",
    "        \"list_directory\",\n",
    "        \"search_files\",\n",
    "        \"directory_tree\",\n",
    "    ],  # we only include the tools we need\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your LLM is running on the background (local server) and you have defined your tools, you need to pick your agent framework to build your agent. Note that the agent you'll build with any-agent can be run across multiple agent frameworks (Smolagent, TinyAgent, OpenAI, etc) and across various LLMs (Llama, DeepSeek, Mistral, etc). For this example, we will use the smolagents framework."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from any_agent import AgentConfig, AnyAgent\n",
    "from any_agent.tools import show_plan\n",
    "\n",
    "# Define the agent\n",
    "agent = AnyAgent.create(\n",
    "    \"smolagents\",\n",
    "    AgentConfig(\n",
    "        model_id=\"ollama/granite3.3\",\n",
    "        instructions=\"\"\"\n",
    "        You must use the available tools to find an answer.\n",
    "        \"\"\",\n",
    "        tools=[mcp_filesystem, show_plan],\n",
    "        model_args={\"num_ctx\": 10000},\n",
    "    ),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agent_trace = agent.run(\n",
    "    \"Follow the instructions below:\"\n",
    "    \"1. Go through the files in the allowed directory, and any folder inside it recursively, \"\n",
    "    \"and identify any files that might contain programming code or documentation. \"\n",
    "    \"2. Read the content of each file identified above and then create a summary in \"\n",
    "    \"markdown format that summarizes what this project is about. \"\n",
    "    \"3. Finally, create and write this summary in a README.md file in the same directory.\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `agent.run` method returns an AgentTrace object, which has a few convenient attributes for displaying some interesting information about the run."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(agent_trace.final_output)  # Final answer\n",
    "print(f\"Duration: {agent_trace.duration.total_seconds():.2f} seconds\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
