{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully offline Agent!\n",
    "\n",
    "[![Agent with Local LLM](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mozilla-ai/any-agent/blob/main/docs/cookbook/agent_with_local_llm.ipynb) \n",
    "\n",
    "This example will guide on how to run agent, fully locally / offline, i.e. running with a local LLM and local MCP servers, so no data will leave your machine!\n",
    "This can be useful for privacy-sensitive applications, or for running agents in environments with limited internet connectivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "any-agent uses the python asyncio module to support async functionality. When running in Jupyter notebooks, this means we need to enable the use of nested event loops. We'll install any-agent and enable this below using nest_asyncio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T14:00:42.451271Z",
     "start_time": "2025-07-11T14:00:41.369260Z"
    }
   },
   "source": [
    "%pip install 'any-agent'\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your own LLM locally\n",
    "\n",
    "### Pick local LLM framework -> Ollama\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!curl -fsSL https://ollama.com/install.sh | sh",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Restart Ollama with a longer context length\n",
    "\n",
    "By default, Ollama has a context length of 8192 tokens, which is not enough for our agent to work properly.\n",
    "So, we will first \"kill\" the current Ollama process, and then restart it with a longer context length of 40000 tokens."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!killall -9 ollama; OLLAMA_CONTEXT_LENGTH=40000 OLLAMA_DEBUG=1 ollama serve",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Agent\n",
    "\n",
    "Now it's time to configure the agent! At this stage you have a few choices:\n",
    "\n",
    "### Pick the framework\n",
    "\n",
    "We support a variety of underlying agent frameworks (OpenAI, Smolagents, Langchain, TinyAgent, etc), which all have their own particular agentic AI implementations. For this tutorial's simple use case, any of the frameworks should work just fine, but any-agent makes it easy to try out a different framework later, if we so choose. For this example, we will use the [TinyAgent](frameworks/tinyagent.md) framework.  \n",
    "\n",
    "### Pick an LLM\n",
    "\n",
    "Regardless of which agent framework you choose, each framework supports LiteLLM, which is a proxy that allows us to use whichever LLM inside the framework, hosted on by any provider. For example, we could use a local model via llama.cpp or llamafile, a google hosted gemini model, or a AWS bedrock hosted Llama model. For this example, we will use [Ollama](https://ollama.com/)!\n",
    "\n",
    "### Pick which tools to use\n",
    "\n",
    " In this tutorial, we will provide the agent with access to web searches and web page visits. In later examples we will show more advanced tool usage like Model Context Protocol (MCP) tool use or inter-agent communication using Agent-2-Agent (A2A)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from any_agent import AgentConfig, AnyAgent\n",
    "from any_agent.tools import show_plan\n",
    "\n",
    "agent = AnyAgent.create(\n",
    "    \"smolagents\",  # See all options in https://mozilla-ai.github.io/any-agent/\n",
    "    AgentConfig(\n",
    "        model_id=\"ollama/devstral:24b\",\n",
    "        instructions=\"\"\"\n",
    "           You must use the available tools to find an answer.\n",
    "        \"\"\",\n",
    "        tools=[show_plan],\n",
    "        model_args={\"tool_choice\": \"required\"}\n",
    "    ),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Agent\n",
    "\n",
    "Now we've configured our agent, so it's time to run it! Let's give it a simple task: find 5 trending new TV shows that were released recently.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "agent_trace = agent.run(\"<><><>\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `agent.run` method returns an AgentTrace object, which has a few convenient attributes for displaying some interesting information about the run."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(agent_trace.final_output)  # Final answer\n",
    "print(f\"Duration: {agent_trace.duration.total_seconds():.2f} seconds\")\n",
    "print(f\"Usage: {agent_trace.tokens.total_tokens:,}\")\n",
    "print(f\"Cost (USD): {agent_trace.cost.total_cost:.6f}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
