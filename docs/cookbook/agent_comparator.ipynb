{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accc9fa7",
   "metadata": {},
   "source": [
    "# ü§ñ AI Agent Performance Comparison Tool\n",
    "\n",
    "## Using Mozilla's Any-Agent Framework\n",
    "\n",
    "This is a command line tool for developers exploring multiple agent frameworks. If you‚Äôre evaluating trade-offs in speed, cost, or style, and you want a quick, reproducible comparison, this tool gives you a ready-to-run example using Mozilla‚Äôs [any-agent](https://github.com/mozilla-ai/any-agent) abstraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd78f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install 'any-agent[openai,google,mistral]'\n",
    "! pip install mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32bf50",
   "metadata": {},
   "source": [
    "### API Keys Configuration\n",
    "Enter your API Keys to compare frameworks. We are using Mistral, OpenAI and Gemini, but you can add more as we explain later. \n",
    "\n",
    "\n",
    "## ü§ñ Supported Models & Frameworks\n",
    "\n",
    "The tool currently supports these any-agent framework combinations:\n",
    "\n",
    "| Model | Framework | Provider | Strengths |\n",
    "|-------|-----------|----------|-----------|\n",
    "| **GPT-4.1 Mini** | `openai` | OpenAI | Ultra-fast, minimal resource usage |\n",
    "| **GPT-3.5 Nano** | `openai` | OpenAI | Balanced speed and accuracy |\n",
    "| **Mistral Small** | `mistral` | Mistral AI | Cost-effective, multilingual capabilities |\n",
    "| **Gemini 2.5 Flash** | `google` | Google | Lightning-fast multimodal processing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bbdccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for each model/framework combination\"\"\"\n",
    "\n",
    "    name: str\n",
    "    framework: str\n",
    "    model_id: str\n",
    "    provider: str\n",
    "    description: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TestResult:\n",
    "    \"\"\"Results from a single agent test\"\"\"\n",
    "\n",
    "    model_config: str\n",
    "    framework: str\n",
    "    model_id: str\n",
    "    prompt: str\n",
    "    response: str\n",
    "    latency_ms: int\n",
    "    tokens_used: int\n",
    "    estimated_cost: float\n",
    "    success: bool\n",
    "    error: str | None\n",
    "    timestamp: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt-4.1-nano\": ModelConfig(\n",
    "        name=\"GPT 4.1 Nano\",\n",
    "        framework=\"openai\",\n",
    "        model_id=\"openai/gpt-4.1-nano\",\n",
    "        provider=\"OpenAI\",\n",
    "        description=\"High-quality model with excellent reasoning capabilities\",\n",
    "    ),\n",
    "    \"gpt-4.1-mini\": ModelConfig(\n",
    "        name=\"GPT 4.1 Mini\",\n",
    "        framework=\"openai\",\n",
    "        model_id=\"openai/gpt-4.1-mini\",\n",
    "        provider=\"OpenAI\",\n",
    "        description=\"Fast and cost-effective for most tasks\",\n",
    "    ),\n",
    "    \"mistral-small\": ModelConfig(\n",
    "        name=\"Mistral Small\",\n",
    "        framework=\"tinyagent\",\n",
    "        model_id=\"mistral/mistral-small-latest\",\n",
    "        provider=\"Mistral AI\",\n",
    "        description=\"Open-source friendly with good performance\",\n",
    "    ),\n",
    "    \"Gemini\": ModelConfig(\n",
    "        name=\"Gemini 2.5 Flash\",\n",
    "        framework=\"google\",  # Using tinyagent as fallback framework\n",
    "        model_id=\"gemini/gemini-2.5-flash\",\n",
    "        provider=\"Google\",\n",
    "        description=\"Balanced model with strong reasoning (Google Gemini 2.5 Flash)\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "for key in (\"MISTRAL_API_KEY\", \"OPENAI_API_KEY\", \"GEMINI_API_KEY\"):\n",
    "    if key not in os.environ:\n",
    "        print(f\"{key} not found in environment!\")\n",
    "        api_key = getpass(f\"Please enter your {key}: \")\n",
    "        os.environ[key] = api_key\n",
    "        print(f\"{key} set for this session!\")\n",
    "    else:\n",
    "        print(f\"{key} found in environment.\")\n",
    "\n",
    "\n",
    "def get_user_input(prompt: str) -> str:\n",
    "    \"\"\"Get user input with optional validation\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            return input(f\"\\n{prompt}\").strip()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nOperation cancelled\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f44fe0",
   "metadata": {},
   "source": [
    "\n",
    "### Easy Extension\n",
    "\n",
    "```python\n",
    "# Add new models easily\n",
    "\"new-model\": ModelConfig(\n",
    "    name=\"New Model\",\n",
    "    framework=\"new_framework\", \n",
    "    model_id=\"new-model-id\",\n",
    "    provider=\"New Provider\",\n",
    "    description=\"Description of capabilities\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53084c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models() -> list[str]:\n",
    "    print(\"We are going to run your prompt against the following models\")\n",
    "    print()\n",
    "\n",
    "    model_keys = list(model_configs.keys())\n",
    "    for i, key in enumerate(model_keys):\n",
    "        config = model_configs[key]\n",
    "        print(f\"{i + 1}. {config.name} ({config.provider})\")\n",
    "        print(f\"   Framework: {config.framework}\")\n",
    "\n",
    "    return model_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a8e9b",
   "metadata": {},
   "source": [
    "\n",
    "## üîß Technical Architecture\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "#### 1. Any-Agent Integration\n",
    "\n",
    "- Uses Mozilla's framework abstraction for consistency\n",
    "- Easy to add new frameworks as any-agent supports them\n",
    "\n",
    "#### 2. Async Performance Testing\n",
    "\n",
    "```python\n",
    "# Concurrent execution for fair comparison\n",
    "tasks = [test_agent_performance(model, prompt) for model in selected_models]\n",
    "results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "```\n",
    "\n",
    "### 3. Comprehensive Error Handling\n",
    "\n",
    "- Graceful degradation when models fail\n",
    "- Detailed error reporting for debugging\n",
    "- Continues testing even if some models error out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec584016",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_agent_performance(model_key: str, prompt: str) -> TestResult:\n",
    "    \"\"\"Test a single agent configuration and measure performance\"\"\"\n",
    "    model_config = model_configs[model_key]\n",
    "    start_time = time.time()\n",
    "    print(f\"TESTING {model_config.name} ({model_config.framework})\")\n",
    "    try:\n",
    "        # Import any-agent components\n",
    "        from any_agent import AgentConfig, AnyAgent\n",
    "\n",
    "        # Create agent configuration\n",
    "        agent_config = AgentConfig(\n",
    "            model_id=model_config.model_id,\n",
    "            instructions=\"You are a helpful AI assistant. Provide clear, concise, and accurate responses.\",\n",
    "            tools=[],  # No tools for basic performance testing\n",
    "        )\n",
    "\n",
    "        # Create agent with specified framework\n",
    "        agent = await AnyAgent.create_async(model_config.framework, agent_config)\n",
    "\n",
    "        # Show progress indicator\n",
    "        print(f\"  ‚Üí {model_config.name}: Processing\", end=\"\", flush=True)\n",
    "\n",
    "        # Run the agent\n",
    "        agent_trace = await agent.run_async(prompt)\n",
    "\n",
    "        latency_ms = agent_trace.duration.total_seconds() * 1000\n",
    "\n",
    "        # Extract response from trace\n",
    "        response = (\n",
    "            str(agent_trace.final_response)\n",
    "            if hasattr(agent_trace, \"final_response\")\n",
    "            else str(agent_trace)\n",
    "        )\n",
    "        output = (\n",
    "            agent_trace.final_output if hasattr(agent_trace, \"final_output\") else \"\"\n",
    "        )\n",
    "\n",
    "        estimated_cost = agent_trace.cost.total_cost\n",
    "\n",
    "        print(f\"\\r  ‚úì {model_config.name}: {latency_ms}ms\")\n",
    "\n",
    "        return TestResult(\n",
    "            model_config=model_key,\n",
    "            framework=model_config.framework,\n",
    "            model_id=model_config.model_id,\n",
    "            prompt=prompt,\n",
    "            response=response,\n",
    "            latency_ms=latency_ms,\n",
    "            tokens_used=agent_trace.tokens.total_tokens,\n",
    "            estimated_cost=estimated_cost,\n",
    "            success=True,\n",
    "            error=None,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            output=output,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        latency_ms = int((end_time - start_time) * 1000)\n",
    "\n",
    "        print(f\"\\r  ‚úó {model_config.name}: Failed ({str(e)[:500]}...)\")\n",
    "\n",
    "        return TestResult(\n",
    "            model_config=model_key,\n",
    "            framework=model_config.framework,\n",
    "            model_id=model_config.model_id,\n",
    "            prompt=prompt,\n",
    "            response=\"\",\n",
    "            latency_ms=latency_ms,\n",
    "            tokens_used=0,\n",
    "            estimated_cost=0,\n",
    "            success=False,\n",
    "            error=str(e),\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            output=\"\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef27ca",
   "metadata": {},
   "source": [
    "\n",
    "## üìä Sample Output\n",
    "\n",
    "Example run for prompt: \"Write a haiku on Uranus\"\n",
    "\n",
    "```bash\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "  PERFORMANCE COMPARISON RESULTS\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "1. Mistral Small\n",
    "   Framework: tinyagent\n",
    "   Latency:   2743.4970000000003ms\n",
    "   Cost:      $0.0000\n",
    "   Tokens:    127\n",
    "   Output:  Icy blue jewel,\n",
    "            Uranus spins on its side,\n",
    "            Mysteries untold. \n",
    "\n",
    "2. GPT-4.1 Nano\n",
    "   Framework: openai\n",
    "   Latency:   2458.0440000000003ms\n",
    "   Cost:      $0.0000\n",
    "   Tokens:    53\n",
    "   Output:  Blue planet afar,  \n",
    "            Majestic and cold in the night,  \n",
    "            Uranus whispers. \n",
    "\n",
    "3. GPT-4.1 Mini\n",
    "   Framework: openai\n",
    "   Latency:   2749.108ms\n",
    "   Cost:      $0.0000\n",
    "   Tokens:    53\n",
    "   Output:  Icy giant spins,  \n",
    "            Skyward blue-green mystery,  \n",
    "            Rings in tilted grace. \n",
    "\n",
    "4. Gemini 2.5 Flash\n",
    "   Framework: google\n",
    "   Latency:   1955.3020000000001ms\n",
    "   Cost:      $0.0008\n",
    "   Tokens:    350\n",
    "   Output:  Blue-green ice giant,\n",
    "            Tilted world, so cold and deep,\n",
    "            Whispers in the dark.   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82275824",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def display_results(results: list[TestResult]) -> None:\n",
    "    \"\"\"Display formatted results with analysis\"\"\"\n",
    "    print(\"PERFORMANCE COMPARISON RESULTS\")\n",
    "\n",
    "    # Filter successful results for analysis\n",
    "    successful_results = [r for r in results if r.success]\n",
    "\n",
    "    if not successful_results:\n",
    "        print(\"‚ùå No successful results to display\")\n",
    "        return\n",
    "\n",
    "    # Sort by latency for ranking\n",
    "    successful_results.sort(key=lambda x: x.latency_ms)\n",
    "\n",
    "    print()\n",
    "    for i, result in enumerate(successful_results):\n",
    "        model_config = model_configs[result.model_config]\n",
    "        rank = i + 1\n",
    "\n",
    "        print(f\"{rank}. {model_config.name}\")\n",
    "        print(f\"   Framework: {result.framework}\")\n",
    "        print(f\"   Latency:   {result.latency_ms}ms\")\n",
    "        print(f\"   Cost:      ${result.estimated_cost:.4f}\")\n",
    "        print(f\"   Tokens:    {result.tokens_used}\")\n",
    "        print(f\"   Output:  {result.output} \")\n",
    "        print()\n",
    "\n",
    "    # display_analysis(successful_results)\n",
    "\n",
    "    # Show failed results if any\n",
    "    failed_results = [r for r in results if not r.success]\n",
    "    if failed_results:\n",
    "        print(f\"\\n‚ö†Ô∏è  {len(failed_results)} model(s) failed:\")\n",
    "        for result in failed_results:\n",
    "            model_config = model_configs[result.model_config]\n",
    "            print(f\"   ‚úó {model_config.name}: {result.error}\")\n",
    "\n",
    "\n",
    "async def run_performance_comparison() -> None:\n",
    "    \"\"\"Main performance comparison workflow\"\"\"\n",
    "    models = list_models()\n",
    "\n",
    "    prompt = get_user_input(\"Enter prompt: \")\n",
    "    # await select_prompt(modelconfigs)\n",
    "\n",
    "    print(\"RUNNING PERFORMANCE COMPARISON\")\n",
    "    print(f\"Testing {len(models)} models with prompt:\")\n",
    "    print(f\"{prompt[:80]}...'\")\n",
    "    print()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run tests concurrently for realistic comparison\n",
    "    tasks = [test_agent_performance(model, prompt) for model in models]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    # Handle any exceptions\n",
    "    valid_results = []\n",
    "    for i, result in enumerate(results):\n",
    "        if isinstance(result, Exception):\n",
    "            model_config = model_configs[models[i]]\n",
    "            print(f\"  ‚úó {model_config.name}: Exception occurred\")\n",
    "        else:\n",
    "            valid_results.append(result)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal test time: {total_time:.2f}s\")\n",
    "\n",
    "    if valid_results:\n",
    "        await display_results(valid_results)\n",
    "\n",
    "\n",
    "await run_performance_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7592ebea",
   "metadata": {},
   "source": [
    "\n",
    "## üöÄ Roadmap\n",
    "\n",
    "1. Add Anthropic and Llamafile support for local models\n",
    "2. Integrate official any-agent.evaluation module for trace-based scoring\n",
    "3. Export results to CSV/JSON for dashboards\n",
    "4. Statistical analysis across multiple prompts\n",
    "5. CLI flags for batch testing and exporting\n",
    "\n",
    "## ü§ù Contributing\n",
    "\n",
    "We welcome contributions! Here are a few ways to help:\n",
    "\n",
    "- Add models/frameworks by extending the ModelConfig dictionary.\n",
    "- Improve evaluation with new scoring rubrics or by integrating any-agent.evaluation.\n",
    "- Suggest prompts for testing creative, technical, or multilingual cases.\n",
    "- Open issues/PRs for bugs, docs, or feature ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
