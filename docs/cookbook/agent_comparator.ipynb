{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accc9fa7",
   "metadata": {},
   "source": [
    "# 🤖 AI Agent Performance Comparison Tool\n",
    "\n",
    "## Using Mozilla's Any-Agent Framework\n",
    "\n",
    "This is a command line tool for developers exploring multiple agent frameworks. If you’re evaluating trade-offs in speed, cost, or style, and you want a quick, reproducible comparison, this tool gives you a ready-to-run example using Mozilla’s [any-agent](https://github.com/mozilla-ai/any-agent) abstraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd78f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! pip install 'any-agent[openai,google,mistral]'\n",
    "! pip install mistralai "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32bf50",
   "metadata": {},
   "source": [
    "### API Keys Configuration\n",
    "Enter your API Keys to compare frameworks. We are using Mistral, OpenAI and Gemini, but you can add more as we explain later. \n",
    "\n",
    "\n",
    "## 🤖 Supported Models & Frameworks\n",
    "\n",
    "The tool currently supports these any-agent framework combinations:\n",
    "\n",
    "| Model | Framework | Provider | Strengths |\n",
    "|-------|-----------|----------|-----------|\n",
    "| **GPT-4.1 Mini** | `openai` | OpenAI | Ultra-fast, minimal resource usage |\n",
    "| **GPT-3.5 Nano** | `openai` | OpenAI | Balanced speed and accuracy |\n",
    "| **Mistral Small** | `mistral` | Mistral AI | Cost-effective, multilingual capabilities |\n",
    "| **Gemini 2.5 Flash** | `google` | Google | Lightning-fast multimodal processing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bbdccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dataclasses import asdict\n",
    "from typing import List, Optional\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "from getpass import getpass\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for each model/framework combination\"\"\"\n",
    "    name: str\n",
    "    framework: str\n",
    "    model_id: str\n",
    "    provider: str\n",
    "    description: str\n",
    "\n",
    "@dataclass\n",
    "class TestResult:\n",
    "    \"\"\"Results from a single agent test\"\"\"\n",
    "    model_config: str\n",
    "    framework: str\n",
    "    model_id: str\n",
    "    prompt: str\n",
    "    response: str\n",
    "    latency_ms: int\n",
    "    tokens_used: int\n",
    "    estimated_cost: float\n",
    "    success: bool\n",
    "    error: Optional[str]\n",
    "    timestamp: str\n",
    "    output: str\n",
    "\n",
    "model_configs = {\n",
    "            \"gpt-4.1-nano\": ModelConfig(\n",
    "                name=\"GPT 4.1 Nano\",\n",
    "                framework=\"openai\",\n",
    "                model_id=\"openai/gpt-4.1-nano\",\n",
    "                provider=\"OpenAI\",\n",
    "                description=\"High-quality model with excellent reasoning capabilities\"\n",
    "            ),\n",
    "            \"gpt-4.1-mini\": ModelConfig(\n",
    "                name=\"GPT 4.1 Mini\",\n",
    "                framework=\"openai\", \n",
    "                model_id=\"openai/gpt-4.1-mini\",\n",
    "                provider=\"OpenAI\",\n",
    "                description=\"Fast and cost-effective for most tasks\"\n",
    "            ),\n",
    "            \"mistral-small\": ModelConfig(\n",
    "                name=\"Mistral Small\",\n",
    "                framework=\"tinyagent\",\n",
    "                model_id=\"mistral/mistral-small-latest\",\n",
    "                provider=\"Mistral AI\",\n",
    "                description=\"Open-source friendly with good performance\"\n",
    "            ),\n",
    "            \"Gemini\": ModelConfig(\n",
    "                name=\"Gemini 2.5 Flash\",\n",
    "                framework=\"google\",  # Using tinyagent as fallback framework\n",
    "                model_id=\"gemini/gemini-2.5-flash\",\n",
    "                provider=\"Google\",\n",
    "                description=\"Balanced model with strong reasoning (Google Gemini 2.5 Flash)\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "for key in (\"MISTRAL_API_KEY\", \"OPENAI_API_KEY\", \"GEMINI_API_KEY\"):\n",
    "    if key not in os.environ:\n",
    "        print(f\"{key} not found in environment!\")\n",
    "        api_key = getpass(f\"Please enter your {key}: \")\n",
    "        os.environ[key] = api_key\n",
    "        print(f\"{key} set for this session!\")\n",
    "    else:\n",
    "        print(f\"{key} found in environment.\")\n",
    "\n",
    "def get_user_input(  prompt: str) -> str:\n",
    "            \"\"\"Get user input with optional validation\"\"\"\n",
    "            while True:\n",
    "                try:\n",
    "                    return input(f\"\\n{prompt}\").strip()\n",
    "                except KeyboardInterrupt:\n",
    "                    print(f\"\\nOperation cancelled\")\n",
    "                    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f44fe0",
   "metadata": {},
   "source": [
    "\n",
    "### Easy Extension\n",
    "\n",
    "```python\n",
    "# Add new models easily\n",
    "\"new-model\": ModelConfig(\n",
    "    name=\"New Model\",\n",
    "    framework=\"new_framework\", \n",
    "    model_id=\"new-model-id\",\n",
    "    provider=\"New Provider\",\n",
    "    description=\"Description of capabilities\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53084c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models() -> List[str]:\n",
    "            print(\"We are going to run your prompt against the following models\")\n",
    "            print()\n",
    "            \n",
    "            model_keys = list(model_configs.keys())\n",
    "            for i, key in enumerate(model_keys):\n",
    "                config = model_configs[key]\n",
    "                print(f\"{i+1}. {config.name} ({config.provider})\") \n",
    "                print(f\"   Framework: {config.framework}\")\n",
    "            \n",
    "            return model_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a8e9b",
   "metadata": {},
   "source": [
    "\n",
    "## 🔧 Technical Architecture\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "#### 1. Any-Agent Integration\n",
    "\n",
    "- Uses Mozilla's framework abstraction for consistency\n",
    "- Easy to add new frameworks as any-agent supports them\n",
    "\n",
    "#### 2. Async Performance Testing\n",
    "\n",
    "```python\n",
    "# Concurrent execution for fair comparison\n",
    "tasks = [test_agent_performance(model, prompt) for model in selected_models]\n",
    "results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "```\n",
    "\n",
    "### 3. Comprehensive Error Handling\n",
    "\n",
    "- Graceful degradation when models fail\n",
    "- Detailed error reporting for debugging\n",
    "- Continues testing even if some models error out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec584016",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_agent_performance(  model_key: str, prompt: str) -> TestResult:\n",
    "            \"\"\"Test a single agent configuration and measure performance\"\"\"\n",
    "            model_config = model_configs[model_key]\n",
    "            start_time = time.time()\n",
    "            print(f\"TESTING {model_config.name} ({model_config.framework})\")\n",
    "            try:\n",
    "                # Import any-agent components\n",
    "                from any_agent import AgentConfig, AnyAgent\n",
    "                \n",
    "                # Create agent configuration\n",
    "                agent_config = AgentConfig(\n",
    "                    model_id=model_config.model_id,\n",
    "                    instructions=\"You are a helpful AI assistant. Provide clear, concise, and accurate responses.\",\n",
    "                    tools=[]  # No tools for basic performance testing\n",
    "                )\n",
    "                \n",
    "                # Create agent with specified framework\n",
    "                agent = await AnyAgent.create_async(model_config.framework, agent_config)\n",
    "                \n",
    "                # Show progress indicator\n",
    "                print(f\"  → {model_config.name}: Processing\", end=\"\", flush=True)\n",
    "                \n",
    "                # Run the agent\n",
    "                agent_trace = await agent.run_async(prompt)\n",
    "        \n",
    "                latency_ms = agent_trace.duration.total_seconds() * 1000; \n",
    "                \n",
    "                # Extract response from trace\n",
    "                response = str(agent_trace.final_response) if hasattr(agent_trace, 'final_response') else str(agent_trace)\n",
    "                output = agent_trace.final_output if hasattr(agent_trace, 'final_output') else \"\"\n",
    "\n",
    "                estimated_cost = agent_trace.cost.total_cost;\n",
    "                \n",
    "                print(f\"\\r  ✓ {model_config.name}: {latency_ms}ms\")\n",
    "                \n",
    "                return TestResult(\n",
    "                    model_config=model_key,\n",
    "                    framework=model_config.framework,\n",
    "                    model_id=model_config.model_id,\n",
    "                    prompt=prompt,\n",
    "                    response=response,\n",
    "                    latency_ms=latency_ms,\n",
    "                    tokens_used=agent_trace.tokens.total_tokens,\n",
    "                    estimated_cost=estimated_cost,\n",
    "                    success=True,\n",
    "                    error=None,\n",
    "                    timestamp=datetime.now().isoformat(),\n",
    "                    output=output\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                end_time = time.time()\n",
    "                latency_ms = int((end_time - start_time) * 1000)\n",
    "                \n",
    "                print(f\"\\r  ✗ {model_config.name}: Failed ({str(e)[:500]}...)\")\n",
    "                \n",
    "                return TestResult(\n",
    "                    model_config=model_key,\n",
    "                    framework=model_config.framework,\n",
    "                    model_id=model_config.model_id,\n",
    "                    prompt=prompt,\n",
    "                    response=\"\",\n",
    "                    latency_ms=latency_ms,\n",
    "                    tokens_used=0,\n",
    "                    estimated_cost=0,\n",
    "                    success=False,\n",
    "                    error=str(e),\n",
    "                    timestamp=datetime.now().isoformat(),\n",
    "                    output=\"\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef27ca",
   "metadata": {},
   "source": [
    "\n",
    "## 📊 Sample Output\n",
    "\n",
    "Example run for prompt: \"Write a haiku on Uranus\"\n",
    "\n",
    "```bash\n",
    "═══════════════════════════════════════\n",
    "  PERFORMANCE COMPARISON RESULTS\n",
    "═══════════════════════════════════════\n",
    "\n",
    "1. Mistral Small\n",
    "   Framework: tinyagent\n",
    "   Latency:   2743.4970000000003ms\n",
    "   Cost:      $0.0000\n",
    "   Tokens:    127\n",
    "   Output:  Icy blue jewel,\n",
    "            Uranus spins on its side,\n",
    "            Mysteries untold. \n",
    "\n",
    "2. GPT-4.1 Nano\n",
    "   Framework: openai\n",
    "   Latency:   2458.0440000000003ms\n",
    "   Cost:      $0.0000\n",
    "   Tokens:    53\n",
    "   Output:  Blue planet afar,  \n",
    "            Majestic and cold in the night,  \n",
    "            Uranus whispers. \n",
    "\n",
    "3. GPT-4.1 Mini\n",
    "   Framework: openai\n",
    "   Latency:   2749.108ms\n",
    "   Cost:      $0.0000\n",
    "   Tokens:    53\n",
    "   Output:  Icy giant spins,  \n",
    "            Skyward blue-green mystery,  \n",
    "            Rings in tilted grace. \n",
    "\n",
    "4. Gemini 2.5 Flash\n",
    "   Framework: google\n",
    "   Latency:   1955.3020000000001ms\n",
    "   Cost:      $0.0008\n",
    "   Tokens:    350\n",
    "   Output:  Blue-green ice giant,\n",
    "            Tilted world, so cold and deep,\n",
    "            Whispers in the dark.   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82275824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def display_results( results: List[TestResult]) -> None:\n",
    "            \"\"\"Display formatted results with analysis\"\"\"\n",
    "            print(\"PERFORMANCE COMPARISON RESULTS\")\n",
    "            \n",
    "            # Filter successful results for analysis\n",
    "            successful_results = [r for r in results if r.success]\n",
    "            \n",
    "            if not successful_results:\n",
    "                print(\"❌ No successful results to display\")\n",
    "                return\n",
    "            \n",
    "            # Sort by latency for ranking\n",
    "            successful_results.sort(key=lambda x: x.latency_ms)\n",
    "            \n",
    "            print()\n",
    "            for i, result in enumerate(successful_results):\n",
    "                model_config = model_configs[result.model_config]\n",
    "                rank = i + 1\n",
    "               \n",
    "                \n",
    "                print(f\"{rank}. {model_config.name}\")\n",
    "                print(f\"   Framework: {result.framework}\")\n",
    "                print(f\"   Latency:   {result.latency_ms}ms\")\n",
    "                print(f\"   Cost:      ${result.estimated_cost:.4f}\")\n",
    "                print(f\"   Tokens:    {result.tokens_used}\")\n",
    "                print(f\"   Output:  {result.output} \")\n",
    "                print()\n",
    "            \n",
    "            # display_analysis(successful_results)\n",
    "            \n",
    "            # Show failed results if any\n",
    "            failed_results = [r for r in results if not r.success]\n",
    "            if failed_results:\n",
    "                print(f\"\\n⚠️  {len(failed_results)} model(s) failed:\")\n",
    "                for result in failed_results:\n",
    "                    model_config = model_configs[result.model_config]\n",
    "                    print(f\"   ✗ {model_config.name}: {result.error}\")\n",
    "\n",
    "async def run_performance_comparison() -> None:\n",
    "            \"\"\"Main performance comparison workflow\"\"\"\n",
    "            models = list_models();\n",
    "\n",
    "            prompt = get_user_input(\"Enter prompt: \"); \n",
    "            # await select_prompt(modelconfigs)\n",
    "            \n",
    "            print(\"RUNNING PERFORMANCE COMPARISON\")\n",
    "            print(f\"Testing {len(models)} models with prompt:\")\n",
    "            print(f\"{prompt[:80]}...'\")\n",
    "            print()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Run tests concurrently for realistic comparison\n",
    "            tasks = [test_agent_performance( model, prompt) for model in models]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            # Handle any exceptions\n",
    "            valid_results = []\n",
    "            for i, result in enumerate(results):\n",
    "                if isinstance(result, Exception):\n",
    "                    model_config = model_configs[models[i]]\n",
    "                    print(f\"  ✗ {model_config.name}: Exception occurred\")\n",
    "                else:\n",
    "                    valid_results.append(result)\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"\\nTotal test time: {total_time:.2f}s\")\n",
    "            \n",
    "            if valid_results:\n",
    "                await display_results(valid_results)\n",
    "                \n",
    "\n",
    "await run_performance_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7592ebea",
   "metadata": {},
   "source": [
    "\n",
    "## 🚀 Roadmap\n",
    "\n",
    "1. Add Anthropic and Llamafile support for local models\n",
    "2. Integrate official any-agent.evaluation module for trace-based scoring\n",
    "3. Export results to CSV/JSON for dashboards\n",
    "4. Statistical analysis across multiple prompts\n",
    "5. CLI flags for batch testing and exporting\n",
    "\n",
    "## 🤝 Contributing\n",
    "\n",
    "We welcome contributions! Here are a few ways to help:\n",
    "\n",
    "- Add models/frameworks by extending the ModelConfig dictionary.\n",
    "- Improve evaluation with new scoring rubrics or by integrating any-agent.evaluation.\n",
    "- Suggest prompts for testing creative, technical, or multilingual cases.\n",
    "- Open issues/PRs for bugs, docs, or feature ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
