{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"any-agent","text":"<p><code>any-agent</code> is a Python library providing a single interface to different agent frameworks.</p> <p>Warning</p> <p>Compared to traditional code-defined workflows, agent frameworks introduce complexity, additional security implications to consider, and demand much more computational power.</p> <p>Before jumping to use one, carefully consider and evaluate how much value you would get compared to manually defining a sequence of tools and LLM calls.</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or newer</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>You can install the bare bones library as follows (only <code>TinyAgent</code> will be available):</p> <pre><code>pip install any-agent\n</code></pre> <p>Or you can install it with the required dependencies for different frameworks:</p> <pre><code>pip install any-agent[agno,openai]\n</code></pre> <p>Refer to pyproject.toml for a list of the options available.</p>"},{"location":"#for-ai-systems","title":"For AI Systems","text":"<p>This documentation is available in two AI-friendly formats:</p> <ul> <li>llms.txt - A structured overview with curated links to key documentation sections</li> <li>llms-full.txt - Complete documentation content concatenated into a single file</li> </ul>"},{"location":"evaluation/","title":"Agent Evaluation","text":"<p>The any-agent evaluation module encourages three approaches for evaluating agent traces:</p> <ol> <li>Custom Code Evaluation: Direct programmatic inspection of traces for deterministic checks</li> <li><code>LlmJudge</code>: LLM-as-a-judge for evaluations that can be answered with a direct LLM call alongside a custom context</li> <li><code>AgentJudge</code>: Complex LLM-based evaluations that utilize built-in and customizable tools to inspect specific parts of the trace or other custom information provided to the agent as a tool</li> </ol>"},{"location":"evaluation/#choosing-the-right-evaluation-method","title":"Choosing the Right Evaluation Method","text":"Method Best For Pros Cons Custom Code Deterministic checks, performance metrics, specific criteria Fast, reliable, cost-effective, precise control Requires manual coding, limited to predefined checks LlmJudge Simple qualitative assessments, text-based evaluations Easy to set up, flexible questions, good for subjective evaluation Can be inconsistent, costs tokens, slower than code AgentJudge Complex multi-step evaluations, tool usage analysis Most flexible, can support tool access to custom additional information sources Highest cost, slowest, most complex setup <p>Both judges work with any-agent's unified tracing format and return structured evaluation results.</p>"},{"location":"evaluation/#custom-code-evaluation","title":"Custom Code Evaluation","text":"<p>Before automatically using an LLM based approach, it is worthwhile to consider whether it is necessary. For deterministic evaluations where you know exactly what to check, you may not want an LLM-based judge at all. Writing a custom evaluation function that directly examines the trace can be more efficient, reliable, and cost-effective. the any-agent <code>AgentTrace</code> provides a few helpful methods that can be used to extract common information.</p>"},{"location":"evaluation/#example-custom-evaluation-function","title":"Example: Custom Evaluation Function","text":"<pre><code>from any_agent.tracing.agent_trace import AgentTrace\n\ndef evaluate_efficiency(trace: AgentTrace) -&gt; dict:\n    \"\"\"Custom evaluation function for efficiency criteria.\"\"\"\n\n    # Direct access to trace properties\n    token_count = trace.tokens.total_tokens\n    step_count = len(trace.spans)\n    final_output = trace.final_output\n\n    # Apply your specific criteria\n    results = {\n        \"token_efficient\": token_count &lt; 1000,\n        \"step_efficient\": step_count &lt;= 5,\n        \"has_output\": final_output is not None,\n        \"token_count\": token_count,\n        \"step_count\": step_count\n    }\n\n    # Calculate overall pass/fail\n    results[\"passed\"] = all([\n        results[\"token_efficient\"],\n        results[\"step_efficient\"],\n        results[\"has_output\"]\n    ])\n\n    return results\n\n# Usage\nfrom any_agent import AgentConfig, AnyAgent\nfrom any_agent.evaluation import LlmJudge\nfrom any_agent.tools import search_web\n\n# First, run an agent to get a trace\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\",\n        tools=[search_web]\n    ),\n)\ntrace = agent.run(\"What is the capital of France?\")\nevaluation = evaluate_efficiency(trace)\nprint(f\"Evaluation results: {evaluation}\")\n</code></pre>"},{"location":"evaluation/#working-with-trace-spans","title":"Working with Trace Spans","text":"<p>You can also examine the conversation flow directly:</p> <pre><code>from any_agent.tracing.attributes import GenAI\n\ndef check_tool_usage(trace: AgentTrace, required_tool: str) -&gt; bool:\n    \"\"\"Check if a specific tool was used in the trace.\"\"\"\n    return any(\n        span.attributes[GenAI.TOOL_NAME] == required_tool\n        for span in trace.spans if span.is_tool_execution()\n    )\n\n# Usage\nused_search = check_tool_usage(trace, \"search_web\")\nprint(f\"Used web search: {used_search}\")\n</code></pre>"},{"location":"evaluation/#llmjudge","title":"LlmJudge","text":"<p>The <code>LlmJudge</code> is ideal for straightforward evaluation questions that can be answered by examining the complete trace text. It's efficient and works well for:</p> <ul> <li>Basic pass/fail assessments</li> <li>Simple criteria checking</li> <li>Text-based evaluations</li> </ul>"},{"location":"evaluation/#example-evaluating-response-quality-and-helpfulness","title":"Example: Evaluating Response Quality and Helpfulness","text":"<pre><code>from any_agent import AnyAgent, AgentConfig\nfrom any_agent.tools import search_web\nfrom any_agent.evaluation import LlmJudge\n# Run an agent on a customer support task\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\",\n        tools=[search_web]\n    ),\n)\n\ntrace = agent.run(\n    \"A customer is asking about setting up a new email account on the latest version of iOS. \"\n    \"They mention they're not very tech-savvy and seem frustrated. \"\n    \"Help them with clear, step-by-step instructions.\"\n)\n\n# Evaluate the quality of the agent's response multiple times\njudge = LlmJudge(model_id=\"mistral:mistral-small-latest\")\nevaluation_questions = [\n    \"Did it provide clear, step-by-step instructions?\",\n    \"Was the tone empathetic and appropriate for a frustrated, non-technical customer?\",\n    \"Did it avoid using technical jargon without explanation?\",\n    \"Was the response complete and actionable?\",\n    \"Does the description specify which version of iOS this works with?\"\n]\n\n\n# Run evaluation 4 times to check consistency\nresults = []\nfor evaluation_question in evaluation_questions:\n    question = f\"Evaluate whether the agent's response demonstrates good customer service by considering: {evaluation_question}.\"\n    result = judge.run(context=str(trace.spans_to_messages()), question=evaluation_question)\n    results.append(result)\n\n# Print all results\nfor i, result in enumerate(results, 1):\n    print(f\"Run {i} - Passed: {result.passed}\")\n    print(f\"Run {i} - Reasoning: {result.reasoning}\")\n    print(\"-\" * 50)\n</code></pre> <p>Async Usage</p> <p>For async applications, use <code>judge.run_async()</code> instead of <code>judge.run()</code>.</p>"},{"location":"evaluation/#agentjudge","title":"AgentJudge","text":"<p>The <code>AgentJudge</code> is designed for complex evaluations that require inspecting specific aspects of the trace. It comes equipped with evaluation tools and can accept additional custom tools for specialized assessments.</p>"},{"location":"evaluation/#built-in-evaluation-tools","title":"Built-in Evaluation Tools","text":"<p>The <code>AgentJudge</code> automatically has access to these evaluation tools:</p> <ul> <li><code>get_final_output()</code>: Get the agent's final output</li> <li><code>get_tokens_used()</code>: Get total token usage</li> <li><code>get_steps_taken()</code>: Get number of steps taken</li> <li><code>get_messages_from_trace()</code>: Get formatted trace messages</li> <li><code>get_duration()</code>: Get the duration in seconds of the trace</li> </ul>"},{"location":"evaluation/#example-agent-judge-with-tool-access","title":"Example: Agent Judge with Tool Access","text":"<pre><code>from any_agent.evaluation import AgentJudge\n\n# Create an agent judge\njudge = AgentJudge(model_id=\"mistral:mistral-small-latest\")\n\n# Evaluate with access to trace inspection tools\neval_trace = judge.run(\n    trace=trace,\n    question=\"Does the final answer provided by the trace mention and correctly specify the most recent major version of iOS? You may need to do a web search to determine the most recent version of iOS. If the final answer does not mention the version at all, this criteria should fail\",\n    additional_tools=[search_web]\n)\n\nresult = eval_trace.final_output\nprint(f\"Passed: {result.passed}\")\nprint(f\"Reasoning: {result.reasoning}\")\n</code></pre> <p>Async Usage</p> <p>For async applications, use <code>judge.run_async()</code> instead of <code>judge.run()</code>.</p>"},{"location":"evaluation/#adding-custom-tools","title":"Adding Custom Tools","text":"<p>You can extend the <code>AgentJudge</code> with additional tools for specialized evaluations:</p> <pre><code>def current_ios_version() -&gt; str:\n    \"\"\"Custom tool to retrieve the most recent version of iOS\n\n    Returns:\n        The version of iOS\n    \"\"\"\n    return \"iOS 18.5\"\n\njudge = AgentJudge(model_id=\"mistral:mistral-small-latest\")\neval_trace = judge.run(\n    trace=trace,\n    question=\"Does the final answer provided by the trace mention and correctly specify the most recent major version of iOS? If the final answer does not mention the version at all, this criteria should fail\",\n    additional_tools=[current_ios_version]\n)\n</code></pre>"},{"location":"evaluation/#custom-output-types","title":"Custom Output Types","text":"<p>Both judges support custom output schemas using Pydantic models:</p> <pre><code>from pydantic import BaseModel\n\nclass DetailedEvaluation(BaseModel):\n    passed: bool\n    reasoning: str\n    confidence_score: float\n    suggestions: list[str]\n\njudge = LlmJudge(\n    model_id=\"mistral:mistral-small-latest\",\n    output_type=DetailedEvaluation\n)\n\nresult = judge.run(trace=trace, question=\"Evaluate the agent's performance\")\nprint(f\"Confidence: {result.confidence_score}\")\nprint(f\"Suggestions: {result.suggestions}\")\n</code></pre>"},{"location":"serving/","title":"Serving","text":"<p><code>any-agent</code> provides a simple way of serving agents from any of the supported frameworks using different protocols:</p> <ul> <li> <p>Agent2Agent Protocol (A2A), via the A2A Python SDK. In order to this protocol, you must install the 'a2a' extra: <code>pip install 'any-agent[a2a]'</code>.</p> </li> <li> <p>Model Context Protocol (MCP), via the MCP Python SDK.</p> </li> </ul>"},{"location":"serving/#configuring-and-serving-agents","title":"Configuring and Serving Agents","text":"<p>You can configure and serve an agent using the <code>A2AServingConfig</code> or <code>MCPServingConfig</code> and the <code>AnyAgent.serve_async</code> method.</p> <p>For illustrative purposes, we are going to define 2 separate scripts, each defining an agent to answer questions about a specific agent framework (either Google ADK or OpenAI Agents SDK):</p> <p>Note</p> <p>We are using here the <code>google</code> and <code>openai</code> frameworks for each corresponding \"expert\" but we could actually use any of the supported frameworks, as the actual behavior is defined by the <code>instructions</code> and <code>description</code>.</p> Google ExpertOpenAI Expert <pre><code># google_expert.py\nimport argparse\nimport asyncio\n\nfrom any_agent import AgentConfig, AnyAgent\nfrom any_agent.tools import search_web\n\n\nasync def serve_agent(protocol):\n    agent = await AnyAgent.create_async(\n        \"google\",\n        AgentConfig(\n            name=\"google_expert\",\n            model_id=\"mistral:mistral-small-latest\",\n            instructions=\"You can answer questions about the Google Agents Development Kit (ADK) but nothing else\",\n            description=\"An agent that can answer questions specifically and only about the Google Agents Development Kit (ADK).\",\n            tools=[search_web]\n        )\n    )\n\n    if protocol == \"a2a\":\n        from any_agent.serving import A2AServingConfig\n        serving_config = A2AServingConfig(port=5001, endpoint=\"/google\")\n    elif protocol == \"mcp\":\n        from any_agent.serving import MCPServingConfig\n        serving_config = MCPServingConfig(port=5001, endpoint=\"/google\")\n\n    server_handle = await agent.serve_async(serving_config)\n    await server_handle.task\n\nif __name__ == \"__main__\":\n    parser=argparse.ArgumentParser()\n    parser.add_argument(\"protocol\", choices=[\"a2a\", \"mcp\"])\n    args = parser.parse_args()\n    asyncio.run(serve_agent(args.protocol))\n</code></pre> <pre><code># openai_expert.py\nimport argparse\nimport asyncio\n\nfrom any_agent import AgentConfig, AnyAgent\nfrom any_agent.tools import search_web\n\n\nasync def serve_agent(protocol):\n    agent = await AnyAgent.create_async(\n        \"openai\",\n        AgentConfig(\n            name=\"openai_expert\",\n            model_id=\"mistral:mistral-small-latest\",\n            instructions=\"You can answer questions about the OpenAI Agents SDK but nothing else.\",\n            description=\"An agent that can answer questions specifically and only about the OpenAI Agents SDK.\",\n            tools=[search_web]\n        )\n    )\n\n    if protocol == \"a2a\":\n        from any_agent.serving import A2AServingConfig\n        serving_config = A2AServingConfig(port=5002, endpoint=\"/openai\")\n    elif protocol == \"mcp\":\n        from any_agent.serving import MCPServingConfig\n        serving_config = MCPServingConfig(port=5002, endpoint=\"/openai\")\n\n    server_handle = await agent.serve_async(serving_config)\n    await server_handle.task\n\nif __name__ == \"__main__\":\n    parser=argparse.ArgumentParser()\n    parser.add_argument(\"protocol\", choices=[\"a2a\", \"mcp\"])\n    args = parser.parse_args()\n    asyncio.run(serve_agent(args.protocol))\n</code></pre> <p>We can now run each of the scripts in a separate terminal and leave them running in the background:</p> <pre><code>python google_expert.py mcp  ## or a2a\n</code></pre> <pre><code>python openai_expert.py a2a  ## or mcp\n</code></pre>"},{"location":"serving/#using-the-served-agents","title":"Using the served agents","text":"<p>Once the agents are being served using the chosen protocol, you can directly use them using the official clients of each protocol:</p> <ul> <li>A2A Client</li> <li>MCP Client</li> </ul> <p>Alternatively, as described in Using Agents-As-Tools, we can run another python script containing the main agent that can use the served agents:</p> <pre><code>import asyncio\n\nfrom any_agent import AgentConfig, AnyAgent\nfrom any_agent.config import MCPSse\nfrom any_agent.tools import a2a_tool_async\n\n\nasync def main():\n    prompt = \"What do you know about the Google ADK?\"\n\n    google_expert = MCPSse(\n        url=\"http://localhost:5001/google/sse\", client_session_timeout_seconds=300)\n    openai_expert = await a2a_tool_async(\n        url=\"http://localhost:5002/openai\")\n\n    main_agent = await AnyAgent.create_async(\n        \"tinyagent\",\n        AgentConfig(\n            model_id=\"gemini/gemini-2.5-pro\",\n            instructions=\"You must use the available tools to answer.\",\n            tools=[google_expert, openai_expert],\n        )\n    )\n\n    await main_agent.run_async(prompt)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"serving/#more-examples","title":"More Examples","text":"<p>Check out our cookbook example for building and serving an agent via A2A:</p> <p>\ud83d\udc49 Serve an Agent with A2A (Jupyter Notebook)</p> <p>\ud83d\udc49 Use an A2a Agent as a tool (Jupyter Notebook)</p>"},{"location":"tracing/","title":"Agent Tracing","text":"<p>An <code>AgentTrace</code> is returned when calling <code>agent.run</code> or <code>agent.run_async</code>.</p> <p><code>any-agent</code> generates standardized (regarding the structure) OpenTelemetry traces regardless of the framework used, based on the Semantic conventions for generative AI systems. This means that any OpenTelemetry exporter compatible with the Python SDK can be added. More information can be found below.</p> <p>Info</p> <p>Check the exposed attributes in <code>GenAI</code></p> <p>You can try to \ud83d\udd0d find \ud83d\udd0d the subtle differences (regarding the content) across frameworks in the examples below.</p> <p>Tip</p> <p>The following are real traces generated by executing one of our integration tests.</p>"},{"location":"tracing/#console","title":"Console","text":"<p>Here is what the console output looks like:</p> AGNOGOOGLELANGCHAINLLAMA_INDEXOPENAISMOLAGENTSTINYAGENT <p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 INPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"system\",                                                    \u2502 \u2502\n\u2502 \u2502     \"content\": \"&lt;instructions&gt;\\nUse the available tools to answer.\\n&lt;/in \u2502 \u2502\n\u2502 \u2502   },                                                                     \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"user\",                                                      \u2502 \u2502\n\u2502 \u2502     \"content\": \"Find what year it is in the America/New_York timezone an \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"get_current_time\",                                     \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{\\\"timezone\\\": \\\"America/New_York\\\"}\"                  \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 395,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 16,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 3.95e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 4.8e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: get_current_time \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\"                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\",                                        \u2502 \u2502\n\u2502 \u2502   \"datetime\": \"2025-09-16T08:43:15-04:00\",                               \u2502 \u2502\n\u2502 \u2502   \"is_dst\": true                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"write_file\",                                           \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{\\\"text\\\": \\\"2025\\\"}\"                                  \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 483,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 14,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 4.8299999999999995e-05,                                  \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 4.2e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: write_file \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"text\": \"2025\"                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {}                                                                       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"steps\": [                                                             \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 1,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Get current time in the America/New_York timezone\" \u2502 \u2502\n\u2502 \u2502     },                                                                   \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 2,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Write the year to a file\"                          \u2502 \u2502\n\u2502 \u2502     }                                                                    \u2502 \u2502\n\u2502 \u2502   ]                                                                      \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 518,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 44,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 5.18e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 1.3199999999999999e-05                                  \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> </p> <p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 INPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"system\",                                                    \u2502 \u2502\n\u2502 \u2502     \"content\": \"Use the available tools to answer.You must call the fina \u2502 \u2502\n\u2502 \u2502   },                                                                     \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"user\",                                                      \u2502 \u2502\n\u2502 \u2502     \"content\": \"Find what year it is in the America/New_York timezone an \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"get_current_time\",                                     \u2502 \u2502\n\u2502 \u2502     \"tool.args\": {                                                       \u2502 \u2502\n\u2502 \u2502       \"timezone\": \"America/New_York\"                                     \u2502 \u2502\n\u2502 \u2502     }                                                                    \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 672,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 16,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 6.72e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 4.8e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: get_current_time \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\"                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\",                                        \u2502 \u2502\n\u2502 \u2502   \"datetime\": \"2025-09-16T08:43:06-04:00\",                               \u2502 \u2502\n\u2502 \u2502   \"is_dst\": true                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"write_file\",                                           \u2502 \u2502\n\u2502 \u2502     \"tool.args\": {                                                       \u2502 \u2502\n\u2502 \u2502       \"text\": \"2025\"                                                     \u2502 \u2502\n\u2502 \u2502     }                                                                    \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 770,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 14,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 7.7e-05,                                                 \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 4.2e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: write_file \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"text\": \"2025\"                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {}                                                                       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"final_output\",                                         \u2502 \u2502\n\u2502 \u2502     \"tool.args\": {                                                       \u2502 \u2502\n\u2502 \u2502       \"answer\": \"{\\\"steps\\\": [{\\\"number\\\": 1, \\\"description\\\": \\\"Get cur \u2502 \u2502\n\u2502 \u2502     }                                                                    \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 809,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 56,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 8.09e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 1.68e-05                                                \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: final_output \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"answer\": \"{\\\"steps\\\": [{\\\"number\\\": 1, \\\"description\\\": \\\"Get current \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"success\": true,                                                       \u2502 \u2502\n\u2502 \u2502   \"result\": {                                                            \u2502 \u2502\n\u2502 \u2502     \"steps\": [                                                           \u2502 \u2502\n\u2502 \u2502       {                                                                  \u2502 \u2502\n\u2502 \u2502         \"number\": 1,                                                     \u2502 \u2502\n\u2502 \u2502         \"description\": \"Get current time in the America/New_York timezon \u2502 \u2502\n\u2502 \u2502       },                                                                 \u2502 \u2502\n\u2502 \u2502       {                                                                  \u2502 \u2502\n\u2502 \u2502         \"number\": 2,                                                     \u2502 \u2502\n\u2502 \u2502         \"description\": \"Write the year to a file.\"                       \u2502 \u2502\n\u2502 \u2502       }                                                                  \u2502 \u2502\n\u2502 \u2502     ]                                                                    \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> </p> <p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 INPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"system\",                                                    \u2502 \u2502\n\u2502 \u2502     \"content\": \"Use the available tools to answer.\"                      \u2502 \u2502\n\u2502 \u2502   },                                                                     \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"user\",                                                      \u2502 \u2502\n\u2502 \u2502     \"content\": \"Find what year it is in the America/New_York timezone an \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"get_current_time\",                                     \u2502 \u2502\n\u2502 \u2502     \"tool.args\": {                                                       \u2502 \u2502\n\u2502 \u2502       \"timezone\": \"America/New_York\"                                     \u2502 \u2502\n\u2502 \u2502     }                                                                    \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 245,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 16,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 2.45e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 4.8e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: get_current_time \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\"                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\",                                        \u2502 \u2502\n\u2502 \u2502   \"datetime\": \"2025-09-16T09:16:41-04:00\",                               \u2502 \u2502\n\u2502 \u2502   \"is_dst\": true                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"write_file\",                                           \u2502 \u2502\n\u2502 \u2502     \"tool.args\": {                                                       \u2502 \u2502\n\u2502 \u2502       \"text\": \"2025\"                                                     \u2502 \u2502\n\u2502 \u2502     }                                                                    \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 331,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 14,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 3.31e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 4.2e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: write_file \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"text\": \"2025\"                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 null                                                                     \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Here are the steps I have taken:                                         \u2502 \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502  1 Found the current time in the America/New_York timezone.              \u2502 \u2502\n\u2502 \u2502  2 Wrote the year to a file.                                             \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 366,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 33,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 3.6599999999999995e-05,                                  \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 9.9e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"steps\": [                                                             \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 1,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Found the current time in the America/New_York tim \u2502 \u2502\n\u2502 \u2502     },                                                                   \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 2,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Wrote the year to a file.\"                         \u2502 \u2502\n\u2502 \u2502     }                                                                    \u2502 \u2502\n\u2502 \u2502   ]                                                                      \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 320,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 62,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 3.2e-05,                                                 \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 1.8599999999999998e-05                                  \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> </p> <p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 INPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"system\",                                                    \u2502 \u2502\n\u2502 \u2502     \"content\": \"Use the available tools to answer.You must call the fina \u2502 \u2502\n\u2502 \u2502   },                                                                     \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"user\",                                                      \u2502 \u2502\n\u2502 \u2502     \"content\": \"Find what year it is in the America/New_York timezone an \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"get_current_time\",                                     \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{}\"                                                    \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 197,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 36,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 1.9699999999999998e-05,                                  \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 1.08e-05                                                \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: get_current_time \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\"                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\",                                        \u2502 \u2502\n\u2502 \u2502   \"datetime\": \"2025-09-16T09:14:59-04:00\",                               \u2502 \u2502\n\u2502 \u2502   \"is_dst\": true                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"write_file\",                                           \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{}\"                                                    \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 262,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 33,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 2.62e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 9.9e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: write_file \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"text\": \"2025\"                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \"None\"                                                                   \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"final_output\",                                         \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{}\"                                                    \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 285,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 94,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 2.8499999999999998e-05,                                  \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 2.8199999999999998e-05                                  \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: final_output \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"answer\": \"{\\\"steps\\\": [{\\\"number\\\": 1, \\\"description\\\": \\\"Get current \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"success\": true,                                                       \u2502 \u2502\n\u2502 \u2502   \"result\": {                                                            \u2502 \u2502\n\u2502 \u2502     \"steps\": [                                                           \u2502 \u2502\n\u2502 \u2502       {                                                                  \u2502 \u2502\n\u2502 \u2502         \"number\": 1,                                                     \u2502 \u2502\n\u2502 \u2502         \"description\": \"Get current time in the America/New_York timezon \u2502 \u2502\n\u2502 \u2502       },                                                                 \u2502 \u2502\n\u2502 \u2502       {                                                                  \u2502 \u2502\n\u2502 \u2502         \"number\": 2,                                                     \u2502 \u2502\n\u2502 \u2502         \"description\": \"Write the year to a file.\"                       \u2502 \u2502\n\u2502 \u2502       }                                                                  \u2502 \u2502\n\u2502 \u2502     ]                                                                    \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Here is the list of steps I have taken:                                  \u2502 \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502  1 Get current time in the America/New_York timezone.                    \u2502 \u2502\n\u2502 \u2502  2 Write the year to a file.                                             \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 403,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 32,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 4.03e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 9.6e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"steps\": [                                                             \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 1,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Get current time in the America/New_York timezone. \u2502 \u2502\n\u2502 \u2502     },                                                                   \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 2,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Write the year to a file.\"                         \u2502 \u2502\n\u2502 \u2502     }                                                                    \u2502 \u2502\n\u2502 \u2502   ]                                                                      \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 161,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 60,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 1.61e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 1.8e-05                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> </p> <p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 INPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"system\",                                                    \u2502 \u2502\n\u2502 \u2502     \"content\": \"Use the available tools to answer.\"                      \u2502 \u2502\n\u2502 \u2502   },                                                                     \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"content\": \"Find what year it is in the America/New_York timezone an \u2502 \u2502\n\u2502 \u2502     \"role\": \"user\"                                                       \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"get_current_time\",                                     \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{\\\"timezone\\\": \\\"America/New_York\\\"}\"                  \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 269,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 16,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 2.69e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 4.8e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: get_current_time \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\"                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\",                                        \u2502 \u2502\n\u2502 \u2502   \"datetime\": \"2025-09-16T08:43:13-04:00\",                               \u2502 \u2502\n\u2502 \u2502   \"is_dst\": true                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"write_file\",                                           \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{\\\"text\\\": \\\"2025\\\"}\"                                  \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 359,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 14,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 3.59e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 4.2e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: write_file \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"text\": \"2025\"                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {}                                                                       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"steps\": [                                                             \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 1,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Get current time in the America/New_York timezone. \u2502 \u2502\n\u2502 \u2502     },                                                                   \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 2,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Write the year to a file.\"                         \u2502 \u2502\n\u2502 \u2502     }                                                                    \u2502 \u2502\n\u2502 \u2502   ]                                                                      \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 392,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 46,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 3.92e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 1.38e-05                                                \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> </p> <p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 INPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"system\",                                                    \u2502 \u2502\n\u2502 \u2502     \"content\": \"Use the available tools to answer.You must call the fina \u2502 \u2502\n\u2502 \u2502   },                                                                     \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"user\",                                                      \u2502 \u2502\n\u2502 \u2502     \"content\": \"New task:\\nFind what year it is in the America/New_York  \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"get_current_time\",                                     \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{\\\"timezone\\\": \\\"America/New_York\\\"}\"                  \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 677,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 16,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 6.769999999999999e-05,                                   \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 4.8e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: get_current_time \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\"                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\",                                        \u2502 \u2502\n\u2502 \u2502   \"datetime\": \"2025-09-16T08:43:20-04:00\",                               \u2502 \u2502\n\u2502 \u2502   \"is_dst\": true                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"write_file\",                                           \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{\\\"text\\\": \\\"2025\\\"}\"                                  \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 782,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 14,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 7.819999999999999e-05,                                   \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 4.2e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: write_file \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"text\": \"2025\"                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {}                                                                       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"final_answer\",                                         \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{\\\"answer\\\": \\\"{\\\\\"steps\\\\\": [{\\\\\"number\\\\\": 1, \\ \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 835,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 57,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 8.35e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 1.71e-05                                                \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: final_answer \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"answer\": \"{\\\"steps\\\": [{\\\"number\\\": 1, \\\"description\\\": \\\"Get the cur \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"steps\": [                                                             \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 1,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Get the current time in the America/New_York timez \u2502 \u2502\n\u2502 \u2502     },                                                                   \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 2,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Write the year to a file.\"                         \u2502 \u2502\n\u2502 \u2502     }                                                                    \u2502 \u2502\n\u2502 \u2502   ]                                                                      \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> </p> <p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 INPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"system\",                                                    \u2502 \u2502\n\u2502 \u2502     \"content\": \"Use the available tools to answer.\"                      \u2502 \u2502\n\u2502 \u2502   },                                                                     \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"role\": \"user\",                                                      \u2502 \u2502\n\u2502 \u2502     \"content\": \"Find what year it is in the America/New_York timezone an \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"get_current_time\",                                     \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{\\\"timezone\\\": \\\"America/New_York\\\"}\"                  \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 328,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 16,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 3.28e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 4.8e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: get_current_time \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\"                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"timezone\": \"America/New_York\",                                        \u2502 \u2502\n\u2502 \u2502   \"datetime\": \"2025-09-16T08:43:21-04:00\",                               \u2502 \u2502\n\u2502 \u2502   \"is_dst\": true                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"write_file\",                                           \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{\\\"text\\\": \\\"2025\\\"}\"                                  \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 414,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 14,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 4.14e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 4.2e-06                                                 \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: write_file \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"text\": \"2025\"                                                         \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 None                                                                     \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 [                                                                        \u2502 \u2502\n\u2502 \u2502   {                                                                      \u2502 \u2502\n\u2502 \u2502     \"tool.name\": \"final_answer\",                                         \u2502 \u2502\n\u2502 \u2502     \"tool.args\": \"{\\\"answer\\\": \\\"1. Get current time in the America/New_ \u2502 \u2502\n\u2502 \u2502   }                                                                      \u2502 \u2502\n\u2502 \u2502 ]                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 449,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 43,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 4.49e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 1.29e-05                                                \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 EXECUTE_TOOL: final_answer \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"answer\": \"1. Get current time in the America/New_York timezone. 2. Wr \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502  1 Get current time in the America/New_York timezone. 2. Write the year  \u2502 \u2502\n\u2502 \u2502    to a file. 3. Return the list of steps taken.                         \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CALL_LLM: mistral/mistral-small-latest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 OUTPUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"steps\": [                                                             \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 1,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Get current time in the America/New_York timezone. \u2502 \u2502\n\u2502 \u2502     },                                                                   \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 2,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Write the year to a file.\"                         \u2502 \u2502\n\u2502 \u2502     },                                                                   \u2502 \u2502\n\u2502 \u2502     {                                                                    \u2502 \u2502\n\u2502 \u2502       \"number\": 3,                                                       \u2502 \u2502\n\u2502 \u2502       \"description\": \"Return the list of steps taken.\"                   \u2502 \u2502\n\u2502 \u2502     }                                                                    \u2502 \u2502\n\u2502 \u2502   ]                                                                      \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u256d\u2500 USAGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 {                                                                        \u2502 \u2502\n\u2502 \u2502   \"input_tokens\": 178,                                                   \u2502 \u2502\n\u2502 \u2502   \"output_tokens\": 83,                                                   \u2502 \u2502\n\u2502 \u2502   \"input_cost\": 1.78e-05,                                                \u2502 \u2502\n\u2502 \u2502   \"output_cost\": 2.49e-05                                                \u2502 \u2502\n\u2502 \u2502 }                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> </p> <p>Tip</p> <p>The spans are printed to the console by default, using the <code>callback</code> mechanism.</p> <p>See Default Callbacks for more information and how to disable this behavior.</p>"},{"location":"tracing/#spans","title":"Spans","text":"<p>Here's what the returned <code>agent_trace.spans</code> look like when dumped to JSON format:</p> AGNOGOOGLELANGCHAINLLAMA_INDEXOPENAISMOLAGENTSTINYAGENT <pre><code>{\n  \"spans\": [\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 39712373237291850001929057769826244787,\n        \"span_id\": 2795294992452118289,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026594777382000,\n      \"end_time\": 1758026595108941000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 39712373237291850001929057769826244787,\n        \"span_id\": 1807164324042463798,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.input.messages\": \"[{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"&lt;instructions&gt;\\\\nUse the available tools to answer.\\\\n&lt;/instructions&gt;\\\\n\\\\nProvide your output as a JSON containing the following fields:\\\\n&lt;json_fields&gt;\\\\n[\\\\\\\"steps\\\\\\\"]\\\\n&lt;/json_fields&gt;\\\\n\\\\nHere are the properties for each field:\\\\n&lt;json_field_properties&gt;\\\\n{\\\\n  \\\\\\\"steps\\\\\\\": {\\\\n    \\\\\\\"items\\\\\\\": {\\\\n      \\\\\\\"$ref\\\\\\\": \\\\\\\"#/$defs/Step\\\\\\\"\\\\n    },\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"array\\\\\\\"\\\\n  },\\\\n  \\\\\\\"$defs\\\\\\\": {\\\\n    \\\\\\\"Step\\\\\\\": {\\\\n      \\\\\\\"number\\\\\\\": {\\\\n        \\\\\\\"type\\\\\\\": \\\\\\\"integer\\\\\\\"\\\\n      },\\\\n      \\\\\\\"description\\\\\\\": {\\\\n        \\\\\\\"type\\\\\\\": \\\\\\\"string\\\\\\\"\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\n&lt;/json_field_properties&gt;\\\\nStart your response with `{` and end it with `}`.\\\\nYour output will be passed to json.loads() to convert it to a Python object.\\\\nMake sure it only contains valid JSON.\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Find what year it is in the America/New_York timezone and write the value (single number) to a file. Finally, return a list of the steps you have taken.\\\"}]\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"get_current_time\\\", \\\"tool.args\\\": \\\"{\\\\\\\"timezone\\\\\\\": \\\\\\\"America/New_York\\\\\\\"}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 395,\n        \"gen_ai.usage.output_tokens\": 16,\n        \"gen_ai.usage.input_cost\": 0.0000395,\n        \"gen_ai.usage.output_cost\": 4.8e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool get_current_time\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 39712373237291850001929057769826244787,\n        \"span_id\": 2795294992452118289,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026595109197000,\n      \"end_time\": 1758026595112362000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 39712373237291850001929057769826244787,\n        \"span_id\": 10363380192615184096,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"get_current_time\",\n        \"gen_ai.tool.description\": \"Get current time in a specific timezones\",\n        \"gen_ai.tool.args\": \"{\\\"timezone\\\": \\\"America/New_York\\\"}\",\n        \"gen_ai.tool.call.id\": \"LlAGPjfO0\",\n        \"gen_ai.output\": \"{\\n  \\\"timezone\\\": \\\"America/New_York\\\",\\n  \\\"datetime\\\": \\\"2025-09-16T08:43:15-04:00\\\",\\n  \\\"is_dst\\\": true\\n}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 39712373237291850001929057769826244787,\n        \"span_id\": 2795294992452118289,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026595112536000,\n      \"end_time\": 1758026595478990000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 39712373237291850001929057769826244787,\n        \"span_id\": 5687879115964223106,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"write_file\\\", \\\"tool.args\\\": \\\"{\\\\\\\"text\\\\\\\": \\\\\\\"2025\\\\\\\"}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 483,\n        \"gen_ai.usage.output_tokens\": 14,\n        \"gen_ai.usage.input_cost\": 0.000048299999999999995,\n        \"gen_ai.usage.output_cost\": 4.2e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool write_file\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 39712373237291850001929057769826244787,\n        \"span_id\": 2795294992452118289,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026595479170000,\n      \"end_time\": 1758026595479979000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 39712373237291850001929057769826244787,\n        \"span_id\": 10172050132064111684,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"write_file\",\n        \"gen_ai.tool.description\": \"write the text to a file in the tmp_path directory\",\n        \"gen_ai.tool.args\": \"{\\\"text\\\": \\\"2025\\\"}\",\n        \"gen_ai.tool.call.id\": \"wzCUjvRq4\",\n        \"gen_ai.output\": \"{}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 39712373237291850001929057769826244787,\n        \"span_id\": 2795294992452118289,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026595480140000,\n      \"end_time\": 1758026596171980000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 39712373237291850001929057769826244787,\n        \"span_id\": 5885670640581518449,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"{\\\"steps\\\": [{\\\"number\\\": 1, \\\"description\\\": \\\"Get current time in the America/New_York timezone\\\"}, {\\\"number\\\": 2, \\\"description\\\": \\\"Write the year to a file\\\"}]}\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 518,\n        \"gen_ai.usage.output_tokens\": 44,\n        \"gen_ai.usage.input_cost\": 0.0000518,\n        \"gen_ai.usage.output_cost\": 0.000013199999999999999\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"invoke_agent [any_agent]\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": null,\n        \"span_id\": null,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026594771631000,\n      \"end_time\": 1758026599652409000,\n      \"status\": {\n        \"status_code\": \"unset\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 39712373237291850001929057769826244787,\n        \"span_id\": 2795294992452118289,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"invoke_agent\",\n        \"gen_ai.agent.name\": \"any_agent\",\n        \"gen_ai.agent.description\": \"No description.\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"any_agent.version\": \"1.9.1.dev0+g8e34f9a88.d20250915\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    }\n  ],\n  \"final_output\": {\n    \"steps\": [\n      {\n        \"number\": 1,\n        \"description\": \"Get current time in the America/New_York timezone\"\n      },\n      {\n        \"number\": 2,\n        \"description\": \"Write the year to a file\"\n      }\n    ]\n  }\n}\n</code></pre> <pre><code>{\n  \"spans\": [\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 17348474965005769013,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026586341103000,\n      \"end_time\": 1758026586853189000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 2932848494077650092,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.input.messages\": \"[{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"Use the available tools to answer.You must call the final_output tool when finished.The 'answer' argument passed to the final_output tool must be a JSON string that matches the following schema:\\\\n{'$defs': {'Step': {'properties': {'number': {'title': 'Number', 'type': 'integer'}, 'description': {'title': 'Description', 'type': 'string'}}, 'required': ['number', 'description'], 'title': 'Step', 'type': 'object'}}, 'properties': {'steps': {'items': {'$ref': '#/$defs/Step'}, 'title': 'Steps', 'type': 'array'}}, 'required': ['steps'], 'title': 'Steps', 'type': 'object'}\\\\n\\\\nYou are an agent. Your internal name is \\\\\\\"any_agent\\\\\\\".\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Find what year it is in the America/New_York timezone and write the value (single number) to a file. Finally, return a list of the steps you have taken.\\\"}]\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"get_current_time\\\", \\\"tool.args\\\": {\\\"timezone\\\": \\\"America/New_York\\\"}}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 672,\n        \"gen_ai.usage.output_tokens\": 16,\n        \"gen_ai.usage.input_cost\": 0.0000672,\n        \"gen_ai.usage.output_cost\": 4.8e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool get_current_time\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 16887867178883245156,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026586853654000,\n      \"end_time\": 1758026586857290000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 14962216559073945637,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"get_current_time\",\n        \"gen_ai.tool.description\": \"Get current time in a specific timezones\\n\\nArgs:\\n    timezone: IANA timezone name (e.g., 'America/New_York', 'Europe/London'). Use 'America/New_York' as local timezone if no timezone provided by the user.\",\n        \"gen_ai.tool.args\": \"{\\\"timezone\\\": \\\"America/New_York\\\"}\",\n        \"gen_ai.tool.call.id\": \"jbUWqgIUr\",\n        \"gen_ai.output\": \"{\\n  \\\"timezone\\\": \\\"America/New_York\\\",\\n  \\\"datetime\\\": \\\"2025-09-16T08:43:06-04:00\\\",\\n  \\\"is_dst\\\": true\\n}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 17348474965005769013,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026586857845000,\n      \"end_time\": 1758026587201860000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 5619252809477224583,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"write_file\\\", \\\"tool.args\\\": {\\\"text\\\": \\\"2025\\\"}}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 770,\n        \"gen_ai.usage.output_tokens\": 14,\n        \"gen_ai.usage.input_cost\": 0.000077,\n        \"gen_ai.usage.output_cost\": 4.2e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool write_file\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 10221318138555217448,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026587202619000,\n      \"end_time\": 1758026587204475000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 7349109554078691264,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"write_file\",\n        \"gen_ai.tool.description\": \"write the text to a file in the tmp_path directory\\n\\nArgs:\\n    text (str): The text to write to the file.\\n\\nReturns:\\n    None\",\n        \"gen_ai.tool.args\": \"{\\\"text\\\": \\\"2025\\\"}\",\n        \"gen_ai.tool.call.id\": \"j5pzf0TaM\",\n        \"gen_ai.output\": \"{}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 17348474965005769013,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026587207556000,\n      \"end_time\": 1758026587925393000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 6735089064917818756,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"final_output\\\", \\\"tool.args\\\": {\\\"answer\\\": \\\"{\\\\\\\"steps\\\\\\\": [{\\\\\\\"number\\\\\\\": 1, \\\\\\\"description\\\\\\\": \\\\\\\"Get current time in the America/New_York timezone.\\\\\\\"}, {\\\\\\\"number\\\\\\\": 2, \\\\\\\"description\\\\\\\": \\\\\\\"Write the year to a file.\\\\\\\"}]}\\\"}}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 809,\n        \"gen_ai.usage.output_tokens\": 56,\n        \"gen_ai.usage.input_cost\": 0.0000809,\n        \"gen_ai.usage.output_cost\": 0.0000168\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool final_output\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 7027657387019838282,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026587926056000,\n      \"end_time\": 1758026587928967000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 8045312650763315758,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"final_output\",\n        \"gen_ai.tool.description\": \"This tool is used to validate the final output. It must be called when the final answer is ready in order to ensure that the output is valid.\\n\\nArgs:\\n    answer: The final output that can be loaded as a Pydantic model. This must be a JSON compatible string that matches the following schema:\\n        {'$defs': {'Step': {'properties': {'number': {'title': 'Number', 'type': 'integer'}, 'description': {'title': 'Description', 'type': 'string'}}, 'required': ['number', 'description'], 'title': 'Step', 'type': 'object'}}, 'properties': {'steps': {'items': {'$ref': '#/$defs/Step'}, 'title': 'Steps', 'type': 'array'}}, 'required': ['steps'], 'title': 'Steps', 'type': 'object'}\\n\\nReturns:\\n    A dictionary with the following keys:\\n        - success: True if the output is valid, False otherwise.\\n        - result: The final output if success is True, otherwise an error message.\",\n        \"gen_ai.tool.args\": \"{\\\"answer\\\": \\\"{\\\\\\\"steps\\\\\\\": [{\\\\\\\"number\\\\\\\": 1, \\\\\\\"description\\\\\\\": \\\\\\\"Get current time in the America/New_York timezone.\\\\\\\"}, {\\\\\\\"number\\\\\\\": 2, \\\\\\\"description\\\\\\\": \\\\\\\"Write the year to a file.\\\\\\\"}]}\\\"}\",\n        \"gen_ai.tool.call.id\": \"Je755BQAS\",\n        \"gen_ai.output\": \"{\\\"success\\\": true, \\\"result\\\": {\\\"steps\\\": [{\\\"number\\\": 1, \\\"description\\\": \\\"Get current time in the America/New_York timezone.\\\"}, {\\\"number\\\": 2, \\\"description\\\": \\\"Write the year to a file.\\\"}]}}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"invoke_agent [any_agent]\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": null,\n        \"span_id\": null,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026586339976000,\n      \"end_time\": 1758026587931400000,\n      \"status\": {\n        \"status_code\": \"unset\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 273475590164507581703505203264503192396,\n        \"span_id\": 8588495004904668441,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"invoke_agent\",\n        \"gen_ai.agent.name\": \"any_agent\",\n        \"gen_ai.agent.description\": \"No description.\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"any_agent.version\": \"1.9.1.dev0+g8e34f9a88.d20250915\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    }\n  ],\n  \"final_output\": {\n    \"steps\": [\n      {\n        \"number\": 1,\n        \"description\": \"Get current time in the America/New_York timezone.\"\n      },\n      {\n        \"number\": 2,\n        \"description\": \"Write the year to a file.\"\n      }\n    ]\n  }\n}\n</code></pre> <pre><code>{\n  \"spans\": [\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 15531323892744358789,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028600965362000,\n      \"end_time\": 1758028601275757000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 12072964954527026452,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.input.messages\": \"[{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"Use the available tools to answer.\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Find what year it is in the America/New_York timezone and write the value (single number) to a file. Finally, return a list of the steps you have taken.\\\"}]\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"get_current_time\\\", \\\"tool.args\\\": {\\\"timezone\\\": \\\"America/New_York\\\"}}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 245,\n        \"gen_ai.usage.output_tokens\": 16,\n        \"gen_ai.usage.input_cost\": 0.0000245,\n        \"gen_ai.usage.output_cost\": 4.8e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool get_current_time\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 15531323892744358789,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028601277624000,\n      \"end_time\": 1758028601280591000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 17250783977442180996,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"get_current_time\",\n        \"gen_ai.tool.description\": \"Get current time in a specific timezones\\n\\nArgs:\\n    timezone: IANA timezone name (e.g., 'America/New_York', 'Europe/London'). Use 'America/New_York' as local timezone if no timezone provided by the user.\",\n        \"gen_ai.tool.args\": \"{\\\"timezone\\\": \\\"America/New_York\\\"}\",\n        \"gen_ai.output\": \"{\\n  \\\"timezone\\\": \\\"America/New_York\\\",\\n  \\\"datetime\\\": \\\"2025-09-16T09:16:41-04:00\\\",\\n  \\\"is_dst\\\": true\\n}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 15531323892744358789,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028601281821000,\n      \"end_time\": 1758028601516648000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 11409606448387202450,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"write_file\\\", \\\"tool.args\\\": {\\\"text\\\": \\\"2025\\\"}}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 331,\n        \"gen_ai.usage.output_tokens\": 14,\n        \"gen_ai.usage.input_cost\": 0.0000331,\n        \"gen_ai.usage.output_cost\": 4.2e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool write_file\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 15531323892744358789,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028601518149000,\n      \"end_time\": 1758028601519026000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 18344230493271694625,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"write_file\",\n        \"gen_ai.tool.description\": \"write the text to a file in the tmp_path directory\\n\\n        Args:\\n            text (str): The text to write to the file.\\n\\n        Returns:\\n            None\",\n        \"gen_ai.tool.args\": \"{\\\"text\\\": \\\"2025\\\"}\",\n        \"gen_ai.output\": \"null\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 15531323892744358789,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028601520276000,\n      \"end_time\": 1758028602113531000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 3673965111321899285,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"Here are the steps I have taken:\\n\\n1. Found the current time in the America/New_York timezone.\\n2. Wrote the year to a file.\",\n        \"gen_ai.output.type\": \"text\",\n        \"gen_ai.usage.input_tokens\": 366,\n        \"gen_ai.usage.output_tokens\": 33,\n        \"gen_ai.usage.input_cost\": 0.000036599999999999995,\n        \"gen_ai.usage.output_cost\": 9.9e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 15531323892744358789,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028602115388000,\n      \"end_time\": 1758028602753586000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 17360679537864909106,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"{\\n  \\\"steps\\\": [\\n    {\\n      \\\"number\\\": 1,\\n      \\\"description\\\": \\\"Found the current time in the America/New_York timezone.\\\"\\n    },\\n    {\\n      \\\"number\\\": 2,\\n      \\\"description\\\": \\\"Wrote the year to a file.\\\"\\n    }\\n  ]\\n}\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 320,\n        \"gen_ai.usage.output_tokens\": 62,\n        \"gen_ai.usage.input_cost\": 0.000032,\n        \"gen_ai.usage.output_cost\": 0.000018599999999999998\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"invoke_agent [any_agent]\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": null,\n        \"span_id\": null,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028600960730000,\n      \"end_time\": 1758028602753668000,\n      \"status\": {\n        \"status_code\": \"unset\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 115825058289444712299650141807529952578,\n        \"span_id\": 15531323892744358789,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"invoke_agent\",\n        \"gen_ai.agent.name\": \"any_agent\",\n        \"gen_ai.agent.description\": \"No description.\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"any_agent.version\": \"1.9.1.dev3+g8bf65aae1.d20250916\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    }\n  ],\n  \"final_output\": {\n    \"steps\": [\n      {\n        \"number\": 1,\n        \"description\": \"Found the current time in the America/New_York timezone.\"\n      },\n      {\n        \"number\": 2,\n        \"description\": \"Wrote the year to a file.\"\n      }\n    ]\n  }\n}\n</code></pre> <pre><code>{\n  \"spans\": [\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 12253070288137891175,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028498931090000,\n      \"end_time\": 1758028499914934000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 4679246220570242795,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.input.messages\": \"[{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"Use the available tools to answer.You must call the final_output tool when finished.The 'answer' argument passed to the final_output tool must be a JSON string that matches the following schema:\\\\n{'$defs': {'Step': {'properties': {'number': {'title': 'Number', 'type': 'integer'}, 'description': {'title': 'Description', 'type': 'string'}}, 'required': ['number', 'description'], 'title': 'Step', 'type': 'object'}}, 'properties': {'steps': {'items': {'$ref': '#/$defs/Step'}, 'title': 'Steps', 'type': 'array'}}, 'required': ['steps'], 'title': 'Steps', 'type': 'object'}\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Find what year it is in the America/New_York timezone and write the value (single number) to a file. Finally, return a list of the steps you have taken.\\\"}]\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"get_current_time\\\", \\\"tool.args\\\": \\\"{}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 197,\n        \"gen_ai.usage.output_tokens\": 36,\n        \"gen_ai.usage.input_cost\": 0.000019699999999999998,\n        \"gen_ai.usage.output_cost\": 0.0000108\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool get_current_time\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 12253070288137891175,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028499915928000,\n      \"end_time\": 1758028499919034000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 10228776255392323169,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"get_current_time\",\n        \"gen_ai.tool.description\": \"get_current_time(*, timezone: str) -&gt; str\\nGet current time in a specific timezones\",\n        \"gen_ai.tool.args\": \"{\\\"timezone\\\": \\\"America/New_York\\\"}\",\n        \"gen_ai.output\": \"{\\n  \\\"timezone\\\": \\\"America/New_York\\\",\\n  \\\"datetime\\\": \\\"2025-09-16T09:14:59-04:00\\\",\\n  \\\"is_dst\\\": true\\n}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 12253070288137891175,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028499920240000,\n      \"end_time\": 1758028500146680000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 9269503876317208151,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"write_file\\\", \\\"tool.args\\\": \\\"{}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 262,\n        \"gen_ai.usage.output_tokens\": 33,\n        \"gen_ai.usage.input_cost\": 0.0000262,\n        \"gen_ai.usage.output_cost\": 9.9e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool write_file\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 12253070288137891175,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028500147635000,\n      \"end_time\": 1758028500148457000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 15403562707434381698,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"write_file\",\n        \"gen_ai.tool.description\": \"write_file(text: str) -&gt; None\\nwrite the text to a file in the tmp_path directory\\n:param text: The text to write to the file.\",\n        \"gen_ai.tool.args\": \"{\\\"text\\\": \\\"2025\\\"}\",\n        \"gen_ai.output\": \"\\\"None\\\"\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 12253070288137891175,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028500149729000,\n      \"end_time\": 1758028500732051000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 1726412464743061000,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"final_output\\\", \\\"tool.args\\\": \\\"{}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 285,\n        \"gen_ai.usage.output_tokens\": 94,\n        \"gen_ai.usage.input_cost\": 0.000028499999999999998,\n        \"gen_ai.usage.output_cost\": 0.000028199999999999998\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool final_output\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 12253070288137891175,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028500733145000,\n      \"end_time\": 1758028500734292000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 18066273280650095232,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"final_output\",\n        \"gen_ai.tool.description\": \"final_output(answer: str) -&gt; dict[str, str | bool | dict[str, typing.Any] | list[typing.Any]]\\nThis tool is used to validate the final output. It must be called when the final answer is ready in order to ensure that the output is valid.\",\n        \"gen_ai.tool.args\": \"{\\\"answer\\\": \\\"{\\\\\\\"steps\\\\\\\": [{\\\\\\\"number\\\\\\\": 1, \\\\\\\"description\\\\\\\": \\\\\\\"Get current time in the America/New_York timezone.\\\\\\\"}, {\\\\\\\"number\\\\\\\": 2, \\\\\\\"description\\\\\\\": \\\\\\\"Write the year to a file.\\\\\\\"}]}\\\"}\",\n        \"gen_ai.output\": \"{\\\"success\\\": true, \\\"result\\\": {\\\"steps\\\": [{\\\"number\\\": 1, \\\"description\\\": \\\"Get current time in the America/New_York timezone.\\\"}, {\\\"number\\\": 2, \\\"description\\\": \\\"Write the year to a file.\\\"}]}}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 12253070288137891175,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028500735645000,\n      \"end_time\": 1758028502141305000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 6597376107037947816,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"Here is the list of steps I have taken:\\n\\n1. Get current time in the America/New_York timezone.\\n2. Write the year to a file.\",\n        \"gen_ai.output.type\": \"text\",\n        \"gen_ai.usage.input_tokens\": 403,\n        \"gen_ai.usage.output_tokens\": 32,\n        \"gen_ai.usage.input_cost\": 0.0000403,\n        \"gen_ai.usage.output_cost\": 9.6e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 12253070288137891175,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028502143530000,\n      \"end_time\": 1758028502855627000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 15563885816911253146,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"{\\n  \\\"steps\\\": [\\n    {\\n      \\\"number\\\": 1,\\n      \\\"description\\\": \\\"Get current time in the America/New_York timezone.\\\"\\n    },\\n    {\\n      \\\"number\\\": 2,\\n      \\\"description\\\": \\\"Write the year to a file.\\\"\\n    }\\n  ]\\n}\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 161,\n        \"gen_ai.usage.output_tokens\": 60,\n        \"gen_ai.usage.input_cost\": 0.0000161,\n        \"gen_ai.usage.output_cost\": 0.000018\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"invoke_agent [any_agent]\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": null,\n        \"span_id\": null,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758028498928802000,\n      \"end_time\": 1758028502855731000,\n      \"status\": {\n        \"status_code\": \"unset\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 183122279777172322684744876400756953563,\n        \"span_id\": 12253070288137891175,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"invoke_agent\",\n        \"gen_ai.agent.name\": \"any_agent\",\n        \"gen_ai.agent.description\": \"No description.\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"any_agent.version\": \"1.9.1.dev3+g8bf65aae1.d20250916\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.37.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    }\n  ],\n  \"final_output\": {\n    \"steps\": [\n      {\n        \"number\": 1,\n        \"description\": \"Get current time in the America/New_York timezone.\"\n      },\n      {\n        \"number\": 2,\n        \"description\": \"Write the year to a file.\"\n      }\n    ]\n  }\n}\n</code></pre> <pre><code>{\n  \"spans\": [\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 100927429609276267646756448943279827607,\n        \"span_id\": 12324293800750531911,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026593210770000,\n      \"end_time\": 1758026593449611000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 100927429609276267646756448943279827607,\n        \"span_id\": 9295669169508793927,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.input.messages\": \"[{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"Use the available tools to answer.\\\"}, {\\\"content\\\": \\\"Find what year it is in the America/New_York timezone and write the value (single number) to a file. Finally, return a list of the steps you have taken.\\\", \\\"role\\\": \\\"user\\\"}]\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"get_current_time\\\", \\\"tool.args\\\": \\\"{\\\\\\\"timezone\\\\\\\": \\\\\\\"America/New_York\\\\\\\"}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 269,\n        \"gen_ai.usage.output_tokens\": 16,\n        \"gen_ai.usage.input_cost\": 0.0000269,\n        \"gen_ai.usage.output_cost\": 4.8e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool get_current_time\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 100927429609276267646756448943279827607,\n        \"span_id\": 12324293800750531911,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026593450406000,\n      \"end_time\": 1758026593452926000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 100927429609276267646756448943279827607,\n        \"span_id\": 13687147528289423029,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"get_current_time\",\n        \"gen_ai.tool.description\": \"Get current time in a specific timezones\",\n        \"gen_ai.tool.args\": \"{\\\"timezone\\\": \\\"America/New_York\\\"}\",\n        \"gen_ai.output\": \"{\\n  \\\"timezone\\\": \\\"America/New_York\\\",\\n  \\\"datetime\\\": \\\"2025-09-16T08:43:13-04:00\\\",\\n  \\\"is_dst\\\": true\\n}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 100927429609276267646756448943279827607,\n        \"span_id\": 12324293800750531911,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026593453682000,\n      \"end_time\": 1758026593767325000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 100927429609276267646756448943279827607,\n        \"span_id\": 1954108595464389762,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"write_file\\\", \\\"tool.args\\\": \\\"{\\\\\\\"text\\\\\\\": \\\\\\\"2025\\\\\\\"}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 359,\n        \"gen_ai.usage.output_tokens\": 14,\n        \"gen_ai.usage.input_cost\": 0.0000359,\n        \"gen_ai.usage.output_cost\": 4.2e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool write_file\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 100927429609276267646756448943279827607,\n        \"span_id\": 12324293800750531911,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026593769194000,\n      \"end_time\": 1758026593771373000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 100927429609276267646756448943279827607,\n        \"span_id\": 3402142098134531952,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"write_file\",\n        \"gen_ai.tool.description\": \"write the text to a file in the tmp_path directory\",\n        \"gen_ai.tool.args\": \"{\\\"text\\\": \\\"2025\\\"}\",\n        \"gen_ai.output\": \"{}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 100927429609276267646756448943279827607,\n        \"span_id\": 12324293800750531911,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026593773377000,\n      \"end_time\": 1758026594435103000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 100927429609276267646756448943279827607,\n        \"span_id\": 10907162358052518539,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"{\\\"steps\\\": [{\\\"number\\\": 1, \\\"description\\\": \\\"Get current time in the America/New_York timezone.\\\"}, {\\\"number\\\": 2, \\\"description\\\": \\\"Write the year to a file.\\\"}]}\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 392,\n        \"gen_ai.usage.output_tokens\": 46,\n        \"gen_ai.usage.input_cost\": 0.0000392,\n        \"gen_ai.usage.output_cost\": 0.0000138\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"invoke_agent [any_agent]\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": null,\n        \"span_id\": null,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026593209236000,\n      \"end_time\": 1758026594436486000,\n      \"status\": {\n        \"status_code\": \"unset\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 100927429609276267646756448943279827607,\n        \"span_id\": 12324293800750531911,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"invoke_agent\",\n        \"gen_ai.agent.name\": \"any_agent\",\n        \"gen_ai.agent.description\": \"No description.\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"any_agent.version\": \"1.9.1.dev0+g8e34f9a88.d20250915\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    }\n  ],\n  \"final_output\": {\n    \"steps\": [\n      {\n        \"number\": 1,\n        \"description\": \"Get current time in the America/New_York timezone.\"\n      },\n      {\n        \"number\": 2,\n        \"description\": \"Write the year to a file.\"\n      }\n    ]\n  }\n}\n</code></pre> <pre><code>{\n  \"spans\": [\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 2377815918452923285,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026599903043000,\n      \"end_time\": 1758026600205276000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 5217401444604234849,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.input.messages\": \"[{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"Use the available tools to answer.You must call the final_output tool when finished.The 'answer' argument passed to the final_output tool must be a JSON string that matches the following schema:\\\\n{'$defs': {'Step': {'properties': {'number': {'title': 'Number', 'type': 'integer'}, 'description': {'title': 'Description', 'type': 'string'}}, 'required': ['number', 'description'], 'title': 'Step', 'type': 'object'}}, 'properties': {'steps': {'items': {'$ref': '#/$defs/Step'}, 'title': 'Steps', 'type': 'array'}}, 'required': ['steps'], 'title': 'Steps', 'type': 'object'}\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"New task:\\\\nFind what year it is in the America/New_York timezone and write the value (single number) to a file. Finally, return a list of the steps you have taken.\\\"}]\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"get_current_time\\\", \\\"tool.args\\\": \\\"{\\\\\\\"timezone\\\\\\\": \\\\\\\"America/New_York\\\\\\\"}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 677,\n        \"gen_ai.usage.output_tokens\": 16,\n        \"gen_ai.usage.input_cost\": 0.00006769999999999999,\n        \"gen_ai.usage.output_cost\": 4.8e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool get_current_time\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 2377815918452923285,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026600206847000,\n      \"end_time\": 1758026600209406000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 6362149822438609756,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"get_current_time\",\n        \"gen_ai.tool.description\": \"Get current time in a specific timezones\",\n        \"gen_ai.tool.args\": \"{\\\"timezone\\\": \\\"America/New_York\\\"}\",\n        \"gen_ai.output\": \"{\\n  \\\"timezone\\\": \\\"America/New_York\\\",\\n  \\\"datetime\\\": \\\"2025-09-16T08:43:20-04:00\\\",\\n  \\\"is_dst\\\": true\\n}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 2377815918452923285,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026600209578000,\n      \"end_time\": 1758026600499567000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 1718758194134576853,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"write_file\\\", \\\"tool.args\\\": \\\"{\\\\\\\"text\\\\\\\": \\\\\\\"2025\\\\\\\"}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 782,\n        \"gen_ai.usage.output_tokens\": 14,\n        \"gen_ai.usage.input_cost\": 0.00007819999999999999,\n        \"gen_ai.usage.output_cost\": 4.2e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool write_file\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 2377815918452923285,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026600500018000,\n      \"end_time\": 1758026600502243000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 15036084660665439760,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"write_file\",\n        \"gen_ai.tool.description\": \"write the text to a file in the tmp_path directory\",\n        \"gen_ai.tool.args\": \"{\\\"text\\\": \\\"2025\\\"}\",\n        \"gen_ai.output\": \"{}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 2377815918452923285,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026600502982000,\n      \"end_time\": 1758026601057378000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 14876458401309029265,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"final_answer\\\", \\\"tool.args\\\": \\\"{\\\\\\\"answer\\\\\\\": \\\\\\\"{\\\\\\\\\\\\\\\"steps\\\\\\\\\\\\\\\": [{\\\\\\\\\\\\\\\"number\\\\\\\\\\\\\\\": 1, \\\\\\\\\\\\\\\"description\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Get the current time in the America/New_York timezone.\\\\\\\\\\\\\\\"}, {\\\\\\\\\\\\\\\"number\\\\\\\\\\\\\\\": 2, \\\\\\\\\\\\\\\"description\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Write the year to a file.\\\\\\\\\\\\\\\"}]}\\\\\\\"}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 835,\n        \"gen_ai.usage.output_tokens\": 57,\n        \"gen_ai.usage.input_cost\": 0.0000835,\n        \"gen_ai.usage.output_cost\": 0.0000171\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool final_answer\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 2377815918452923285,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026601057895000,\n      \"end_time\": 1758026601060767000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 13738272699182380164,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"final_answer\",\n        \"gen_ai.tool.description\": \"Provides a final answer to the given problem.\",\n        \"gen_ai.tool.args\": \"{\\\"answer\\\": \\\"{\\\\\\\"steps\\\\\\\": [{\\\\\\\"number\\\\\\\": 1, \\\\\\\"description\\\\\\\": \\\\\\\"Get the current time in the America/New_York timezone.\\\\\\\"}, {\\\\\\\"number\\\\\\\": 2, \\\\\\\"description\\\\\\\": \\\\\\\"Write the year to a file.\\\\\\\"}]}\\\"}\",\n        \"gen_ai.output\": \"{\\\"steps\\\": [{\\\"number\\\": 1, \\\"description\\\": \\\"Get the current time in the America/New_York timezone.\\\"}, {\\\"number\\\": 2, \\\"description\\\": \\\"Write the year to a file.\\\"}]}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"invoke_agent [any_agent]\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": null,\n        \"span_id\": null,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026599902637000,\n      \"end_time\": 1758026601061025000,\n      \"status\": {\n        \"status_code\": \"unset\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 193014249579837554382251753808266698816,\n        \"span_id\": 2377815918452923285,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"invoke_agent\",\n        \"gen_ai.agent.name\": \"any_agent\",\n        \"gen_ai.agent.description\": \"No description.\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"any_agent.version\": \"1.9.1.dev0+g8e34f9a88.d20250915\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    }\n  ],\n  \"final_output\": {\n    \"steps\": [\n      {\n        \"number\": 1,\n        \"description\": \"Get the current time in the America/New_York timezone.\"\n      },\n      {\n        \"number\": 2,\n        \"description\": \"Write the year to a file.\"\n      }\n    ]\n  }\n}\n</code></pre> <pre><code>{\n  \"spans\": [\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 10398286333944367643,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026601289496000,\n      \"end_time\": 1758026601631557000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 17462846557343803630,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.input.messages\": \"[{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"Use the available tools to answer.\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Find what year it is in the America/New_York timezone and write the value (single number) to a file. Finally, return a list of the steps you have taken.\\\"}]\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"get_current_time\\\", \\\"tool.args\\\": \\\"{\\\\\\\"timezone\\\\\\\": \\\\\\\"America/New_York\\\\\\\"}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 328,\n        \"gen_ai.usage.output_tokens\": 16,\n        \"gen_ai.usage.input_cost\": 0.0000328,\n        \"gen_ai.usage.output_cost\": 4.8e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool get_current_time\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 10398286333944367643,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026601631711000,\n      \"end_time\": 1758026601634284000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 12552362084418536424,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"get_current_time\",\n        \"gen_ai.tool.args\": \"{\\\"timezone\\\": \\\"America/New_York\\\"}\",\n        \"gen_ai.output\": \"{\\n  \\\"timezone\\\": \\\"America/New_York\\\",\\n  \\\"datetime\\\": \\\"2025-09-16T08:43:21-04:00\\\",\\n  \\\"is_dst\\\": true\\n}\",\n        \"gen_ai.output.type\": \"json\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 10398286333944367643,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026601634336000,\n      \"end_time\": 1758026602963917000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 9395046233243236617,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"write_file\\\", \\\"tool.args\\\": \\\"{\\\\\\\"text\\\\\\\": \\\\\\\"2025\\\\\\\"}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 414,\n        \"gen_ai.usage.output_tokens\": 14,\n        \"gen_ai.usage.input_cost\": 0.0000414,\n        \"gen_ai.usage.output_cost\": 4.2e-6\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool write_file\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 10398286333944367643,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026602964206000,\n      \"end_time\": 1758026602967081000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 13094185588603238916,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"write_file\",\n        \"gen_ai.tool.args\": \"{\\\"text\\\": \\\"2025\\\"}\",\n        \"gen_ai.output\": \"None\",\n        \"gen_ai.output.type\": \"text\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 10398286333944367643,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026602967229000,\n      \"end_time\": 1758026603465838000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 12817121744145091219,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"[{\\\"tool.name\\\": \\\"final_answer\\\", \\\"tool.args\\\": \\\"{\\\\\\\"answer\\\\\\\": \\\\\\\"1. Get current time in the America/New_York timezone. 2. Write the year to a file. 3. Return the list of steps taken.\\\\\\\"}\\\"}]\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 449,\n        \"gen_ai.usage.output_tokens\": 43,\n        \"gen_ai.usage.input_cost\": 0.0000449,\n        \"gen_ai.usage.output_cost\": 0.0000129\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"execute_tool final_answer\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 10398286333944367643,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026603470642000,\n      \"end_time\": 1758026603472241000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 7916564517670538421,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"execute_tool\",\n        \"gen_ai.tool.name\": \"final_answer\",\n        \"gen_ai.tool.args\": \"{\\\"answer\\\": \\\"1. Get current time in the America/New_York timezone. 2. Write the year to a file. 3. Return the list of steps taken.\\\"}\",\n        \"gen_ai.output\": \"1. Get current time in the America/New_York timezone. 2. Write the year to a file. 3. Return the list of steps taken.\",\n        \"gen_ai.output.type\": \"text\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"call_llm mistral/mistral-small-latest\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 10398286333944367643,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026603472911000,\n      \"end_time\": 1758026604388771000,\n      \"status\": {\n        \"status_code\": \"ok\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 549755261599922540,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"call_llm\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"gen_ai.output\": \"{\\n  \\\"steps\\\": [\\n    {\\n      \\\"number\\\": 1,\\n      \\\"description\\\": \\\"Get current time in the America/New_York timezone.\\\"\\n    },\\n    {\\n      \\\"number\\\": 2,\\n      \\\"description\\\": \\\"Write the year to a file.\\\"\\n    },\\n    {\\n      \\\"number\\\": 3,\\n      \\\"description\\\": \\\"Return the list of steps taken.\\\"\\n    }\\n  ]\\n}\",\n        \"gen_ai.output.type\": \"json\",\n        \"gen_ai.usage.input_tokens\": 178,\n        \"gen_ai.usage.output_tokens\": 83,\n        \"gen_ai.usage.input_cost\": 0.0000178,\n        \"gen_ai.usage.output_cost\": 0.0000249\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    },\n    {\n      \"name\": \"invoke_agent [any_agent]\",\n      \"kind\": \"internal\",\n      \"parent\": {\n        \"trace_id\": null,\n        \"span_id\": null,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"start_time\": 1758026601289354000,\n      \"end_time\": 1758026604388853000,\n      \"status\": {\n        \"status_code\": \"unset\",\n        \"description\": null\n      },\n      \"context\": {\n        \"trace_id\": 200754113673326459792359196831899614724,\n        \"span_id\": 10398286333944367643,\n        \"is_remote\": false,\n        \"trace_flags\": {\n          \"value\": 0\n        },\n        \"trace_state\": {\n          \"entries\": {}\n        }\n      },\n      \"attributes\": {\n        \"gen_ai.operation.name\": \"invoke_agent\",\n        \"gen_ai.agent.name\": \"any_agent\",\n        \"gen_ai.agent.description\": \"No description.\",\n        \"gen_ai.request.model\": \"mistral/mistral-small-latest\",\n        \"any_agent.version\": \"1.9.1.dev0+g8e34f9a88.d20250915\"\n      },\n      \"links\": [],\n      \"events\": [],\n      \"resource\": {\n        \"attributes\": {\n          \"telemetry.sdk.language\": \"python\",\n          \"telemetry.sdk.name\": \"opentelemetry\",\n          \"telemetry.sdk.version\": \"1.36.0\",\n          \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n      }\n    }\n  ],\n  \"final_output\": {\n    \"steps\": [\n      {\n        \"number\": 1,\n        \"description\": \"Get current time in the America/New_York timezone.\"\n      },\n      {\n        \"number\": 2,\n        \"description\": \"Write the year to a file.\"\n      },\n      {\n        \"number\": 3,\n        \"description\": \"Return the list of steps taken.\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"tracing/#dumping-to-file","title":"Dumping to File","text":"<p>The AgentTrace object is a pydantic model and can be saved to disk via standard pydantic practices:</p> <pre><code>from any_agent import AgentConfig, AnyAgent\nfrom any_agent.tools import search_web\n\nagent = AnyAgent.create(\n    \"openai\",\n    agent_config=AgentConfig(\n            model_id=\"mistral:mistral-small-latest\",\n            tools=[search_web],\n    )\n)\nagent_trace = agent.run(\"Which agent framework is the best?\")\nwith open(\"agent_trace.json\", \"w\", encoding=\"utf-8\") as f:\n  f.write(agent_trace.model_dump_json(indent=2, serialize_as_any=True))\n</code></pre> <p>Tip</p> <p>Passing <code>serialize_as_any=True</code> makes sure that the <code>final_output</code> gets dumped even when <code>AgentConfig.output_type</code> is used.</p>"},{"location":"tracing/#adding-an-opentelemetry-exporter","title":"Adding an OpenTelemetry exporter","text":"<p>Before starting to use the library, you can add new OpenTelemetry exporters and processors as needed.</p> <p>The following code will use the OpenTelemetry Python SDK to send the agent traces to an additional endpoint using OTLP over HTTP in the indicated URL:</p> <p>Tip</p> <p>To use custom exporters you need to install their required dependencies, In this example, it would be <code>pip install opentelemetry-exporter-otlp</code>.</p> <pre><code>from opentelemetry.trace import get_tracer_provider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n\ntp = get_tracer_provider()\nhttp_exporter = OTLPSpanExporter(endpoint=\"http://localhost:4318/v1/traces\")\ntp.add_span_processor(SimpleSpanProcessor(http_exporter))\n</code></pre>"},{"location":"agents/","title":"Defining and Running Agents","text":""},{"location":"agents/#defining-agents","title":"Defining Agents","text":"<p>To define any agent system you will always use the same imports:</p> <pre><code>from any_agent import AgentConfig, AnyAgent, AgentRunError\n# In these examples, the built-in tools will be used\nfrom any_agent.tools import search_web, visit_webpage\n</code></pre> <p>Check <code>AgentConfig</code> for more info on how to configure agents.</p>"},{"location":"agents/#single-agent","title":"Single Agent","text":"<pre><code>agent = AnyAgent.create(\n    \"openai\",  # See other options under `Frameworks`\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\",\n        instructions=\"Use the tools to find an answer\",\n        tools=[search_web, visit_webpage]\n    ),\n)\n</code></pre>"},{"location":"agents/#multi-agent","title":"Multi-Agent","text":"<p>Warning</p> <p>A multi-agent system introduces even more complexity than a single agent.</p> <p>As stated before, carefully consider whether you need to adopt this pattern to solve the task.</p> <p>Multi-Agent systems can be implemented using Agent-As-Tools.</p>"},{"location":"agents/#framework-specific-arguments","title":"Framework Specific Arguments","text":"<p>Sometimes, there may be a new feature in a framework that you want to use that isn't yet supported universally in any-agent.</p> <p>The <code>agent_args</code> parameter in <code>AgentConfig</code> allows you to pass arguments specific to the underlying framework that the agent instance is built on.</p>"},{"location":"agents/#running-agents","title":"Running Agents","text":"<pre><code>try:\n    agent_trace = agent.run(\"Which Agent Framework is the best??\")\n    print(agent_trace.final_output)\nexcept AgentRunError as e:\n    agent_trace = e.trace\n</code></pre> <p>Check <code>AgentTrace</code> for more info on the return type.</p> <p>Exceptions are wrapped in an <code>AgentRunError</code>, that carries the original exception in the <code>__cause__</code> attribute. Additionally, its <code>trace</code> property holds the trace containing the spans collected so far.</p>"},{"location":"agents/#async","title":"Async","text":"<p>If you are running in <code>async</code> context, you should use the equivalent <code>create_async</code> and <code>run_async</code> methods:</p> <pre><code>import asyncio\n\nasync def main():\n    agent = await AnyAgent.create_async(\n        \"openai\",\n        AgentConfig(\n            model_id=\"mistral:mistral-small-latest\",\n            instructions=\"Use the tools to find an answer\",\n            tools=[search_web, visit_webpage]\n        )\n    )\n\n    agent_trace = await agent.run_async(\"Which Agent Framework is the best??\")\n    print(agent_trace.final_output)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"agents/#batch-processing","title":"Batch Processing","text":"<p>While any-agent doesn't provide a dedicated <code>.run_batch()</code> API, we recommend using <code>asyncio.gather</code> with the <code>AnyAgent.run_async</code> API for concurrent processing:</p> <pre><code>import asyncio\nfrom any_agent import AgentConfig, AnyAgent\n\nasync def process_batch():\n    agent = await AnyAgent.create_async(\"tinyagent\", AgentConfig(...))\n    inputs = [\"Input 1\", \"Input 2\", \"Input 3\"]\n    tasks = [agent.run_async(input_text) for input_text in inputs]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return results\n</code></pre>"},{"location":"agents/#multi-turn-conversations","title":"Multi-Turn Conversations","text":"<p>For scenarios where you need to maintain conversation history across multiple agent interactions, you can leverage the <code>spans_to_messages</code> method built into the AgentTrace. This function converts agent traces into a standardized message format that can be used to provide context in subsequent conversations.</p> <p>When to Use Each Approach</p> <ul> <li>Multi-turn with <code>spans_to_messages</code>: When you need to maintain context across separate agent invocations or implement complex conversation management logic</li> <li>User interaction tools: When you want the agent to naturally interact with users during its execution, asking questions as needed to complete its task</li> <li>Hybrid approach: Combine both patterns for sophisticated agents that maintain long-term context while also gathering real-time user input</li> </ul>"},{"location":"agents/#basic-multi-turn-example","title":"Basic Multi-Turn Example","text":"<pre><code>from any_agent import AgentConfig, AnyAgent\n\n# Create your agent\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\",\n        instructions=\"You are a helpful assistant. Use previous conversation context when available.\",\n    )\n)\n\nresponse1 = agent.run(\"What's the capital of California?\")\nprint(f\"Agent: {response1.final_output}\")\nconversation_history = response1.spans_to_messages()\n# Convert previous conversation to readable format\nhistory_text = \"\\n\".join([\n    f\"{msg.role.capitalize()}: {msg.content}\"\n    for msg in conversation_history\n    if msg.role != \"system\"\n])\n\nuser_message = \"What's the closest national park to that city\"\n\nfull_prompt = f\"\"\"Previous conversation:\n{history_text}\n\nCurrent user message: {user_message}\n\nPlease respond taking into account the conversation history above.\"\"\"\n\nresponse2 = agent.run(full_prompt)\nprint(f\"Agent: {response2.final_output}\")  # Agent will understand \"that city\" refers to Sacramento\n</code></pre>"},{"location":"agents/#design-philosophy-thoughtful-message-history-management","title":"Design Philosophy: Thoughtful Message History Management","text":"<p>You may notice that the <code>agent.run()</code> method doesn't accept a <code>messages</code> parameter directly. This is an intentional design choice to encourage thoughtful handling of conversation history by developers. Rather than automatically managing message history, any-agent empowers you to:</p> <ul> <li>Choose your context strategy: Decide what parts of conversation history are relevant</li> <li>Manage token usage: Control how much context you include to optimize costs and performance</li> <li>Handle complex scenarios: Implement custom logic for conversation branching, summarization, or context windowing</li> </ul> <p>This approach ensures that conversation context is handled intentionally rather than automatically, leading to more efficient and purposeful agent interactions.</p>"},{"location":"agents/#using-user-interaction-tools-for-regular-conversations","title":"Using User Interaction Tools for Regular Conversations","text":"<p>For scenarios where you need regular, back-and-forth interaction with users, we recommend using or building your own user interaction tools rather than managing conversation history manually. This pattern allows the agent to naturally ask follow-up questions and gather information as needed. We provide a default <code>send_console_message</code> tool which uses console inputs and outputs, but you may need to use a more advanced tool (such as a Slack MCP Server) to handle user interaction.</p> <pre><code>from any_agent import AgentConfig, AnyAgent\nfrom any_agent.tools.user_interaction import send_console_message\n\n# Create agent with user interaction capabilities\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\",\n        instructions=\"You are a helpful travel assistant. Send console messages to ask more questions. Do not stop until you've answered the question.\",\n        tools=[send_console_message]\n    )\n)\n\n# The agent can now naturally ask questions during its execution\nprompt = \"\"\"\nI'm planning a trip and need help finding accommodations.\nPlease ask me some questions to understand my preferences, then provide recommendations.\n\"\"\"\n\nagent_trace = agent.run(prompt)\nprint(f\"Final recommendations: {agent_trace.final_output}\")\n</code></pre> <p>This approach is demonstrated in our MCP Agent cookbook example, where an agent uses user interaction tools to gather trip planning information dynamically. The agent can ask clarifying questions, get user preferences, and provide personalized recommendations all within a single <code>run()</code> call.</p>"},{"location":"agents/callbacks/","title":"Agent Callbacks","text":"<p>Callbacks provide hooks into the lifecycle of an <code>AnyAgent</code> execution. Using callbacks, you can monitor, control, and extend agent behavior without modifying the core underlying agent logic.</p>"},{"location":"agents/callbacks/#implementing-callbacks","title":"Implementing Callbacks","text":"<p>All callbacks must inherit from the base <code>Callback</code> class and can choose to implement any subset of the available callback methods. These methods include:</p> Callback Method When It Fires Example Use Cases before_agent_invocation Once at start, before any LLM calls Initialize counters, validate inputs, set up logging before_llm_call Before each LLM API call Content filtering, cost tracking, prompt inspection after_llm_call After LLM responds, before adding to history Response validation, token counting, logging before_tool_execution Before each tool runs Rate limiting, input validation, authorization checks after_tool_execution After tool completes Result validation, metrics collection, error handling after_agent_invocation Once at end, before returning final response Cleanup, final metrics, audit logging <pre><code># Minimum valid implementation\ndef before_llm_call(self, context: Context, *args, **kwargs) -&gt; Context:\n    return context  # &lt;--- Essential!\n</code></pre>"},{"location":"agents/callbacks/#managing-state-context","title":"Managing State (<code>Context</code>)","text":"<p>During an agent run ( <code>agent.run_async</code> or <code>agent.run</code> ), a unique <code>Context</code> object is created and shared across all callbacks.</p> <p>Use <code>Context.shared</code> (a dictionary) to persist data across different steps and callbacks.</p> <p>Note: The <code>Context</code> object is mutable. You should modify <code>Context.shared</code> directly and return the same object.</p> <p><code>any-agent</code> populates the <code>Context.current_span</code> property so that callbacks can access information in a framework-agnostic way.</p> <p>You can see what attributes are available for LLM Calls and Tool Executions by examining the <code>GenAI</code> class.</p> <p>Common Pattern: Initialize a counter in one callback and check it in another.</p> <pre><code>from any_agent.callbacks import Callback, Context\nfrom any_agent.tracing.attributes import GenAI\n\nclass CountSearchWeb(Callback):\n    def after_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:\n        if \"search_web_count\" not in context.shared:\n            context.shared[\"search_web_count\"] = 0\n        if context.current_span.attributes[GenAI.TOOL_NAME] == \"search_web\":\n            context.shared[\"search_web_count\"] += 1\n        return context\n</code></pre>"},{"location":"agents/callbacks/#stopping-execution","title":"Stopping Execution","text":"<p>Callbacks can raise exceptions to stop agent execution. This is useful for implementing safety guardrails or validation logic.</p> <p>Exceptions act as a circuit breaker</p> <p>Raising any exception from a callback immediately halts the agent loop. Use this intentionally to enforce limits or abort on invalid states.</p>"},{"location":"agents/callbacks/#using-agentcancel-recommended","title":"Using <code>AgentCancel</code> (Recommended)","text":"<p>For intentional cancellation (rate limits, guardrails, validation), subclass <code>AgentCancel</code>. These exceptions propagate directly to your code, allowing you to catch them by their specific type:</p> <pre><code>from any_agent import AgentCancel, AgentConfig, AnyAgent\nfrom any_agent.callbacks import Callback\nfrom any_agent.callbacks.context import Context\n\nclass SearchLimitReached(AgentCancel):\n    \"\"\"Raised when the search limit is exceeded.\"\"\"\n\nclass LimitSearchWeb(Callback):\n    def __init__(self, max_calls: int):\n        self.max_calls = max_calls\n\n    def before_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:\n        if context.shared.get(\"search_web_count\", 0) &gt; self.max_calls:\n            raise SearchLimitReached(f\"Exceeded {self.max_calls} search calls\")\n        return context\n\n# In your application code:\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=\"gpt-4.1-nano\",\n        callbacks=[LimitSearchWeb(max_calls=3)],\n    ),\n)\ntry:\n    trace = agent.run(\"Find information about Python\")\nexcept SearchLimitReached as e:\n    print(f\"Search limit reached: {e}\")\n    print(f\"Trace: {e.trace}\")  # Access spans collected before cancellation\n</code></pre>"},{"location":"agents/callbacks/#using-regular-exceptions","title":"Using Regular Exceptions","text":"<p>Regular exceptions (like <code>RuntimeError</code>) are automatically wrapped in <code>AgentRunError</code> by the framework, which provides access to the execution trace but requires you to inspect the wrapped exception:</p> <pre><code>from any_agent import AgentConfig, AgentRunError, AnyAgent\nfrom any_agent.callbacks import Callback\nfrom any_agent.callbacks.context import Context\n\nclass LimitSearchWeb(Callback):\n    def __init__(self, max_calls: int):\n        self.max_calls = max_calls\n\n    def before_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:\n        if context.shared.get(\"search_web_count\", 0) &gt; self.max_calls:\n            msg = \"Reached limit of `search_web` calls.\"\n            raise RuntimeError(msg)\n        return context\n\n# In your application code:\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=\"gpt-4.1-nano\",\n        callbacks=[LimitSearchWeb(max_calls=3)],\n    ),\n)\ntry:\n    trace = agent.run(\"Find information about Python\")\nexcept AgentRunError as e:\n    print(f\"Error: {e.original_exception}\")\n    print(f\"Trace: {e.trace}\")\n</code></pre> <p>Choosing the right exception type</p> <ul> <li><code>AgentCancel</code>: Use when cancellation is expected behavior and you want to handle it distinctly (e.g., rate limits, safety guardrails).</li> <li>Regular exceptions: Use when something unexpected goes wrong and you want consistent error handling via <code>AgentRunError</code>.</li> </ul> <p>Both expose the execution trace via <code>.trace</code> for debugging and inspection.</p>"},{"location":"agents/callbacks/#inspecting-data-contextcurrent_span","title":"Inspecting Data (<code>Context.current_span</code>)","text":"<p>The <code>Context.current_span</code> attribute provides access to the active trace span. This allows you to inspect (and modify) the data being processed, such as LLM inputs or Tool outputs.</p> <p>Common attributes (available via <code>any_agent.tracing.attributes.GenAI</code>) include:</p> <ul> <li> <p><code>GenAI.INPUT_MESSAGES</code>: The chat history sent to the model.</p> </li> <li> <p><code>GenAI.TOOL_NAME</code>: The name of the tool currently being executed.</p> </li> <li> <p><code>GenAI.OUTPUT_MESSAGES</code>: The response received from the model.</p> </li> </ul>"},{"location":"agents/callbacks/#how-it-works","title":"How it Works","text":"<p>When <code>agent.run()</code> or <code>agent.run_async()</code> executes, it triggers a series of events (e.g., before the LLM is called, after a tool is executed). You can register custom <code>Callback</code> classes to listen for these events.</p>"},{"location":"agents/callbacks/#the-callback-contract","title":"The Callback Contract","text":"<p>All callbacks share a strict contract: They receive the current <code>Context</code> as input and must return a <code>Context</code> as output.</p> <pre><code># pseudocode of an Agent run\n\nhistory = [system_prompt, user_prompt]\ncontext = Context()\n\nfor callback in agent.config.callbacks:\n    # 1. Agent Start\n    context = callback.before_agent_invocation(context)\n\nwhile True:\n\n    for callback in agent.config.callbacks:\n        # 2. Pre-LLM\n        context = callback.before_llm_call(context)\n\n    response = CALL_LLM(history)\n\n    for callback in agent.config.callbacks:\n        # 3. Post-LLM\n        context = callback.after_llm_call(context)\n\n    history.append(response)\n\n    if response.tool_executions:\n        for tool_execution in tool_executions:\n            # 4. Pre-Tool\n            for callback in agent.config.callbacks:\n                context = callback.before_tool_execution(context)\n\n            tool_response = EXECUTE_TOOL(tool_execution)\n\n            for callback in agent.config.callbacks:\n                # 5. Post-Tool\n                context = callback.after_tool_execution(context)\n\n            history.append(tool_response)\n\n    else:\n        for callback in agent.config.callbacks:\n            # 6. Agent DONE\n            context = callback.after_agent_invocation(context)\n        return response\n</code></pre> <p>Advanced designs such as safety guardrails or custom side-effects can be integrated into your agentic system using this functionality.</p>"},{"location":"agents/callbacks/#default-callbacks","title":"Default Callbacks","text":"<p><code>any-agent</code> comes with a set of default callbacks that will be used by default (if you don't pass a value to <code>AgentConfig.callbacks</code>):</p> <ul> <li><code>ConsolePrintSpan</code></li> </ul> <p>If you want to disable these default callbacks, you can pass an empty list:</p> <pre><code>from any_agent import AgentConfig, AnyAgent\nfrom any_agent.tools import search_web, visit_webpage\n\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\",\n        instructions=\"Use the tools to find an answer\",\n        tools=[search_web, visit_webpage],\n        callbacks=[]\n    ),\n)\n</code></pre>"},{"location":"agents/callbacks/#registering-your-own-callbacks","title":"Registering your own Callbacks","text":"<p>Callbacks are provided to the agent using the <code>AgentConfig.callbacks</code> property.</p> Extending default callbacksOverriding default callbacks <p><code>any-agent</code> includes default callbacks (like console logging). Use <code>get_default_callbacks</code> to keep them:</p> <pre><code>from any_agent import AgentConfig, AnyAgent\nfrom any_agent.callbacks import get_default_callbacks\nfrom any_agent.tools import search_web, visit_webpage\n\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=\"gpt-4.1-nano\",\n        instructions=\"Use the tools to find an answer\",\n        tools=[search_web, visit_webpage],\n        callbacks=[\n            CountSearchWeb(),           # Custom callbacks first\n            LimitSearchWeb(max_calls=3),\n            *get_default_callbacks() #Runs after custom callbacks\n        ]\n    ),\n)\n</code></pre> <p>To disable default logging or replace it entirely, pass a list without the defaults:</p> <pre><code>from any_agent import AgentConfig, AnyAgent\nfrom any_agent.tools import search_web, visit_webpage\n\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=\"gpt-4.1-nano\",\n        instructions=\"Use the tools to find an answer\",\n        tools=[search_web, visit_webpage],\n        callbacks=[\n            CountSearchWeb(),\n            LimitSearchWeb(max_calls=3)  # Default console logging disabled\n        ]\n    ),\n)\n</code></pre> <p>Warning</p> <p>Callbacks will be called in the order that they are added, so it is important to pay attention to the order in which you set the callback configuration.</p> <p>In the above example, passing:</p> <pre><code>    callbacks=[\n        LimitSearchWeb(max_calls=3) # \u2190 This will fail!\n        CountSearchWeb()    # Counter must come first\n    ]\n</code></pre> <p>Would fail because <code>context.shared[\"search_web_count\"]</code> was not set yet.</p>"},{"location":"agents/callbacks/#examples","title":"Examples","text":""},{"location":"agents/callbacks/#offloading-sensitive-information","title":"Offloading sensitive information","text":"<p>Some inputs and/or outputs in your traces might contain sensitive information that you don't want to be exposed in the traces.</p> <p>You can use callbacks to offload the sensitive information to an external location and replace the span attributes with a reference to that location:</p> <pre><code>import json\nfrom pathlib import Path\n\nfrom any_agent.callbacks.base import Callback\nfrom any_agent.callbacks.context import Context\nfrom any_agent.tracing.attributes import GenAI\n\nclass SensitiveDataOffloader(Callback):\n\n    def __init__(self, output_dir: str) -&gt; None:\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(exist_ok=True, parents=True)\n\n    def before_llm_call(self, context: Context, *args, **kwargs) -&gt; Context:\n\n        span = context.current_span\n\n        if input_messages := span.attributes.get(GenAI.INPUT_MESSAGES):\n            output_file = self.output_dir / f\"{span.get_span_context().trace_id}.txt\"\n            output_file.write_text(str(input_messages))\n\n            span.set_attribute(\n                GenAI.INPUT_MESSAGES,\n                json.dumps(\n                    {\"ref\": str(output_file)}\n                )\n            )\n\n        return context\n</code></pre> <p>You can find a working example in the Callbacks Cookbook.</p>"},{"location":"agents/callbacks/#limit-the-number-of-steps","title":"Limit the number of steps","text":"<p>Some agent frameworks allow you to limit how many steps an agent can take and some don't. In addition, each framework defines a <code>step</code> differently: some count the LLM calls, some the tool executions, and some the sum of both.</p> <p>You can use callbacks to limit how many steps an agent can take, and you can decide what to count as a <code>step</code>:</p> <pre><code>from any_agent import AgentCancel\nfrom any_agent.callbacks.base import Callback\nfrom any_agent.callbacks.context import Context\n\n\nclass LLMCallLimitReached(AgentCancel):\n    \"\"\"Raised when the LLM call limit is exceeded.\"\"\"\n\n\nclass ToolExecutionLimitReached(AgentCancel):\n    \"\"\"Raised when the tool execution limit is exceeded.\"\"\"\n\n\nclass LimitLLMCalls(Callback):\n    def __init__(self, max_llm_calls: int) -&gt; None:\n        self.max_llm_calls = max_llm_calls\n\n    def before_llm_call(self, context: Context, *args, **kwargs) -&gt; Context:\n        if \"n_llm_calls\" not in context.shared:\n            context.shared[\"n_llm_calls\"] = 0\n\n        context.shared[\"n_llm_calls\"] += 1\n\n        if context.shared[\"n_llm_calls\"] &gt; self.max_llm_calls:\n            raise LLMCallLimitReached(f\"Exceeded {self.max_llm_calls} LLM calls\")\n\n        return context\n\n\nclass LimitToolExecutions(Callback):\n    def __init__(self, max_tool_executions: int) -&gt; None:\n        self.max_tool_executions = max_tool_executions\n\n    def before_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:\n        if \"n_tool_executions\" not in context.shared:\n            context.shared[\"n_tool_executions\"] = 0\n\n        context.shared[\"n_tool_executions\"] += 1\n\n        if context.shared[\"n_tool_executions\"] &gt; self.max_tool_executions:\n            raise ToolExecutionLimitReached(\n                f\"Exceeded {self.max_tool_executions} tool executions\"\n            )\n\n        return context\n</code></pre>"},{"location":"agents/models/","title":"Model Configuration","text":""},{"location":"agents/models/#overview","title":"Overview","text":"<p>Model configuration in <code>any-agent</code> is designed to be consistent across all supported frameworks. We use <code>any-llm</code> as the default model provider, which acts as a unified interface allowing you to use any language model from any provider with the same syntax.</p>"},{"location":"agents/models/#configuration-parameters","title":"Configuration Parameters","text":"<p>The model configuration is defined through several parameters in <code>AgentConfig</code>:</p> <p>The <code>model_id</code> parameter selects which language model your agent will use. The format depends on the provider:</p> <p>The <code>model_args</code> parameter allows you to pass additional arguments to the model, such as <code>temperature</code>, <code>top_k</code>, and other provider-specific parameters.</p> <p>The <code>api_base</code> parameter allows you to specify a custom API endpoint. This is useful when:</p> <ul> <li>Using a local model server (e.g., Ollama, llama.cpp, llamafile)</li> <li>Routing through a proxy</li> <li>Using a self-hosted model endpoint</li> </ul> <p>The <code>api_key</code> parameter allows you to explicitly specify an API key for authentication. By default, <code>any-llm</code> will automatically search for common environment variables (like <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, etc.).\u00e5</p> <p>See the AnyLLM Provider Documentation for the complete list of supported providers.\u00e5</p>"},{"location":"agents/tools/","title":"Agent Tools","text":"<p><code>any-agent</code> provides 2 options to specify what <code>tools</code> are available to your agent: <code>Callables</code> and <code>MCP</code> (Model Context Protocol).</p> <p>Tip</p> <p>Multi-agent can be implemented using Agents-As-Tools.</p> <p>You can use any combination of options within the same agent.</p>"},{"location":"agents/tools/#callables","title":"Callables","text":"<p>Any Python callable can be directly passed as tools. You can define them in the same script, import it from an external package, etc.</p> <p>Under the hood, <code>any-agent</code> takes care of wrapping the tool so it becomes usable by the selected framework.</p> <p>Tip</p> <p>Check all the built-in callable tools that any-agent provides.</p> <pre><code>from any_agent import AgentConfig\nfrom any_agent.tools import search_web\n\nmain_agent = AgentConfig(\n    model_id=\"mistral:mistral-small-latest\",\n    tools=[search_web]\n)\n</code></pre>"},{"location":"agents/tools/#composio","title":"Composio","text":"<p>We have a custom Composio provider that allows to use any Composio tools in <code>any-agent</code>:</p> <p>Tip</p> <p>Check the different options for Fetching and Filtering Composio Tools.</p> <pre><code>from any_agent import AgentConfig\nfrom any_agent.tools.composio import CallableProvider\nfrom composio import Composio\n\ncpo = Composio(CallableProvider())\n\nmain_agent = AgentConfig(\n    model_id=\"mistral:mistral-small-latest\",\n    tools=cpo.tools.get(\n        user_id=\"daavoo\",\n        toolkits=[\"GITHUB\", \"HACKERNEWS\"],\n    )\n)\n</code></pre>"},{"location":"agents/tools/#using-agents-as-tools","title":"Using Agents-As-Tools","text":"<p>To directly use one agent as a tool for another agent, <code>any-agent</code> provides 3 different approaches:</p> <p>The agent to be used as a tool can be defined as usual:</p> <pre><code>from any_agent import AgentConfig, AnyAgent\nfrom any_agent.tools import search_web\n\ngoogle_agent = await AnyAgent.create_async(\n    \"google\",\n    AgentConfig(\n        name=\"google_expert\",\n        model_id=\"mistral:mistral-small-latest\",\n        instructions=\"Use the available tools to answer questions about the Google ADK\",\n        description=\"An agent that can answer questions about the Google Agents Development Kit (ADK).\",\n        tools=[search_web]\n    )\n)\n</code></pre> <p>You can then choose to wrap the agent using different approaches:</p> Agent as CallableAgent as MCPAgent as A2A <pre><code>async def google_agent_as_tool(query: str) -&gt; str:\n    agent_trace = await google_agent.run_async(prompt=query)\n    return str(agent_trace.final_output)\n\ngoogle_agent_as_tool.__doc__ = google_agent.config.description\n</code></pre> <pre><code>from any_agent.config import MCPSse\nfrom any_agent.serving import MCPServingConfig\n\nmcp_handle = await google_agent.serve_async(\n    MCPServingConfig(port=5001, endpoint=\"/google-agent\"))\n\ngoogle_agent_as_tool = MCPSse(\n    url=\"http://localhost:5001/google-agent/sse\")\n</code></pre> <pre><code>from any_agent.serving import A2AServingConfig\nfrom any_agent.tools import a2a_tool_async\n\na2a_handle = await google_agent.serve_async(\n    A2AServingConfig(port=5001, endpoint=\"/google-agent\"))\n\ngoogle_agent_as_tool = await a2a_tool_async(\n    url=\"http://localhost:5001/google-agent\")\n</code></pre> <p>Finally, regardless of the option chosen above, you can pass the agent as a tool to another agent:</p> <pre><code>main_agent = await AnyAgent.create_async(\n    \"tinyagent\",\n    AgentConfig(\n        name=\"main_agent\",\n        model_id=\"mistral-small-latest\",\n        instructions=\"Use the available tools to obtain additional information to answer the query.\",\n        tools=[google_agent_as_tool],\n    )\n)\n</code></pre>"},{"location":"agents/tools/#mcp","title":"MCP","text":"<p>MCP can either be run locally (MCPStdio) or you can connect to an MCP that is running elsewhere (using either MCPSse or MCPStreamableHttp).</p> <p>Tip</p> <p>There are tools like SuperGateway providing an easy way to turn a Stdio server into an SSE server.</p> <p>Warning</p> <p>The SSE remote transport has been deprecated as of MCP specification version 2025-03-26. Please use the HTTP Stream Transport instead.</p> MCP (Stdio)MCP (Streamable HTTP)MCP (SSE) <p>See the MCPStdio API Reference.</p> <pre><code>from any_agent import AgentConfig\nfrom any_agent.config import MCPStdio\n\nmain_agent = AgentConfig(\n    model_id=\"mistral:mistral-small-latest\",\n    tools=[\n        MCPStdio(\n            command=\"docker\",\n            args=[\"run\", \"-i\", \"--rm\", \"mcp/fetch\"],\n            tools=[\"fetch\"]\n        ),\n    ]\n)\n</code></pre> <p>See the MCPStreamableHttp API Reference.</p> <pre><code>from any_agent import AgentConfig\nfrom any_agent.config import MCPStreamableHttp\n\nmain_agent = AgentConfig(\n    model_id=\"mistral:mistral-small-latest\",\n    tools=[\n        MCPStreamableHttp(\n            url=\"http://localhost:8000/mcp\"\n        ),\n    ]\n)\n</code></pre> <p>See the MCPSse API Reference.</p> <pre><code>from any_agent import AgentConfig\nfrom any_agent.config import MCPSse\n\nmain_agent = AgentConfig(\n    model_id=\"mistral:mistral-small-latest\",\n    tools=[\n        MCPSse(\n            url=\"http://localhost:8000/sse\"\n        ),\n    ]\n)\n</code></pre>"},{"location":"agents/tools/#resource-management","title":"Resource Management","text":"<p>When using tools where the python process running the agent is also responsible for managing the lifetime of the tool process (e.g., <code>MCPStdio</code>), the agent creates connections that should be properly cleaned up in order to avoid error messages like <code>RuntimeError: Attempted to exit cancel scope in a different task than it was entered in</code>.</p>"},{"location":"agents/tools/#using-context-manager-recommended","title":"Using Context Manager (Recommended)","text":"<p>The recommended approach is to use the async context manager pattern, which automatically handles cleanup:</p> <pre><code>import asyncio\nfrom any_agent import AgentConfig, AnyAgent\nfrom any_agent.config import MCPStdio\n\nasync def main():\n    time_tool = MCPStdio(\n        command=\"uvx\",\n        args=[\"mcp-server-time\"],\n    )\n\n    async with await AnyAgent.create_async(\n        \"tinyagent\",\n        AgentConfig(\n            model_id=\"mistral:mistral-small-latest\",\n            tools=[time_tool],\n        ),\n    ) as agent:\n        result = await agent.run_async(\"What time is it?\")\n        print(result.final_output)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"agents/tools/#manual-cleanup","title":"Manual Cleanup","text":"<p>If you can't use a context manager, you can manually clean up resources:</p> <pre><code>import asyncio\nfrom any_agent import AgentConfig, AnyAgent\nfrom any_agent.config import MCPStdio\n\nasync def main():\n    time_tool = MCPStdio(\n        command=\"uvx\",\n        args=[\"mcp-server-time\"],\n    )\n\n    agent = await AnyAgent.create_async(\n        \"tinyagent\",\n        AgentConfig(\n            model_id=\"mistral:mistral-small-latest\",\n            tools=[time_tool],\n        ),\n    )\n\n    try:\n        result = await agent.run_async(\"What time is it?\")\n        print(result.final_output)\n    finally:\n        await agent.cleanup_async()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"agents/frameworks/agno/","title":"Agno","text":"<p>https://github.com/agno-agi/agno</p>"},{"location":"agents/frameworks/agno/#default-agent-type","title":"Default Agent Type","text":"<p>We use <code>agno.agent.Agent</code> as default. Check the reference to find additional supported <code>agent_args</code>.</p>"},{"location":"agents/frameworks/agno/#default-model-type","title":"Default Model Type","text":"<p>We use <code>any_llm</code> as the default model provider. Check the AnyLLM documentation for supported providers and <code>model_args</code>.</p>"},{"location":"agents/frameworks/google_adk/","title":"Google Agent Development Kit (ADK)","text":"<p>https://github.com/google/adk-python</p>"},{"location":"agents/frameworks/google_adk/#default-agent-type","title":"Default Agent Type","text":"<p>We use <code>google.adk.agents.llm_agent.LlmAgent</code> as default. Check the reference to find additional supported <code>agent_args</code>.</p>"},{"location":"agents/frameworks/google_adk/#default-model-type","title":"Default Model Type","text":"<p>We use <code>any_llm</code> as the default model provider. Check the AnyLLM documentation for supported providers and <code>model_args</code>.</p>"},{"location":"agents/frameworks/google_adk/#run-args","title":"Run args","text":"<p>Check <code>RunConfig</code> to find additional supported <code>AnyAgent.run</code> args.</p>"},{"location":"agents/frameworks/langchain/","title":"LangChain","text":"<p>https://github.com/langchain-ai/langchain</p> <p>https://github.com/langchain-ai/langgraph</p>"},{"location":"agents/frameworks/langchain/#default-agent-type","title":"Default Agent Type","text":"<p>We use <code>langgraph.prebuilt.create_react_agent</code> as default. Check the reference to find additional supported <code>agent_args</code>.</p>"},{"location":"agents/frameworks/langchain/#default-model-type","title":"Default Model Type","text":"<p>We use <code>any_llm</code> as the default model provider. Check the AnyLLM documentation for supported providers and <code>model_args</code>.</p>"},{"location":"agents/frameworks/langchain/#run-args","title":"Run args","text":"<p>Check <code>RunnableConfig</code> to find additional supported <code>AnyAgent.run</code> args.</p>"},{"location":"agents/frameworks/llama_index/","title":"LlamaIndex","text":"<p>https://github.com/run-llama/llama_index</p>"},{"location":"agents/frameworks/llama_index/#default-agent-type","title":"Default Agent Type","text":"<p>We use <code>llama_index.core.agent.workflow.react_agent.FunctionAgent</code> as default. However, this agent requires that tools are used. If no tools are used, any-agent will default to <code>llama_index.core.agent.workflow.react_agent.ReActAgent</code>. Check the reference to find additional supported <code>agent_args</code>.</p>"},{"location":"agents/frameworks/llama_index/#default-model-type","title":"Default Model Type","text":"<p>We use <code>any_llm</code> as the default model provider. Check the AnyLLM documentation for supported providers and <code>model_args</code>.</p>"},{"location":"agents/frameworks/openai/","title":"OpenAI Agents SDK","text":"<p>https://github.com/openai/openai-agents-python</p>"},{"location":"agents/frameworks/openai/#default-agent-type","title":"Default Agent Type","text":"<p>We use <code>agents.Agent</code> as default. Check the reference to find additional supported <code>agent_args</code>.</p>"},{"location":"agents/frameworks/openai/#default-model-type","title":"Default Model Type","text":"<p>We use <code>any_llm</code> as the default model provider. Check the AnyLLM documentation for supported providers and <code>model_args</code>.</p>"},{"location":"agents/frameworks/openai/#run-args","title":"Run args","text":"<p>Check <code>agents.run.Runner.run</code> to find additional supported <code>AnyAgent.run</code> args.</p>"},{"location":"agents/frameworks/smolagents/","title":"smolagents","text":"<p>https://github.com/huggingface/smolagents</p>"},{"location":"agents/frameworks/smolagents/#default-agent-type","title":"Default Agent Type","text":"<p>We use <code>smolagents.ToolCallingAgent</code> as default. Check the reference to find additional supported <code>agent_args</code>.</p>"},{"location":"agents/frameworks/smolagents/#default-model-type","title":"Default Model Type","text":"<p>We use <code>any_llm</code> as the default model provider. Check the AnyLLM documentation for supported providers and <code>model_args</code>.</p>"},{"location":"agents/frameworks/smolagents/#run-args","title":"Run args","text":"<p>Check <code>smolagents.MultiStepAgent.run</code> to find additional supported <code>AnyAgent.run</code> args.</p>"},{"location":"agents/frameworks/tinyagent/","title":"TinyAgent","text":"<p>As part of the bare bones library, we provide our own Python implementation based on HuggingFace Tiny Agents.</p> <p>You can find it in <code>any_agent.frameworks.tinyagent</code>.</p>"},{"location":"agents/frameworks/tinyagent/#examples","title":"Examples","text":""},{"location":"agents/frameworks/tinyagent/#use-mcp-tools","title":"Use MCP Tools","text":"<pre><code>from any_agent import AnyAgent, AgentConfig\nfrom any_agent.config import MCPStdio\n\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\",\n        instructions=\"You must use the available tools to find an answer\",\n        tools=[\n            MCPStdio(\n                command=\"uvx\",\n                args=[\"duckduckgo-mcp-server\"]\n            )\n        ]\n    )\n)\n\nresult = agent.run(\n    \"Which Agent Framework is the best??\"\n)\nprint(result.final_output)\n</code></pre>"},{"location":"api/agent/","title":"Agent","text":""},{"location":"api/agent/#agent","title":"Agent","text":""},{"location":"api/agent/#any_agent.AnyAgent","title":"<code>any_agent.AnyAgent</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base abstract class for all agent implementations.</p> <p>This provides a unified interface for different agent frameworks.</p> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>class AnyAgent(ABC):\n    \"\"\"Base abstract class for all agent implementations.\n\n    This provides a unified interface for different agent frameworks.\n    \"\"\"\n\n    def __init__(self, config: AgentConfig):\n        self.config = config\n\n        self._mcp_clients: list[MCPClient] = []\n        self._tools: list[Any] = []\n\n        self._add_span_callbacks()\n        self._wrapper = _get_wrapper_by_framework(self.framework)\n\n        self._tracer: Tracer = otel_trace.get_tracer(\"any_agent\")\n\n        self._lock = asyncio.Lock()\n        self._callback_contexts: dict[int, Context] = {}\n\n    @staticmethod\n    def _get_agent_type_by_framework(\n        framework_raw: AgentFramework | str,\n    ) -&gt; type[AnyAgent]:\n        framework = AgentFramework.from_string(framework_raw)\n\n        if framework is AgentFramework.SMOLAGENTS:\n            from any_agent.frameworks.smolagents import SmolagentsAgent\n\n            return SmolagentsAgent\n\n        if framework is AgentFramework.LANGCHAIN:\n            from any_agent.frameworks.langchain import LangchainAgent\n\n            return LangchainAgent\n\n        if framework is AgentFramework.OPENAI:\n            from any_agent.frameworks.openai import OpenAIAgent\n\n            return OpenAIAgent\n\n        if framework is AgentFramework.LLAMA_INDEX:\n            from any_agent.frameworks.llama_index import LlamaIndexAgent\n\n            return LlamaIndexAgent\n\n        if framework is AgentFramework.GOOGLE:\n            from any_agent.frameworks.google import GoogleAgent\n\n            return GoogleAgent\n\n        if framework is AgentFramework.AGNO:\n            from any_agent.frameworks.agno import AgnoAgent\n\n            return AgnoAgent\n\n        if framework is AgentFramework.TINYAGENT:\n            from any_agent.frameworks.tinyagent import TinyAgent\n\n            return TinyAgent\n\n        assert_never(framework)\n\n    @classmethod\n    def create(\n        cls,\n        agent_framework: AgentFramework | str,\n        agent_config: AgentConfig,\n    ) -&gt; AnyAgent:\n        \"\"\"Create an agent using the given framework and config.\"\"\"\n        return run_async_in_sync(\n            cls.create_async(\n                agent_framework=agent_framework,\n                agent_config=agent_config,\n            ),\n            allow_running_loop=INSIDE_NOTEBOOK,\n        )\n\n    @classmethod\n    async def create_async(\n        cls,\n        agent_framework: AgentFramework | str,\n        agent_config: AgentConfig,\n    ) -&gt; AnyAgent:\n        \"\"\"Create an agent using the given framework and config.\"\"\"\n        agent_cls = cls._get_agent_type_by_framework(agent_framework)\n        agent = agent_cls(agent_config)\n        await agent._load_agent()\n        return agent\n\n    async def _load_tools(self, tools: Sequence[Tool]) -&gt; list[Any]:\n        tools, mcp_clients = await _wrap_tools(tools, self.framework)\n        self._mcp_clients.extend(mcp_clients)\n        return tools\n\n    async def cleanup_async(self) -&gt; None:\n        \"\"\"Clean up resources including MCP client connections.\n\n        This should be called when you're done using the agent to ensure\n        all resources are properly released.\n        \"\"\"\n        for client in self._mcp_clients:\n            await client.disconnect()\n        self._mcp_clients.clear()\n\n    async def __aenter__(self) -&gt; Self:\n        \"\"\"Enter the async context manager.\"\"\"\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: types.TracebackType | None,\n    ) -&gt; None:\n        \"\"\"Exit the async context manager and clean up resources.\"\"\"\n        await self.cleanup_async()\n\n    def run(self, prompt: str, **kwargs: Any) -&gt; AgentTrace:\n        \"\"\"Run the agent with the given prompt.\"\"\"\n        return run_async_in_sync(\n            self.run_async(prompt, **kwargs), allow_running_loop=INSIDE_NOTEBOOK\n        )\n\n    async def run_async(self, prompt: str, **kwargs: Any) -&gt; AgentTrace:\n        \"\"\"Run the agent asynchronously with the given prompt.\n\n        Args:\n            prompt: The user prompt to be passed to the agent.\n\n            kwargs: Will be passed to the underlying runner used\n                by the framework.\n\n        Returns:\n            The `AgentTrace` containing information about the\n                steps taken by the agent.\n\n        \"\"\"\n        trace = AgentTrace()\n        trace_id: int\n\n        # This design is so that we only catch exceptions thrown by _run_async. All other exceptions will not be caught.\n        try:\n            with self._tracer.start_as_current_span(\n                f\"invoke_agent [{self.config.name}]\"\n            ) as invoke_span:\n                async with self._lock:\n                    trace_id = invoke_span.get_span_context().trace_id\n                    self._wrapper.callback_context[trace_id] = Context(\n                        current_span=invoke_span,\n                        trace=AgentTrace(),\n                        tracer=self._tracer,\n                        shared={},\n                    )\n\n                    if len(self._wrapper.callback_context) == 1:\n                        # If there is more than 1 entry in `callback_context`, it means that the agent has\n                        # already being wrapped so we won't wrap it again.\n                        await self._wrapper.wrap(\n                            agent=self,  # type: ignore[arg-type]\n                        )\n\n                # Importing here to avoid circular import issues\n                from any_agent import __version__ as _ANY_AGENT_VERSION  # noqa: N812\n\n                invoke_span.set_attributes(\n                    {\n                        GenAI.OPERATION_NAME: \"invoke_agent\",\n                        GenAI.AGENT_NAME: self.config.name,\n                        GenAI.AGENT_DESCRIPTION: self.config.description\n                        or \"No description.\",\n                        GenAI.REQUEST_MODEL: self.config.model_id,\n                        AnyAgentAttributes.VERSION: _ANY_AGENT_VERSION,\n                    }\n                )\n\n                context = self._wrapper.callback_context[trace_id]\n                for callback in self.config.callbacks:\n                    result = callback.before_agent_invocation(context, prompt, **kwargs)\n                    if asyncio.iscoroutinefunction(callback.before_agent_invocation):\n                        context = await result  # type: ignore[misc]\n                    else:\n                        context = result\n\n                final_output = await self._run_async(prompt, **kwargs)\n\n        except Exception as e:\n            async with self._lock:\n                if len(self._wrapper.callback_context) == 1:\n                    await self._wrapper.unwrap(self)  # type: ignore[arg-type]\n                wrapped_context = self._wrapper.callback_context.pop(trace_id, None)\n                if wrapped_context is not None:\n                    trace = wrapped_context.trace\n                    for callback in self.config.callbacks:\n                        assert wrapped_context is not None\n                        result = callback.after_agent_invocation(\n                            wrapped_context, prompt, **kwargs\n                        )\n                        if asyncio.iscoroutinefunction(callback.after_agent_invocation):\n                            wrapped_context = await result  # type: ignore[misc]\n                        else:\n                            wrapped_context = result\n\n            trace.add_span(invoke_span)\n\n            # Preserve control-flow exceptions without wrapping.\n            if isinstance(e, AgentCancel):\n                e._trace = trace\n                raise\n\n            # Check if the framework wrapped an AgentCancel in its own error type.\n            if cancel := _unwrap_agent_cancel(e):\n                cancel._trace = trace\n                raise cancel from e\n\n            raise AgentRunError(trace, e) from e\n\n        async with self._lock:\n            if len(self._wrapper.callback_context) == 1:\n                await self._wrapper.unwrap(self)  # type: ignore[arg-type]\n            wrapped_context = self._wrapper.callback_context.pop(trace_id, None)\n            if wrapped_context is not None:\n                trace = wrapped_context.trace\n                for callback in self.config.callbacks:\n                    assert wrapped_context is not None\n                    result = callback.after_agent_invocation(\n                        wrapped_context, prompt, **kwargs\n                    )\n                    if asyncio.iscoroutinefunction(callback.after_agent_invocation):\n                        wrapped_context = await result  # type: ignore[misc]\n                    else:\n                        wrapped_context = result\n\n        trace.add_span(invoke_span)\n        trace.final_output = final_output\n        return trace\n\n    async def _serve_a2a_async(\n        self, serving_config: A2AServingConfig | None\n    ) -&gt; ServerHandle:\n        from any_agent.serving import (\n            A2AServingConfig,\n            _get_a2a_app_async,\n            serve_a2a_async,\n        )\n\n        if serving_config is None:\n            serving_config = A2AServingConfig()\n\n        app = await _get_a2a_app_async(self, serving_config=serving_config)\n\n        return await serve_a2a_async(\n            app,\n            host=serving_config.host,\n            port=serving_config.port,\n            endpoint=serving_config.endpoint,\n            log_level=serving_config.log_level,\n        )\n\n    async def _serve_mcp_async(self, serving_config: MCPServingConfig) -&gt; ServerHandle:\n        from any_agent.serving import serve_mcp_async\n\n        return await serve_mcp_async(\n            self,\n            host=serving_config.host,\n            port=serving_config.port,\n            endpoint=serving_config.endpoint,\n            log_level=serving_config.log_level,\n        )\n\n    @overload\n    async def serve_async(self, serving_config: MCPServingConfig) -&gt; ServerHandle: ...\n\n    @overload\n    async def serve_async(\n        self, serving_config: A2AServingConfig | None = None\n    ) -&gt; ServerHandle: ...\n\n    async def serve_async(\n        self, serving_config: MCPServingConfig | A2AServingConfig | None = None\n    ) -&gt; ServerHandle:\n        \"\"\"Serve this agent asynchronously using the protocol defined in the serving_config.\n\n        Args:\n            serving_config: Configuration for serving the agent. If None, uses default A2AServingConfig.\n                          Must be an instance of A2AServingConfig or MCPServingConfig.\n\n        Returns:\n            A ServerHandle instance that provides methods for managing the server lifecycle.\n\n        Raises:\n            ImportError: If the `a2a` dependencies are not installed and an `A2AServingConfig` is used.\n\n        Example:\n            ```\n            agent = await AnyAgent.create_async(\"tinyagent\", AgentConfig(...))\n            config = MCPServingConfig(port=8080)\n            server_handle = await agent.serve_async(config)\n            try:\n                # Server is running\n                await asyncio.sleep(10)\n            finally:\n                await server_handle.shutdown()\n            ```\n\n        \"\"\"\n        from any_agent.serving import MCPServingConfig\n\n        if isinstance(serving_config, MCPServingConfig):\n            return await self._serve_mcp_async(serving_config)\n        return await self._serve_a2a_async(serving_config)\n\n    def _add_span_callbacks(self) -&gt; None:\n        if self.config.callbacks is None:\n            return\n\n        from any_agent.callbacks.span_end import SpanEndCallback\n        from any_agent.callbacks.span_generation import (\n            SpanGeneration,\n            _get_span_generation_callback,\n        )\n\n        if not any(isinstance(c, SpanGeneration) for c in self.config.callbacks):\n            self.config.callbacks.insert(\n                0, _get_span_generation_callback(self.framework)\n            )\n        if not any(isinstance(c, SpanEndCallback) for c in self.config.callbacks):\n            self.config.callbacks.append(SpanEndCallback())\n\n    @abstractmethod\n    async def _load_agent(self) -&gt; None:\n        \"\"\"Load the agent instance.\"\"\"\n\n    @abstractmethod\n    async def _run_async(self, prompt: str, **kwargs: Any) -&gt; str | BaseModel:\n        \"\"\"To be implemented by each framework.\"\"\"\n\n    @abstractmethod\n    async def update_output_type_async(\n        self, output_type: type[BaseModel] | None\n    ) -&gt; None:\n        \"\"\"Update the output type of the agent in-place.\n\n        This method allows updating the agent's output type without recreating\n        the entire agent instance, which is more efficient than the current\n        approach of recreating the agent.\n\n        Args:\n            output_type: The new output type to use, or None to remove output type constraint\n\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def framework(self) -&gt; AgentFramework:\n        \"\"\"The Agent Framework used.\"\"\"\n\n    @property\n    def agent(self) -&gt; Any:\n        \"\"\"The underlying agent implementation from the framework.\n\n        This property is intentionally restricted to maintain framework abstraction\n        and prevent direct dependency on specific agent implementations.\n\n        If you need functionality that relies on accessing the underlying agent:\n        1. Consider if the functionality can be added to the AnyAgent interface\n        2. Submit a GitHub issue describing your use case\n        3. Contribute a PR implementing the needed functionality\n\n        Raises:\n            NotImplementedError: Always raised when this property is accessed\n\n        \"\"\"\n        msg = \"Cannot access the 'agent' property of AnyAgent, if you need to use functionality that relies on the underlying agent framework, please file a Github Issue or we welcome a PR to add the functionality to the AnyAgent class\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"api/agent/#any_agent.AnyAgent.agent","title":"<code>agent</code>  <code>property</code>","text":"<p>The underlying agent implementation from the framework.</p> <p>This property is intentionally restricted to maintain framework abstraction and prevent direct dependency on specific agent implementations.</p> <p>If you need functionality that relies on accessing the underlying agent: 1. Consider if the functionality can be added to the AnyAgent interface 2. Submit a GitHub issue describing your use case 3. Contribute a PR implementing the needed functionality</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always raised when this property is accessed</p>"},{"location":"api/agent/#any_agent.AnyAgent.framework","title":"<code>framework</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>The Agent Framework used.</p>"},{"location":"api/agent/#any_agent.AnyAgent.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Enter the async context manager.</p> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>async def __aenter__(self) -&gt; Self:\n    \"\"\"Enter the async context manager.\"\"\"\n    return self\n</code></pre>"},{"location":"api/agent/#any_agent.AnyAgent.__aexit__","title":"<code>__aexit__(exc_type, exc_val, exc_tb)</code>  <code>async</code>","text":"<p>Exit the async context manager and clean up resources.</p> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>async def __aexit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: types.TracebackType | None,\n) -&gt; None:\n    \"\"\"Exit the async context manager and clean up resources.\"\"\"\n    await self.cleanup_async()\n</code></pre>"},{"location":"api/agent/#any_agent.AnyAgent.cleanup_async","title":"<code>cleanup_async()</code>  <code>async</code>","text":"<p>Clean up resources including MCP client connections.</p> <p>This should be called when you're done using the agent to ensure all resources are properly released.</p> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>async def cleanup_async(self) -&gt; None:\n    \"\"\"Clean up resources including MCP client connections.\n\n    This should be called when you're done using the agent to ensure\n    all resources are properly released.\n    \"\"\"\n    for client in self._mcp_clients:\n        await client.disconnect()\n    self._mcp_clients.clear()\n</code></pre>"},{"location":"api/agent/#any_agent.AnyAgent.create","title":"<code>create(agent_framework, agent_config)</code>  <code>classmethod</code>","text":"<p>Create an agent using the given framework and config.</p> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    agent_framework: AgentFramework | str,\n    agent_config: AgentConfig,\n) -&gt; AnyAgent:\n    \"\"\"Create an agent using the given framework and config.\"\"\"\n    return run_async_in_sync(\n        cls.create_async(\n            agent_framework=agent_framework,\n            agent_config=agent_config,\n        ),\n        allow_running_loop=INSIDE_NOTEBOOK,\n    )\n</code></pre>"},{"location":"api/agent/#any_agent.AnyAgent.create_async","title":"<code>create_async(agent_framework, agent_config)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Create an agent using the given framework and config.</p> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>@classmethod\nasync def create_async(\n    cls,\n    agent_framework: AgentFramework | str,\n    agent_config: AgentConfig,\n) -&gt; AnyAgent:\n    \"\"\"Create an agent using the given framework and config.\"\"\"\n    agent_cls = cls._get_agent_type_by_framework(agent_framework)\n    agent = agent_cls(agent_config)\n    await agent._load_agent()\n    return agent\n</code></pre>"},{"location":"api/agent/#any_agent.AnyAgent.run","title":"<code>run(prompt, **kwargs)</code>","text":"<p>Run the agent with the given prompt.</p> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>def run(self, prompt: str, **kwargs: Any) -&gt; AgentTrace:\n    \"\"\"Run the agent with the given prompt.\"\"\"\n    return run_async_in_sync(\n        self.run_async(prompt, **kwargs), allow_running_loop=INSIDE_NOTEBOOK\n    )\n</code></pre>"},{"location":"api/agent/#any_agent.AnyAgent.run_async","title":"<code>run_async(prompt, **kwargs)</code>  <code>async</code>","text":"<p>Run the agent asynchronously with the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The user prompt to be passed to the agent.</p> required <code>kwargs</code> <code>Any</code> <p>Will be passed to the underlying runner used by the framework.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AgentTrace</code> <p>The <code>AgentTrace</code> containing information about the steps taken by the agent.</p> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>async def run_async(self, prompt: str, **kwargs: Any) -&gt; AgentTrace:\n    \"\"\"Run the agent asynchronously with the given prompt.\n\n    Args:\n        prompt: The user prompt to be passed to the agent.\n\n        kwargs: Will be passed to the underlying runner used\n            by the framework.\n\n    Returns:\n        The `AgentTrace` containing information about the\n            steps taken by the agent.\n\n    \"\"\"\n    trace = AgentTrace()\n    trace_id: int\n\n    # This design is so that we only catch exceptions thrown by _run_async. All other exceptions will not be caught.\n    try:\n        with self._tracer.start_as_current_span(\n            f\"invoke_agent [{self.config.name}]\"\n        ) as invoke_span:\n            async with self._lock:\n                trace_id = invoke_span.get_span_context().trace_id\n                self._wrapper.callback_context[trace_id] = Context(\n                    current_span=invoke_span,\n                    trace=AgentTrace(),\n                    tracer=self._tracer,\n                    shared={},\n                )\n\n                if len(self._wrapper.callback_context) == 1:\n                    # If there is more than 1 entry in `callback_context`, it means that the agent has\n                    # already being wrapped so we won't wrap it again.\n                    await self._wrapper.wrap(\n                        agent=self,  # type: ignore[arg-type]\n                    )\n\n            # Importing here to avoid circular import issues\n            from any_agent import __version__ as _ANY_AGENT_VERSION  # noqa: N812\n\n            invoke_span.set_attributes(\n                {\n                    GenAI.OPERATION_NAME: \"invoke_agent\",\n                    GenAI.AGENT_NAME: self.config.name,\n                    GenAI.AGENT_DESCRIPTION: self.config.description\n                    or \"No description.\",\n                    GenAI.REQUEST_MODEL: self.config.model_id,\n                    AnyAgentAttributes.VERSION: _ANY_AGENT_VERSION,\n                }\n            )\n\n            context = self._wrapper.callback_context[trace_id]\n            for callback in self.config.callbacks:\n                result = callback.before_agent_invocation(context, prompt, **kwargs)\n                if asyncio.iscoroutinefunction(callback.before_agent_invocation):\n                    context = await result  # type: ignore[misc]\n                else:\n                    context = result\n\n            final_output = await self._run_async(prompt, **kwargs)\n\n    except Exception as e:\n        async with self._lock:\n            if len(self._wrapper.callback_context) == 1:\n                await self._wrapper.unwrap(self)  # type: ignore[arg-type]\n            wrapped_context = self._wrapper.callback_context.pop(trace_id, None)\n            if wrapped_context is not None:\n                trace = wrapped_context.trace\n                for callback in self.config.callbacks:\n                    assert wrapped_context is not None\n                    result = callback.after_agent_invocation(\n                        wrapped_context, prompt, **kwargs\n                    )\n                    if asyncio.iscoroutinefunction(callback.after_agent_invocation):\n                        wrapped_context = await result  # type: ignore[misc]\n                    else:\n                        wrapped_context = result\n\n        trace.add_span(invoke_span)\n\n        # Preserve control-flow exceptions without wrapping.\n        if isinstance(e, AgentCancel):\n            e._trace = trace\n            raise\n\n        # Check if the framework wrapped an AgentCancel in its own error type.\n        if cancel := _unwrap_agent_cancel(e):\n            cancel._trace = trace\n            raise cancel from e\n\n        raise AgentRunError(trace, e) from e\n\n    async with self._lock:\n        if len(self._wrapper.callback_context) == 1:\n            await self._wrapper.unwrap(self)  # type: ignore[arg-type]\n        wrapped_context = self._wrapper.callback_context.pop(trace_id, None)\n        if wrapped_context is not None:\n            trace = wrapped_context.trace\n            for callback in self.config.callbacks:\n                assert wrapped_context is not None\n                result = callback.after_agent_invocation(\n                    wrapped_context, prompt, **kwargs\n                )\n                if asyncio.iscoroutinefunction(callback.after_agent_invocation):\n                    wrapped_context = await result  # type: ignore[misc]\n                else:\n                    wrapped_context = result\n\n    trace.add_span(invoke_span)\n    trace.final_output = final_output\n    return trace\n</code></pre>"},{"location":"api/agent/#any_agent.AnyAgent.serve_async","title":"<code>serve_async(serving_config=None)</code>  <code>async</code>","text":"<pre><code>serve_async(serving_config: MCPServingConfig) -&gt; ServerHandle\n</code></pre><pre><code>serve_async(serving_config: A2AServingConfig | None = None) -&gt; ServerHandle\n</code></pre> <p>Serve this agent asynchronously using the protocol defined in the serving_config.</p> <p>Parameters:</p> Name Type Description Default <code>serving_config</code> <code>MCPServingConfig | A2AServingConfig | None</code> <p>Configuration for serving the agent. If None, uses default A2AServingConfig.           Must be an instance of A2AServingConfig or MCPServingConfig.</p> <code>None</code> <p>Returns:</p> Type Description <code>ServerHandle</code> <p>A ServerHandle instance that provides methods for managing the server lifecycle.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the <code>a2a</code> dependencies are not installed and an <code>A2AServingConfig</code> is used.</p> Example <pre><code>agent = await AnyAgent.create_async(\"tinyagent\", AgentConfig(...))\nconfig = MCPServingConfig(port=8080)\nserver_handle = await agent.serve_async(config)\ntry:\n    # Server is running\n    await asyncio.sleep(10)\nfinally:\n    await server_handle.shutdown()\n</code></pre> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>async def serve_async(\n    self, serving_config: MCPServingConfig | A2AServingConfig | None = None\n) -&gt; ServerHandle:\n    \"\"\"Serve this agent asynchronously using the protocol defined in the serving_config.\n\n    Args:\n        serving_config: Configuration for serving the agent. If None, uses default A2AServingConfig.\n                      Must be an instance of A2AServingConfig or MCPServingConfig.\n\n    Returns:\n        A ServerHandle instance that provides methods for managing the server lifecycle.\n\n    Raises:\n        ImportError: If the `a2a` dependencies are not installed and an `A2AServingConfig` is used.\n\n    Example:\n        ```\n        agent = await AnyAgent.create_async(\"tinyagent\", AgentConfig(...))\n        config = MCPServingConfig(port=8080)\n        server_handle = await agent.serve_async(config)\n        try:\n            # Server is running\n            await asyncio.sleep(10)\n        finally:\n            await server_handle.shutdown()\n        ```\n\n    \"\"\"\n    from any_agent.serving import MCPServingConfig\n\n    if isinstance(serving_config, MCPServingConfig):\n        return await self._serve_mcp_async(serving_config)\n    return await self._serve_a2a_async(serving_config)\n</code></pre>"},{"location":"api/agent/#any_agent.AnyAgent.update_output_type_async","title":"<code>update_output_type_async(output_type)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Update the output type of the agent in-place.</p> <p>This method allows updating the agent's output type without recreating the entire agent instance, which is more efficient than the current approach of recreating the agent.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>type[BaseModel] | None</code> <p>The new output type to use, or None to remove output type constraint</p> required Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>@abstractmethod\nasync def update_output_type_async(\n    self, output_type: type[BaseModel] | None\n) -&gt; None:\n    \"\"\"Update the output type of the agent in-place.\n\n    This method allows updating the agent's output type without recreating\n    the entire agent instance, which is more efficient than the current\n    approach of recreating the agent.\n\n    Args:\n        output_type: The new output type to use, or None to remove output type constraint\n\n    \"\"\"\n</code></pre>"},{"location":"api/agent/#any_agent.AgentCancel","title":"<code>any_agent.AgentCancel</code>","text":"<p>               Bases: <code>ABC</code>, <code>Exception</code></p> <p>Abstract base class for control-flow exceptions raised in callbacks.</p> <p>Within a callback, raise an exception inherited from AgentCancel when you want to intentionally stop agent execution and handle that specific case in your application code.</p> <p>Unlike regular exceptions (which are wrapped in AgentRunError), AgentCancel subclasses propagate directly to the caller, allowing you to catch them by their specific type.</p> When to use AgentCancel vs regular exceptions <ul> <li>Use AgentCancel: When stopping execution is expected behavior   (rate limits, safety guardrails, validation failures) and you   want to handle it distinctly in your application.</li> <li>Use regular exceptions: When something unexpected goes wrong,   and you want consistent error handling via AgentRunError.</li> </ul> Example <p>class StopOnLimit(AgentCancel):     pass</p> <p>class LimitCallsCallback(Callback):     def before_tool_execution(self, context, args, *kwargs):         if context.shared.get(\"call_count\", 0) &gt; 10:             raise StopOnLimit(\"Exceeded call limit\")         return context</p> <p>try:     agent.run(\"prompt\") except StopOnLimit as e:     # Handle the expected cancellation.     print(f\"Canceled: {e}\")     print(f\"Collected {len(e.trace.spans)} spans\") except AgentRunError as e:     # Handle unexpected errors.     print(f\"Unexpected error: {e.original_exception}\")</p> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>class AgentCancel(ABC, Exception):  # noqa: N818\n    \"\"\"Abstract base class for control-flow exceptions raised in callbacks.\n\n    Within a callback, raise an exception inherited from AgentCancel when you\n    want to intentionally stop agent execution and handle that specific case in\n    your application code.\n\n    Unlike regular exceptions (which are wrapped in AgentRunError), AgentCancel\n    subclasses propagate directly to the caller, allowing you to catch them by\n    their specific type.\n\n    When to use AgentCancel vs regular exceptions:\n        - Use AgentCancel: When stopping execution is expected behavior\n          (rate limits, safety guardrails, validation failures) and you\n          want to handle it distinctly in your application.\n        - Use regular exceptions: When something unexpected goes wrong,\n          and you want consistent error handling via AgentRunError.\n\n    Example:\n        class StopOnLimit(AgentCancel):\n            pass\n\n        class LimitCallsCallback(Callback):\n            def before_tool_execution(self, context, *args, **kwargs):\n                if context.shared.get(\"call_count\", 0) &gt; 10:\n                    raise StopOnLimit(\"Exceeded call limit\")\n                return context\n\n        try:\n            agent.run(\"prompt\")\n        except StopOnLimit as e:\n            # Handle the expected cancellation.\n            print(f\"Canceled: {e}\")\n            print(f\"Collected {len(e.trace.spans)} spans\")\n        except AgentRunError as e:\n            # Handle unexpected errors.\n            print(f\"Unexpected error: {e.original_exception}\")\n\n    \"\"\"\n\n    _trace: AgentTrace | None\n\n    def __new__(cls, *args: Any, **kwargs: Any) -&gt; Self:\n        if cls is AgentCancel:\n            msg = \"AgentCancel cannot be instantiated directly; subclass it instead\"\n            raise TypeError(msg)\n        return super().__new__(cls)\n\n    def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self._trace = None\n\n    @property\n    def trace(self) -&gt; AgentTrace | None:\n        \"\"\"Execution trace collected before cancellation.\n\n        Returns None if accessed before the framework processes the exception.\n        \"\"\"\n        return self._trace\n</code></pre>"},{"location":"api/agent/#any_agent.AgentCancel.trace","title":"<code>trace</code>  <code>property</code>","text":"<p>Execution trace collected before cancellation.</p> <p>Returns None if accessed before the framework processes the exception.</p>"},{"location":"api/agent/#any_agent.AgentRunError","title":"<code>any_agent.AgentRunError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Wrapper for unexpected exceptions that occur during agent execution.</p> <p>When an unexpected exception is raised during agent execution (from callbacks, tools, or the underlying framework), it is caught and wrapped in AgentRunError.</p> Exceptions that inherit from AgentCancel are not wrapped, <p>they propagate directly to the caller.</p> <p>AgentRunError ensures:</p> <ul> <li>The execution trace is preserved - you can inspect what happened    before the error via the <code>trace</code> property.</li> <li>Consistent error handling - all unexpected errors are wrapped in    the same type, regardless of the underlying framework.</li> <li>Original exception access - the wrapped exception is available    via <code>original_exception</code> for debugging.</li> </ul> Example <p>try:     agent.run(\"prompt\") except AgentRunError as e:     print(f\"Error: {e.original_exception}\")     print(f\"Trace had {len(e.trace.spans)} spans before failure\")</p> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>class AgentRunError(Exception):\n    \"\"\"Wrapper for unexpected exceptions that occur during agent execution.\n\n    When an unexpected exception is raised during agent execution (from\n    callbacks, tools, or the underlying framework), it is caught and\n    wrapped in AgentRunError.\n\n    Note: Exceptions that inherit from AgentCancel are not wrapped,\n        they propagate directly to the caller.\n\n    AgentRunError ensures:\n\n    * The execution trace is preserved - you can inspect what happened\n       before the error via the `trace` property.\n    * Consistent error handling - all unexpected errors are wrapped in\n       the same type, regardless of the underlying framework.\n    * Original exception access - the wrapped exception is available\n       via `original_exception` for debugging.\n\n    Example:\n        try:\n            agent.run(\"prompt\")\n        except AgentRunError as e:\n            print(f\"Error: {e.original_exception}\")\n            print(f\"Trace had {len(e.trace.spans)} spans before failure\")\n\n    \"\"\"\n\n    _trace: AgentTrace\n    _original_exception: Exception\n\n    def __init__(self, trace: AgentTrace, original_exception: Exception):\n        self._trace = trace\n        self._original_exception = original_exception\n        super().__init__(str(original_exception))\n\n    @property\n    def trace(self) -&gt; AgentTrace:\n        \"\"\"The execution trace collected up to the point of failure.\"\"\"\n        return self._trace\n\n    @property\n    def original_exception(self) -&gt; Exception:\n        \"\"\"The underlying exception that was caught.\"\"\"\n        return self._original_exception\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the original exception.\"\"\"\n        return str(self._original_exception)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the detailed representation of the AgentRunError.\"\"\"\n        return f\"AgentRunError({self._original_exception!r})\"\n</code></pre>"},{"location":"api/agent/#any_agent.AgentRunError.original_exception","title":"<code>original_exception</code>  <code>property</code>","text":"<p>The underlying exception that was caught.</p>"},{"location":"api/agent/#any_agent.AgentRunError.trace","title":"<code>trace</code>  <code>property</code>","text":"<p>The execution trace collected up to the point of failure.</p>"},{"location":"api/agent/#any_agent.AgentRunError.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the detailed representation of the AgentRunError.</p> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the detailed representation of the AgentRunError.\"\"\"\n    return f\"AgentRunError({self._original_exception!r})\"\n</code></pre>"},{"location":"api/agent/#any_agent.AgentRunError.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the original exception.</p> Source code in <code>src/any_agent/frameworks/any_agent.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the original exception.\"\"\"\n    return str(self._original_exception)\n</code></pre>"},{"location":"api/callbacks/","title":"Callbacks","text":""},{"location":"api/callbacks/#any_agent.callbacks.base.Callback","title":"<code>any_agent.callbacks.base.Callback</code>","text":"<p>Base class for AnyAgent callbacks.</p> Source code in <code>src/any_agent/callbacks/base.py</code> <pre><code>class Callback:\n    \"\"\"Base class for AnyAgent callbacks.\"\"\"\n\n    def before_agent_invocation(self, context: Context, *args, **kwargs) -&gt; Context:\n        \"\"\"Will be called before the Agent invocation starts.\"\"\"\n        return context\n\n    def before_llm_call(self, context: Context, *args, **kwargs) -&gt; Context:\n        \"\"\"Will be called before any LLM Call starts.\"\"\"\n        return context\n\n    def before_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:\n        \"\"\"Will be called before any Tool Execution starts.\"\"\"\n        return context\n\n    def after_agent_invocation(self, context: Context, *args, **kwargs) -&gt; Context:\n        \"\"\"Will be called once the Agent invocation ends.\"\"\"\n        return context\n\n    def after_llm_call(self, context: Context, *args, **kwargs) -&gt; Context:\n        \"\"\"Will be called after any LLM Call is completed.\"\"\"\n        return context\n\n    def after_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:\n        \"\"\"Will be called after any Tool Execution is completed.\"\"\"\n        return context\n</code></pre>"},{"location":"api/callbacks/#any_agent.callbacks.base.Callback.after_agent_invocation","title":"<code>after_agent_invocation(context, *args, **kwargs)</code>","text":"<p>Will be called once the Agent invocation ends.</p> Source code in <code>src/any_agent/callbacks/base.py</code> <pre><code>def after_agent_invocation(self, context: Context, *args, **kwargs) -&gt; Context:\n    \"\"\"Will be called once the Agent invocation ends.\"\"\"\n    return context\n</code></pre>"},{"location":"api/callbacks/#any_agent.callbacks.base.Callback.after_llm_call","title":"<code>after_llm_call(context, *args, **kwargs)</code>","text":"<p>Will be called after any LLM Call is completed.</p> Source code in <code>src/any_agent/callbacks/base.py</code> <pre><code>def after_llm_call(self, context: Context, *args, **kwargs) -&gt; Context:\n    \"\"\"Will be called after any LLM Call is completed.\"\"\"\n    return context\n</code></pre>"},{"location":"api/callbacks/#any_agent.callbacks.base.Callback.after_tool_execution","title":"<code>after_tool_execution(context, *args, **kwargs)</code>","text":"<p>Will be called after any Tool Execution is completed.</p> Source code in <code>src/any_agent/callbacks/base.py</code> <pre><code>def after_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:\n    \"\"\"Will be called after any Tool Execution is completed.\"\"\"\n    return context\n</code></pre>"},{"location":"api/callbacks/#any_agent.callbacks.base.Callback.before_agent_invocation","title":"<code>before_agent_invocation(context, *args, **kwargs)</code>","text":"<p>Will be called before the Agent invocation starts.</p> Source code in <code>src/any_agent/callbacks/base.py</code> <pre><code>def before_agent_invocation(self, context: Context, *args, **kwargs) -&gt; Context:\n    \"\"\"Will be called before the Agent invocation starts.\"\"\"\n    return context\n</code></pre>"},{"location":"api/callbacks/#any_agent.callbacks.base.Callback.before_llm_call","title":"<code>before_llm_call(context, *args, **kwargs)</code>","text":"<p>Will be called before any LLM Call starts.</p> Source code in <code>src/any_agent/callbacks/base.py</code> <pre><code>def before_llm_call(self, context: Context, *args, **kwargs) -&gt; Context:\n    \"\"\"Will be called before any LLM Call starts.\"\"\"\n    return context\n</code></pre>"},{"location":"api/callbacks/#any_agent.callbacks.base.Callback.before_tool_execution","title":"<code>before_tool_execution(context, *args, **kwargs)</code>","text":"<p>Will be called before any Tool Execution starts.</p> Source code in <code>src/any_agent/callbacks/base.py</code> <pre><code>def before_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:\n    \"\"\"Will be called before any Tool Execution starts.\"\"\"\n    return context\n</code></pre>"},{"location":"api/callbacks/#any_agent.callbacks.context.Context","title":"<code>any_agent.callbacks.context.Context</code>  <code>dataclass</code>","text":"<p>Object that will be shared across callbacks.</p> <p>Each AnyAgent.run has a separate <code>Context</code> available.</p> <p><code>shared</code> can be used to store and pass information across different callbacks.</p> Source code in <code>src/any_agent/callbacks/context.py</code> <pre><code>@dataclass\nclass Context:\n    \"\"\"Object that will be shared across callbacks.\n\n    Each AnyAgent.run has a separate `Context` available.\n\n    `shared` can be used to store and pass information\n    across different callbacks.\n    \"\"\"\n\n    current_span: Span\n    \"\"\"You can use the span in your callbacks to get information consistently across frameworks.\n\n    You can find information about the attributes (available under `current_span.attributes`) in\n    [Attributes Reference](./tracing.md#any_agent.tracing.attributes).\n    \"\"\"\n\n    trace: AgentTrace\n    tracer: Tracer\n\n    shared: dict[str, Any]\n    \"\"\"Can be used to store arbitrary information for sharing across callbacks.\"\"\"\n</code></pre>"},{"location":"api/callbacks/#any_agent.callbacks.context.Context.current_span","title":"<code>current_span</code>  <code>instance-attribute</code>","text":"<p>You can use the span in your callbacks to get information consistently across frameworks.</p> <p>You can find information about the attributes (available under <code>current_span.attributes</code>) in Attributes Reference.</p>"},{"location":"api/callbacks/#any_agent.callbacks.context.Context.shared","title":"<code>shared</code>  <code>instance-attribute</code>","text":"<p>Can be used to store arbitrary information for sharing across callbacks.</p>"},{"location":"api/callbacks/#any_agent.callbacks.span_print.ConsolePrintSpan","title":"<code>any_agent.callbacks.span_print.ConsolePrintSpan</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Use rich's console to print the <code>Context.current_span</code>.</p> Source code in <code>src/any_agent/callbacks/span_print.py</code> <pre><code>class ConsolePrintSpan(Callback):\n    \"\"\"Use rich's console to print the `Context.current_span`.\"\"\"\n\n    def __init__(self, console: Console | None = None) -&gt; None:\n        \"\"\"Init the ConsolePrintSpan.\n\n        Args:\n            console: An optional instance of `rich.console.Console`.\n                If `None`, a new instance will be used.\n\n        \"\"\"\n        self.console = console or Console()\n\n    def after_llm_call(self, context: Context, *args, **kwargs) -&gt; Context:\n        span = context.current_span\n\n        operation_name = span.attributes.get(GenAI.OPERATION_NAME, \"\")\n\n        if operation_name != \"call_llm\":\n            return context\n\n        panels = []\n\n        if messages := span.attributes.get(GenAI.INPUT_MESSAGES):\n            panels.append(\n                Panel(JSON(messages), title=\"INPUT\", style=\"white\", title_align=\"left\")\n            )\n\n        if output_panel := _get_output_panel(span):\n            panels.append(output_panel)\n\n        if usage := {\n            k.replace(\"gen_ai.usage.\", \"\"): v\n            for k, v in span.attributes.items()\n            if \"usage\" in k\n        }:\n            panels.append(\n                Panel(\n                    JSON(json.dumps(usage)),\n                    title=\"USAGE\",\n                    style=\"white\",\n                    title_align=\"left\",\n                )\n            )\n\n        self.console.print(\n            Panel(\n                Group(*panels),\n                title=f\"{operation_name.upper()}: {span.attributes.get(GenAI.REQUEST_MODEL)}\",\n                style=\"yellow\",\n            )\n        )\n\n        return context\n\n    def after_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:\n        span = context.current_span\n\n        operation_name = span.attributes.get(GenAI.OPERATION_NAME, \"\")\n\n        if operation_name != \"execute_tool\":\n            return context\n\n        panels = [\n            Panel(\n                JSON(span.attributes.get(GenAI.TOOL_ARGS, \"{}\")),\n                title=\"Input\",\n                style=\"white\",\n                title_align=\"left\",\n            )\n        ]\n\n        if output_panel := _get_output_panel(span):\n            panels.append(output_panel)\n\n        self.console.print(\n            Panel(\n                Group(*panels),\n                title=f\"{operation_name.upper()}: {span.attributes.get(GenAI.TOOL_NAME)}\",\n                style=\"blue\",\n            )\n        )\n        return context\n</code></pre>"},{"location":"api/callbacks/#any_agent.callbacks.span_print.ConsolePrintSpan.__init__","title":"<code>__init__(console=None)</code>","text":"<p>Init the ConsolePrintSpan.</p> <p>Parameters:</p> Name Type Description Default <code>console</code> <code>Console | None</code> <p>An optional instance of <code>rich.console.Console</code>. If <code>None</code>, a new instance will be used.</p> <code>None</code> Source code in <code>src/any_agent/callbacks/span_print.py</code> <pre><code>def __init__(self, console: Console | None = None) -&gt; None:\n    \"\"\"Init the ConsolePrintSpan.\n\n    Args:\n        console: An optional instance of `rich.console.Console`.\n            If `None`, a new instance will be used.\n\n    \"\"\"\n    self.console = console or Console()\n</code></pre>"},{"location":"api/callbacks/#any_agent.callbacks.get_default_callbacks","title":"<code>any_agent.callbacks.get_default_callbacks()</code>","text":"<p>Return instances of the default callbacks used in any-agent.</p> <p>This function is called internally when the user doesn't provide a value for <code>AgentConfig.callbacks</code>.</p> <p>Returns:</p> Type Description <code>list[Callback]</code> <p>A list of instances containing:</p> <ul> <li><code>ConsolePrintSpan</code></li> </ul> Source code in <code>src/any_agent/callbacks/__init__.py</code> <pre><code>def get_default_callbacks() -&gt; list[Callback]:\n    \"\"\"Return instances of the default callbacks used in any-agent.\n\n    This function is called internally when the user doesn't provide a\n    value for [`AgentConfig.callbacks`][any_agent.config.AgentConfig.callbacks].\n\n    Returns:\n        A list of instances containing:\n\n            - [`ConsolePrintSpan`][any_agent.callbacks.span_print.ConsolePrintSpan]\n\n    \"\"\"\n    return [ConsolePrintSpan()]\n</code></pre>"},{"location":"api/config/","title":"Config","text":""},{"location":"api/config/#any_agent.config.AgentConfig","title":"<code>any_agent.config.AgentConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/any_agent/config.py</code> <pre><code>class AgentConfig(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True, extra=\"forbid\")\n\n    model_id: str\n    \"\"\"Select the underlying model used by the agent.\n\n    If you are using the default model_type (AnyLLM), you can refer to [AnyLLM Provider Docs](https://mozilla-ai.github.io/any-llm/providers/) for the list of providers and how to access them.\n    \"\"\"\n\n    api_base: str | None = None\n    \"\"\"Custom API endpoint URL for the model provider.\n\n    Use this to specify custom endpoints for local models (Ollama, llama.cpp, etc.) or proxy services.\n    For example: `http://localhost:11434/v1` for Ollama.\n    \"\"\"\n\n    api_key: str | None = None\n    \"\"\"API key for authenticating with the model provider.\n\n    By default, any-llm automatically searches for common environment variables (OPENAI_API_KEY, ANTHROPIC_API_KEY, etc.).\n    Only set this explicitly when using custom environment variable names or providing keys dynamically.\n    \"\"\"\n\n    description: str | None = None\n    \"\"\"Description of the agent.\"\"\"\n\n    name: str = \"any_agent\"\n    \"\"\"The name of the agent.\n\n    Defaults to `any_agent`.\n    \"\"\"\n\n    instructions: str | None = None\n    \"\"\"Specify the instructions for the agent (often also referred to as a `system_prompt`).\"\"\"\n\n    tools: list[Tool] = Field(default_factory=list)\n    \"\"\"List of tools to be used by the agent.\n\n    See more info at [Tools](../agents/tools.md).\n    \"\"\"\n\n    callbacks: list[Callback] = Field(default_factory=get_default_callbacks)\n    \"\"\"List of callbacks to use during agent invocation.\n\n    See more info at [Callbacks](../agents/callbacks.md).\n    \"\"\"\n\n    agent_type: Callable[..., Any] | None = None\n    \"\"\"Control the type of agent class that is used by the framework, and is unique to the framework used.\n\n    Check the individual `Frameworks` pages for more info on the defaults.\n    \"\"\"\n\n    agent_args: MutableMapping[str, Any] | None = None\n    \"\"\"Pass arguments to the instance used by the underlying framework.\n\n    For example, you can pass `output_type` when using the OpenAI Agents SDK:\n\n    ```py\n    from pydantic import BaseModel\n\n    class CalendarEvent(BaseModel):\n        name: str\n        date: str\n        participants: list[str]\n\n    agent = AnyAgent.create(\n        AgentConfig(\n            model_id=\"mistral:mistral-small-latest\",\n            instructions=\"Extract calendar events from text\",\n            agent_args={\n                \"output_type\": CalendarEvent\n            }\n        )\n    )\n    ```\n    \"\"\"\n\n    model_type: Callable[..., Any] | None = None\n    \"\"\"Control the type of model class that is used by the agent framework, and is unique to the agent framework being used.\n\n    For each framework, we use AnyLLM as the default model_type, allowing you to use the same model_id syntax across these frameworks.\n    \"\"\"\n\n    model_args: MutableMapping[str, Any] | None = None\n    \"\"\"Pass arguments to the model instance like `temperature`, `top_k`, as well as any other provider-specific parameters.\n\n    Refer to [any-llm Completion API Docs](https://mozilla-ai.github.io/any-llm/api/completion/) for more info.\n    \"\"\"\n\n    output_type: type[BaseModel] | None = None\n    \"\"\"Control the output schema from calling `run`. By default, the agent will return a type str.\n\n    Using this parameter you can define a Pydantic model that will be returned by the agent run methods.\n    \"\"\"\n</code></pre>"},{"location":"api/config/#any_agent.config.AgentConfig.agent_args","title":"<code>agent_args = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Pass arguments to the instance used by the underlying framework.</p> <p>For example, you can pass <code>output_type</code> when using the OpenAI Agents SDK:</p> <pre><code>from pydantic import BaseModel\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\nagent = AnyAgent.create(\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\",\n        instructions=\"Extract calendar events from text\",\n        agent_args={\n            \"output_type\": CalendarEvent\n        }\n    )\n)\n</code></pre>"},{"location":"api/config/#any_agent.config.AgentConfig.agent_type","title":"<code>agent_type = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Control the type of agent class that is used by the framework, and is unique to the framework used.</p> <p>Check the individual <code>Frameworks</code> pages for more info on the defaults.</p>"},{"location":"api/config/#any_agent.config.AgentConfig.api_base","title":"<code>api_base = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom API endpoint URL for the model provider.</p> <p>Use this to specify custom endpoints for local models (Ollama, llama.cpp, etc.) or proxy services. For example: <code>http://localhost:11434/v1</code> for Ollama.</p>"},{"location":"api/config/#any_agent.config.AgentConfig.api_key","title":"<code>api_key = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>API key for authenticating with the model provider.</p> <p>By default, any-llm automatically searches for common environment variables (OPENAI_API_KEY, ANTHROPIC_API_KEY, etc.). Only set this explicitly when using custom environment variable names or providing keys dynamically.</p>"},{"location":"api/config/#any_agent.config.AgentConfig.callbacks","title":"<code>callbacks = Field(default_factory=get_default_callbacks)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of callbacks to use during agent invocation.</p> <p>See more info at Callbacks.</p>"},{"location":"api/config/#any_agent.config.AgentConfig.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Description of the agent.</p>"},{"location":"api/config/#any_agent.config.AgentConfig.instructions","title":"<code>instructions = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Specify the instructions for the agent (often also referred to as a <code>system_prompt</code>).</p>"},{"location":"api/config/#any_agent.config.AgentConfig.model_args","title":"<code>model_args = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Pass arguments to the model instance like <code>temperature</code>, <code>top_k</code>, as well as any other provider-specific parameters.</p> <p>Refer to any-llm Completion API Docs for more info.</p>"},{"location":"api/config/#any_agent.config.AgentConfig.model_id","title":"<code>model_id</code>  <code>instance-attribute</code>","text":"<p>Select the underlying model used by the agent.</p> <p>If you are using the default model_type (AnyLLM), you can refer to AnyLLM Provider Docs for the list of providers and how to access them.</p>"},{"location":"api/config/#any_agent.config.AgentConfig.model_type","title":"<code>model_type = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Control the type of model class that is used by the agent framework, and is unique to the agent framework being used.</p> <p>For each framework, we use AnyLLM as the default model_type, allowing you to use the same model_id syntax across these frameworks.</p>"},{"location":"api/config/#any_agent.config.AgentConfig.name","title":"<code>name = 'any_agent'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the agent.</p> <p>Defaults to <code>any_agent</code>.</p>"},{"location":"api/config/#any_agent.config.AgentConfig.output_type","title":"<code>output_type = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Control the output schema from calling <code>run</code>. By default, the agent will return a type str.</p> <p>Using this parameter you can define a Pydantic model that will be returned by the agent run methods.</p>"},{"location":"api/config/#any_agent.config.AgentConfig.tools","title":"<code>tools = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of tools to be used by the agent.</p> <p>See more info at Tools.</p>"},{"location":"api/config/#any_agent.config.MCPStdio","title":"<code>any_agent.config.MCPStdio</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/any_agent/config.py</code> <pre><code>class MCPStdio(BaseModel):\n    command: str\n    \"\"\"The executable to run to start the server.\n\n    For example, `docker`, `uvx`, `npx`.\n    \"\"\"\n\n    args: Sequence[str]\n    \"\"\"Command line args to pass to the command executable.\n\n    For example, `[\"run\", \"-i\", \"--rm\", \"mcp/fetch\"]`.\n    \"\"\"\n\n    env: dict[str, str] | None = None\n    \"\"\"The environment variables to set for the server.\"\"\"\n\n    tools: Sequence[str] | None = None\n    \"\"\"List of tool names to use from the MCP Server.\n\n    Use it to limit the tools accessible by the agent.\n    For example, if you use [`mcp/filesystem`](https://hub.docker.com/r/mcp/filesystem),\n    you can pass `tools=[\"read_file\", \"list_directory\"]` to limit the agent to read-only operations.\n\n    If none is specified, the default behavior is that the agent will have access to all tools under that MCP server.\n    \"\"\"\n\n    client_session_timeout_seconds: float | None = 5\n    \"\"\"the read timeout passed to the MCP ClientSession.\"\"\"\n\n    model_config = ConfigDict(frozen=True, extra=\"forbid\")\n</code></pre>"},{"location":"api/config/#any_agent.config.MCPStdio.args","title":"<code>args</code>  <code>instance-attribute</code>","text":"<p>Command line args to pass to the command executable.</p> <p>For example, <code>[\"run\", \"-i\", \"--rm\", \"mcp/fetch\"]</code>.</p>"},{"location":"api/config/#any_agent.config.MCPStdio.client_session_timeout_seconds","title":"<code>client_session_timeout_seconds = 5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>the read timeout passed to the MCP ClientSession.</p>"},{"location":"api/config/#any_agent.config.MCPStdio.command","title":"<code>command</code>  <code>instance-attribute</code>","text":"<p>The executable to run to start the server.</p> <p>For example, <code>docker</code>, <code>uvx</code>, <code>npx</code>.</p>"},{"location":"api/config/#any_agent.config.MCPStdio.env","title":"<code>env = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The environment variables to set for the server.</p>"},{"location":"api/config/#any_agent.config.MCPStdio.tools","title":"<code>tools = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of tool names to use from the MCP Server.</p> <p>Use it to limit the tools accessible by the agent. For example, if you use <code>mcp/filesystem</code>, you can pass <code>tools=[\"read_file\", \"list_directory\"]</code> to limit the agent to read-only operations.</p> <p>If none is specified, the default behavior is that the agent will have access to all tools under that MCP server.</p>"},{"location":"api/config/#any_agent.config.MCPStreamableHttp","title":"<code>any_agent.config.MCPStreamableHttp</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/any_agent/config.py</code> <pre><code>class MCPStreamableHttp(BaseModel):\n    url: str\n    \"\"\"The URL of the server.\"\"\"\n\n    headers: Mapping[str, str] | None = None\n    \"\"\"The headers to send to the server.\"\"\"\n\n    tools: Sequence[str] | None = None\n    \"\"\"List of tool names to use from the MCP Server.\n\n    Use it to limit the tools accessible by the agent.\n    For example, if you use [`mcp/filesystem`](https://hub.docker.com/r/mcp/filesystem),\n    you can pass `tools=[\"read_file\", \"list_directory\"]` to limit the agent to read-only operations.\n    \"\"\"\n\n    client_session_timeout_seconds: float | None = 5\n    \"\"\"the read timeout passed to the MCP ClientSession.\"\"\"\n\n    model_config = ConfigDict(frozen=True)\n</code></pre>"},{"location":"api/config/#any_agent.config.MCPStreamableHttp.client_session_timeout_seconds","title":"<code>client_session_timeout_seconds = 5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>the read timeout passed to the MCP ClientSession.</p>"},{"location":"api/config/#any_agent.config.MCPStreamableHttp.headers","title":"<code>headers = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The headers to send to the server.</p>"},{"location":"api/config/#any_agent.config.MCPStreamableHttp.tools","title":"<code>tools = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of tool names to use from the MCP Server.</p> <p>Use it to limit the tools accessible by the agent. For example, if you use <code>mcp/filesystem</code>, you can pass <code>tools=[\"read_file\", \"list_directory\"]</code> to limit the agent to read-only operations.</p>"},{"location":"api/config/#any_agent.config.MCPStreamableHttp.url","title":"<code>url</code>  <code>instance-attribute</code>","text":"<p>The URL of the server.</p>"},{"location":"api/config/#any_agent.config.MCPSse","title":"<code>any_agent.config.MCPSse</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/any_agent/config.py</code> <pre><code>class MCPSse(BaseModel):\n    @model_validator(mode=\"before\")\n    @classmethod\n    def sse_deprecation(cls, data: Any) -&gt; Any:\n        warnings.warn(\n            \"SSE is deprecated in the MCP specification in favor of Streamable HTTP as of version 2025-03-26\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return data\n\n    url: str\n    \"\"\"The URL of the server.\"\"\"\n\n    headers: Mapping[str, str] | None = None\n    \"\"\"The headers to send to the server.\"\"\"\n\n    tools: Sequence[str] | None = None\n    \"\"\"List of tool names to use from the MCP Server.\n\n    Use it to limit the tools accessible by the agent.\n    For example, if you use [`mcp/filesystem`](https://hub.docker.com/r/mcp/filesystem),\n    you can pass `tools=[\"read_file\", \"list_directory\"]` to limit the agent to read-only operations.\n    \"\"\"\n\n    client_session_timeout_seconds: float | None = 5\n    \"\"\"the read timeout passed to the MCP ClientSession.\"\"\"\n\n    model_config = ConfigDict(frozen=True)\n</code></pre>"},{"location":"api/config/#any_agent.config.MCPSse.client_session_timeout_seconds","title":"<code>client_session_timeout_seconds = 5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>the read timeout passed to the MCP ClientSession.</p>"},{"location":"api/config/#any_agent.config.MCPSse.headers","title":"<code>headers = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The headers to send to the server.</p>"},{"location":"api/config/#any_agent.config.MCPSse.tools","title":"<code>tools = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of tool names to use from the MCP Server.</p> <p>Use it to limit the tools accessible by the agent. For example, if you use <code>mcp/filesystem</code>, you can pass <code>tools=[\"read_file\", \"list_directory\"]</code> to limit the agent to read-only operations.</p>"},{"location":"api/config/#any_agent.config.MCPSse.url","title":"<code>url</code>  <code>instance-attribute</code>","text":"<p>The URL of the server.</p>"},{"location":"api/config/#any_agent.serving.A2AServingConfig","title":"<code>any_agent.serving.A2AServingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for serving an agent using the Agent2Agent Protocol (A2A).</p> Example <p>config = A2AServingConfig(     port=8080,     endpoint=\"/my-agent\",     skills=[         AgentSkill(             id=\"search\",             name=\"web_search\",             description=\"Search the web for information\"         )     ],     context_timeout_minutes=15 )</p> Source code in <code>src/any_agent/serving/a2a/config_a2a.py</code> <pre><code>class A2AServingConfig(BaseModel):\n    \"\"\"Configuration for serving an agent using the Agent2Agent Protocol (A2A).\n\n    Example:\n        config = A2AServingConfig(\n            port=8080,\n            endpoint=\"/my-agent\",\n            skills=[\n                AgentSkill(\n                    id=\"search\",\n                    name=\"web_search\",\n                    description=\"Search the web for information\"\n                )\n            ],\n            context_timeout_minutes=15\n        )\n\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    host: str = \"localhost\"\n    \"\"\"Will be passed as argument to `uvicorn.run`.\"\"\"\n\n    port: int = 5000\n    \"\"\"Will be passed as argument to `uvicorn.run`.\"\"\"\n\n    endpoint: str = \"/\"\n    \"\"\"Will be pass as argument to `Starlette().add_route`\"\"\"\n\n    log_level: str = \"warning\"\n    \"\"\"Will be passed as argument to the `uvicorn` server.\"\"\"\n\n    skills: list[AgentSkill] | None = None\n    \"\"\"List of skills to be used by the agent.\n\n    If not provided, the skills will be inferred from the tools.\n\n    By default, an agent's skills are automatically inferred from its tools.\n    However, you can explicitly define skills for more control over the agent card:\n\n    ```python\n    from a2a.types import AgentSkill\n    from any_agent.serving import A2AServingConfig\n\n    # Define custom skills\n    custom_skills = [\n        AgentSkill(\n            id=\"web-search\",\n            name=\"search_web\",\n            description=\"Search the web for current information\",\n            tags=[\"search\", \"web\", \"information\"]\n        ),\n        AgentSkill(\n            id=\"data-analysis\",\n            name=\"analyze_data\",\n            description=\"Analyze datasets and provide insights\",\n            tags=[\"analysis\", \"data\", \"insights\"]\n        )\n    ]\n\n    config = A2AServingConfig(\n        port=8080,\n        skills=custom_skills\n    )\n    ```\n    \"\"\"\n\n    version: str = \"0.1.0\"\n\n    context_timeout_minutes: int = 10\n    \"\"\"Context timeout in minutes. Contexts will be cleaned up after this period of inactivity.\"\"\"\n\n    history_formatter: HistoryFormatter = default_history_formatter\n    \"\"\"Function to format conversation history and current query into a single prompt.\n    Takes (messages, current_query) and returns formatted string.\"\"\"\n\n    task_cleanup_interval_minutes: int = 5\n    \"\"\"Interval in minutes between task cleanup runs.\"\"\"\n\n    push_notifier_store_type: type[PushNotificationConfigStore] = (\n        InMemoryPushNotificationConfigStore\n    )\n\n    \"\"\"Push notifier config store to be used by the agent.\n\n    If not provided, a default in-memory push notifier config store will be used.\n    \"\"\"\n\n    push_notifier_sender_type: type[PushNotificationSender] = BasePushNotificationSender\n    \"\"\"Push notifier sender to be used by the agent.\n\n    If not provided, a default async httpx-based push notifier sender will be used.\n    \"\"\"\n\n    task_store_type: type[TaskStore] = InMemoryTaskStore\n    \"\"\"Task store to be used by the agent.\n\n    If not provided, a default in-memory task store will be used.\n    \"\"\"\n\n    stream_tool_usage: bool = False\n    \"\"\"whether to stream tool execution results\"\"\"\n</code></pre>"},{"location":"api/config/#any_agent.serving.A2AServingConfig.context_timeout_minutes","title":"<code>context_timeout_minutes = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Context timeout in minutes. Contexts will be cleaned up after this period of inactivity.</p>"},{"location":"api/config/#any_agent.serving.A2AServingConfig.endpoint","title":"<code>endpoint = '/'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Will be pass as argument to <code>Starlette().add_route</code></p>"},{"location":"api/config/#any_agent.serving.A2AServingConfig.history_formatter","title":"<code>history_formatter = default_history_formatter</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Function to format conversation history and current query into a single prompt. Takes (messages, current_query) and returns formatted string.</p>"},{"location":"api/config/#any_agent.serving.A2AServingConfig.host","title":"<code>host = 'localhost'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Will be passed as argument to <code>uvicorn.run</code>.</p>"},{"location":"api/config/#any_agent.serving.A2AServingConfig.log_level","title":"<code>log_level = 'warning'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Will be passed as argument to the <code>uvicorn</code> server.</p>"},{"location":"api/config/#any_agent.serving.A2AServingConfig.port","title":"<code>port = 5000</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Will be passed as argument to <code>uvicorn.run</code>.</p>"},{"location":"api/config/#any_agent.serving.A2AServingConfig.push_notifier_sender_type","title":"<code>push_notifier_sender_type = BasePushNotificationSender</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Push notifier sender to be used by the agent.</p> <p>If not provided, a default async httpx-based push notifier sender will be used.</p>"},{"location":"api/config/#any_agent.serving.A2AServingConfig.push_notifier_store_type","title":"<code>push_notifier_store_type = InMemoryPushNotificationConfigStore</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Push notifier config store to be used by the agent.</p> <p>If not provided, a default in-memory push notifier config store will be used.</p>"},{"location":"api/config/#any_agent.serving.A2AServingConfig.skills","title":"<code>skills = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of skills to be used by the agent.</p> <p>If not provided, the skills will be inferred from the tools.</p> <p>By default, an agent's skills are automatically inferred from its tools. However, you can explicitly define skills for more control over the agent card:</p> <pre><code>from a2a.types import AgentSkill\nfrom any_agent.serving import A2AServingConfig\n\n# Define custom skills\ncustom_skills = [\n    AgentSkill(\n        id=\"web-search\",\n        name=\"search_web\",\n        description=\"Search the web for current information\",\n        tags=[\"search\", \"web\", \"information\"]\n    ),\n    AgentSkill(\n        id=\"data-analysis\",\n        name=\"analyze_data\",\n        description=\"Analyze datasets and provide insights\",\n        tags=[\"analysis\", \"data\", \"insights\"]\n    )\n]\n\nconfig = A2AServingConfig(\n    port=8080,\n    skills=custom_skills\n)\n</code></pre>"},{"location":"api/config/#any_agent.serving.A2AServingConfig.stream_tool_usage","title":"<code>stream_tool_usage = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>whether to stream tool execution results</p>"},{"location":"api/config/#any_agent.serving.A2AServingConfig.task_cleanup_interval_minutes","title":"<code>task_cleanup_interval_minutes = 5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Interval in minutes between task cleanup runs.</p>"},{"location":"api/config/#any_agent.serving.A2AServingConfig.task_store_type","title":"<code>task_store_type = InMemoryTaskStore</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Task store to be used by the agent.</p> <p>If not provided, a default in-memory task store will be used.</p>"},{"location":"api/config/#any_agent.serving.MCPServingConfig","title":"<code>any_agent.serving.MCPServingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for serving an agent using the Model Context Protocol (MCP).</p> Example <p>config = MCPServingConfig(     port=8080,     endpoint=\"/my-agent\", )</p> Source code in <code>src/any_agent/serving/mcp/config_mcp.py</code> <pre><code>class MCPServingConfig(BaseModel):\n    \"\"\"Configuration for serving an agent using the Model Context Protocol (MCP).\n\n    Example:\n        config = MCPServingConfig(\n            port=8080,\n            endpoint=\"/my-agent\",\n        )\n\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    host: str = \"localhost\"\n    \"\"\"Will be passed as argument to `uvicorn.run`.\"\"\"\n\n    port: int = 5000\n    \"\"\"Will be passed as argument to `uvicorn.run`.\"\"\"\n\n    endpoint: str = \"/\"\n    \"\"\"Will be pass as argument to `Starlette().add_route`\"\"\"\n\n    log_level: str = \"warning\"\n    \"\"\"Will be passed as argument to the `uvicorn` server.\"\"\"\n\n    version: str = \"0.1.0\"\n</code></pre>"},{"location":"api/config/#any_agent.serving.MCPServingConfig.endpoint","title":"<code>endpoint = '/'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Will be pass as argument to <code>Starlette().add_route</code></p>"},{"location":"api/config/#any_agent.serving.MCPServingConfig.host","title":"<code>host = 'localhost'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Will be passed as argument to <code>uvicorn.run</code>.</p>"},{"location":"api/config/#any_agent.serving.MCPServingConfig.log_level","title":"<code>log_level = 'warning'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Will be passed as argument to the <code>uvicorn</code> server.</p>"},{"location":"api/config/#any_agent.serving.MCPServingConfig.port","title":"<code>port = 5000</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Will be passed as argument to <code>uvicorn.run</code>.</p>"},{"location":"api/config/#any_agent.config.AgentFramework","title":"<code>any_agent.config.AgentFramework</code>","text":"<p>               Bases: <code>StrEnum</code></p> Source code in <code>src/any_agent/config.py</code> <pre><code>class AgentFramework(StrEnum):\n    GOOGLE = auto()\n    LANGCHAIN = auto()\n    LLAMA_INDEX = auto()\n    OPENAI = auto()\n    AGNO = auto()\n    SMOLAGENTS = auto()\n    TINYAGENT = auto()\n\n    @classmethod\n    def from_string(cls, value: str | Self) -&gt; Self:\n        if isinstance(value, cls):\n            return value\n\n        formatted_value = value.strip().upper()\n        if formatted_value not in cls.__members__:\n            error_message = (\n                f\"Unsupported agent framework: '{value}'. \"\n                f\"Valid frameworks are: {list(cls.__members__.keys())}\"\n            )\n            raise ValueError(error_message)\n\n        return cls[formatted_value]\n</code></pre>"},{"location":"api/evaluation/","title":"Evaluation","text":""},{"location":"api/evaluation/#any_agent.evaluation.LlmJudge","title":"<code>any_agent.evaluation.LlmJudge</code>","text":"Source code in <code>src/any_agent/evaluation/llm_judge.py</code> <pre><code>class LlmJudge:\n    def __init__(\n        self,\n        model_id: str,\n        framework: AgentFramework = AgentFramework.TINYAGENT,\n        output_type: type[BaseModel] = EvaluationOutput,\n        model_args: dict[str, Any] | None = None,\n        system_prompt: str = LLM_JUDGE_SYSTEM_PROMPT,\n    ):\n        if model_args is None:\n            model_args = {}\n        self.model_id = model_id\n        self.framework = framework\n        self.model_args = model_args\n        self.output_type = output_type\n        self.system_prompt = system_prompt.format(\n            response_schema=self.output_type.model_json_schema()\n        )\n        self.model_args[\"response_format\"] = self.output_type\n\n    def _create_prompt(self, context: str, question: str, prompt: str) -&gt; str:\n        if \"{context}\" not in prompt or \"{question}\" not in prompt:\n            msg = \"Prompt must contain the following placeholders: {context} and {question}\"\n            raise ValueError(msg)\n        return prompt.format(\n            context=context,\n            question=question,\n        )\n\n    def run(\n        self,\n        context: str,\n        question: str,\n        prompt_template: str = DEFAULT_PROMPT_TEMPLATE,\n    ) -&gt; BaseModel:\n        \"\"\"Run the judge synchronously.\n\n        Args:\n            context: Any relevant information that may be needed to answer the question\n            question: The question to ask the agent\n            prompt_template: The prompt to use for the LLM\n\n        Returns:\n            The evaluation result\n\n        \"\"\"\n        return run_async_in_sync(\n            self.run_async(context, question, prompt_template),\n            allow_running_loop=INSIDE_NOTEBOOK,\n        )\n\n    async def run_async(\n        self,\n        context: str,\n        question: str,\n        prompt_template: str = DEFAULT_PROMPT_TEMPLATE,\n    ) -&gt; BaseModel:\n        \"\"\"Run the LLM asynchronously.\n\n        Args:\n            context: Any relevant information that may be needed to answer the question\n            question: The question to ask the agent\n            prompt_template: The prompt to use for the LLM\n\n        Returns:\n            The evaluation result\n\n        \"\"\"\n        prompt = self._create_prompt(context, question, prompt_template)\n\n        # Make the LLM call\n        response = await acompletion(\n            model=self.model_id,\n            messages=[\n                {\"role\": \"system\", \"content\": self.system_prompt},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            **self.model_args,\n        )\n        if not isinstance(response, ChatCompletion):\n            error_message = f\"Expected ChatCompletion, got {type(response)}\"\n            raise ValueError(error_message)\n        if response.choices[0].message.content is None:\n            error_message = \"No content in the response\"\n            raise ValueError(error_message)\n        return self.output_type.model_validate_json(response.choices[0].message.content)\n</code></pre>"},{"location":"api/evaluation/#any_agent.evaluation.LlmJudge.run","title":"<code>run(context, question, prompt_template=DEFAULT_PROMPT_TEMPLATE)</code>","text":"<p>Run the judge synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str</code> <p>Any relevant information that may be needed to answer the question</p> required <code>question</code> <code>str</code> <p>The question to ask the agent</p> required <code>prompt_template</code> <code>str</code> <p>The prompt to use for the LLM</p> <code>DEFAULT_PROMPT_TEMPLATE</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>The evaluation result</p> Source code in <code>src/any_agent/evaluation/llm_judge.py</code> <pre><code>def run(\n    self,\n    context: str,\n    question: str,\n    prompt_template: str = DEFAULT_PROMPT_TEMPLATE,\n) -&gt; BaseModel:\n    \"\"\"Run the judge synchronously.\n\n    Args:\n        context: Any relevant information that may be needed to answer the question\n        question: The question to ask the agent\n        prompt_template: The prompt to use for the LLM\n\n    Returns:\n        The evaluation result\n\n    \"\"\"\n    return run_async_in_sync(\n        self.run_async(context, question, prompt_template),\n        allow_running_loop=INSIDE_NOTEBOOK,\n    )\n</code></pre>"},{"location":"api/evaluation/#any_agent.evaluation.LlmJudge.run_async","title":"<code>run_async(context, question, prompt_template=DEFAULT_PROMPT_TEMPLATE)</code>  <code>async</code>","text":"<p>Run the LLM asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str</code> <p>Any relevant information that may be needed to answer the question</p> required <code>question</code> <code>str</code> <p>The question to ask the agent</p> required <code>prompt_template</code> <code>str</code> <p>The prompt to use for the LLM</p> <code>DEFAULT_PROMPT_TEMPLATE</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>The evaluation result</p> Source code in <code>src/any_agent/evaluation/llm_judge.py</code> <pre><code>async def run_async(\n    self,\n    context: str,\n    question: str,\n    prompt_template: str = DEFAULT_PROMPT_TEMPLATE,\n) -&gt; BaseModel:\n    \"\"\"Run the LLM asynchronously.\n\n    Args:\n        context: Any relevant information that may be needed to answer the question\n        question: The question to ask the agent\n        prompt_template: The prompt to use for the LLM\n\n    Returns:\n        The evaluation result\n\n    \"\"\"\n    prompt = self._create_prompt(context, question, prompt_template)\n\n    # Make the LLM call\n    response = await acompletion(\n        model=self.model_id,\n        messages=[\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        **self.model_args,\n    )\n    if not isinstance(response, ChatCompletion):\n        error_message = f\"Expected ChatCompletion, got {type(response)}\"\n        raise ValueError(error_message)\n    if response.choices[0].message.content is None:\n        error_message = \"No content in the response\"\n        raise ValueError(error_message)\n    return self.output_type.model_validate_json(response.choices[0].message.content)\n</code></pre>"},{"location":"api/evaluation/#any_agent.evaluation.AgentJudge","title":"<code>any_agent.evaluation.AgentJudge</code>","text":"<p>An agent that evaluates the correctness of another agent's trace.</p> Source code in <code>src/any_agent/evaluation/agent_judge.py</code> <pre><code>class AgentJudge:\n    \"\"\"An agent that evaluates the correctness of another agent's trace.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        framework: AgentFramework = AgentFramework.TINYAGENT,\n        output_type: type[BaseModel] = EvaluationOutput,\n        model_args: dict[str, Any] | None = None,\n    ):\n        self.model_id = model_id\n        self.framework = framework\n        self.model_args = model_args\n        self.output_type = output_type\n\n    def run(\n        self,\n        trace: AgentTrace,\n        question: str,\n        additional_tools: list[Callable[[], Any]] | None = None,\n    ) -&gt; AgentTrace:\n        \"\"\"Run the agent judge.\n\n        Args:\n            trace: The agent trace to evaluate\n            question: The question to ask the agent\n            additional_tools: Additional tools to use for the agent\n\n        Returns:\n            The trace of the evaluation run.\n            You can access the evaluation result in the `final_output`\n            property.\n\n        \"\"\"\n        if additional_tools is None:\n            additional_tools = []\n        return run_async_in_sync(\n            self.run_async(trace, question, additional_tools),\n            allow_running_loop=INSIDE_NOTEBOOK,\n        )\n\n    async def run_async(\n        self,\n        trace: AgentTrace,\n        question: str,\n        additional_tools: list[Callable[[], Any]] | None = None,\n    ) -&gt; AgentTrace:\n        \"\"\"Run the agent judge asynchronously.\n\n        Args:\n            trace: The agent trace to evaluate\n            question: The question to ask the agent\n            additional_tools: Additional tools to use for the agent\n        Returns:\n            The trace of the evaluation run.\n            You can access the evaluation result in the `final_output`\n            property.\n\n        \"\"\"\n        if additional_tools is None:\n            additional_tools = []\n        tooling = TraceTools(trace)\n\n        agent_config = AgentConfig(\n            model_id=self.model_id,\n            instructions=AGENT_INSTRUCTIONS.format(\n                response_schema=self.output_type.model_json_schema()\n            ),\n            tools=tooling.get_all_tools() + additional_tools,\n            output_type=self.output_type,\n            model_args=self.model_args,\n        )\n\n        agent = await AnyAgent.create_async(\n            self.framework,\n            agent_config=agent_config,\n        )\n        agent_trace = await agent.run_async(question)\n        if not isinstance(agent_trace.final_output, self.output_type):\n            msg = f\"Agent output is not an {self.output_type} instance.\"\n            raise ValueError(msg)\n        return agent_trace\n</code></pre>"},{"location":"api/evaluation/#any_agent.evaluation.AgentJudge.run","title":"<code>run(trace, question, additional_tools=None)</code>","text":"<p>Run the agent judge.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>AgentTrace</code> <p>The agent trace to evaluate</p> required <code>question</code> <code>str</code> <p>The question to ask the agent</p> required <code>additional_tools</code> <code>list[Callable[[], Any]] | None</code> <p>Additional tools to use for the agent</p> <code>None</code> <p>Returns:</p> Type Description <code>AgentTrace</code> <p>The trace of the evaluation run.</p> <code>AgentTrace</code> <p>You can access the evaluation result in the <code>final_output</code></p> <code>AgentTrace</code> <p>property.</p> Source code in <code>src/any_agent/evaluation/agent_judge.py</code> <pre><code>def run(\n    self,\n    trace: AgentTrace,\n    question: str,\n    additional_tools: list[Callable[[], Any]] | None = None,\n) -&gt; AgentTrace:\n    \"\"\"Run the agent judge.\n\n    Args:\n        trace: The agent trace to evaluate\n        question: The question to ask the agent\n        additional_tools: Additional tools to use for the agent\n\n    Returns:\n        The trace of the evaluation run.\n        You can access the evaluation result in the `final_output`\n        property.\n\n    \"\"\"\n    if additional_tools is None:\n        additional_tools = []\n    return run_async_in_sync(\n        self.run_async(trace, question, additional_tools),\n        allow_running_loop=INSIDE_NOTEBOOK,\n    )\n</code></pre>"},{"location":"api/evaluation/#any_agent.evaluation.AgentJudge.run_async","title":"<code>run_async(trace, question, additional_tools=None)</code>  <code>async</code>","text":"<p>Run the agent judge asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>AgentTrace</code> <p>The agent trace to evaluate</p> required <code>question</code> <code>str</code> <p>The question to ask the agent</p> required <code>additional_tools</code> <code>list[Callable[[], Any]] | None</code> <p>Additional tools to use for the agent</p> <code>None</code> <p>Returns:     The trace of the evaluation run.     You can access the evaluation result in the <code>final_output</code>     property.</p> Source code in <code>src/any_agent/evaluation/agent_judge.py</code> <pre><code>async def run_async(\n    self,\n    trace: AgentTrace,\n    question: str,\n    additional_tools: list[Callable[[], Any]] | None = None,\n) -&gt; AgentTrace:\n    \"\"\"Run the agent judge asynchronously.\n\n    Args:\n        trace: The agent trace to evaluate\n        question: The question to ask the agent\n        additional_tools: Additional tools to use for the agent\n    Returns:\n        The trace of the evaluation run.\n        You can access the evaluation result in the `final_output`\n        property.\n\n    \"\"\"\n    if additional_tools is None:\n        additional_tools = []\n    tooling = TraceTools(trace)\n\n    agent_config = AgentConfig(\n        model_id=self.model_id,\n        instructions=AGENT_INSTRUCTIONS.format(\n            response_schema=self.output_type.model_json_schema()\n        ),\n        tools=tooling.get_all_tools() + additional_tools,\n        output_type=self.output_type,\n        model_args=self.model_args,\n    )\n\n    agent = await AnyAgent.create_async(\n        self.framework,\n        agent_config=agent_config,\n    )\n    agent_trace = await agent.run_async(question)\n    if not isinstance(agent_trace.final_output, self.output_type):\n        msg = f\"Agent output is not an {self.output_type} instance.\"\n        raise ValueError(msg)\n    return agent_trace\n</code></pre>"},{"location":"api/logging/","title":"Logging with <code>any-agent</code>","text":"<p><code>any-agent</code> comes with a logger powered by Rich</p>"},{"location":"api/logging/#quick-start","title":"Quick Start","text":"<p>By default, logging is set up for you. But if you want to customize it, you can call:</p> <pre><code>from any_agent.logging import setup_logger\n\nsetup_logger()\n</code></pre>"},{"location":"api/logging/#customizing-the-logger","title":"Customizing the Logger","text":"<p>View the docstring in <code>setup_logger</code> for a description of the arguments available .</p>"},{"location":"api/logging/#example-set-log-level-to-debug","title":"Example: Set Log Level to DEBUG","text":"<pre><code>from any_agent.logging import setup_logger\nimport logging\n\nsetup_logger(level=logging.DEBUG)\n</code></pre>"},{"location":"api/logging/#example-custom-log-format","title":"Example: Custom Log Format","text":"<pre><code>setup_logger(log_format=\"%(asctime)s - %(levelname)s - %(message)s\")\n</code></pre>"},{"location":"api/logging/#example-propagate-logs","title":"Example: Propagate Logs","text":"<pre><code>setup_logger(propagate=True)\n</code></pre>"},{"location":"api/logging/#any_agent.logging.setup_logger","title":"<code>any_agent.logging.setup_logger(level=logging.ERROR, rich_tracebacks=True, log_format=None, propagate=False, **kwargs)</code>","text":"<p>Configure the any_agent logger with the specified settings.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>The logging level to use (default: logging.INFO)</p> <code>ERROR</code> <code>rich_tracebacks</code> <code>bool</code> <p>Whether to enable rich tracebacks (default: True)</p> <code>True</code> <code>log_format</code> <code>str | None</code> <p>Optional custom log format string</p> <code>None</code> <code>propagate</code> <code>bool</code> <p>Whether to propagate logs to parent loggers (default: False)</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to RichHandler</p> <code>{}</code> Source code in <code>src/any_agent/logging.py</code> <pre><code>def setup_logger(\n    level: int = logging.ERROR,\n    rich_tracebacks: bool = True,\n    log_format: str | None = None,\n    propagate: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Configure the any_agent logger with the specified settings.\n\n    Args:\n        level: The logging level to use (default: logging.INFO)\n        rich_tracebacks: Whether to enable rich tracebacks (default: True)\n        log_format: Optional custom log format string\n        propagate: Whether to propagate logs to parent loggers (default: False)\n        **kwargs: Additional keyword arguments to pass to RichHandler\n\n    \"\"\"\n    logger.setLevel(level)\n    logger.propagate = propagate\n\n    # Remove any existing handlers\n    for handler in logger.handlers[:]:\n        logger.removeHandler(handler)\n\n    handler = RichHandler(rich_tracebacks=rich_tracebacks, markup=True, **kwargs)\n\n    if log_format:\n        formatter = logging.Formatter(log_format)\n        handler.setFormatter(formatter)\n\n    logger.addHandler(handler)\n</code></pre>"},{"location":"api/serving/","title":"Serving","text":""},{"location":"api/serving/#any_agent.serving.ServerHandle","title":"<code>any_agent.serving.ServerHandle</code>  <code>dataclass</code>","text":"<p>A handle for managing an async server instance.</p> <p>This class provides a clean interface for managing the lifecycle of a server without requiring manual management of the underlying task and server objects.</p> Source code in <code>src/any_agent/serving/server_handle.py</code> <pre><code>@dataclass\nclass ServerHandle:\n    \"\"\"A handle for managing an async server instance.\n\n    This class provides a clean interface for managing the lifecycle of a server\n    without requiring manual management of the underlying task and server objects.\n    \"\"\"\n\n    task: asyncio.Task[Any]\n    server: UvicornServer\n\n    async def shutdown(self, timeout_seconds: float = 10.0) -&gt; None:\n        \"\"\"Gracefully shutdown the server with a timeout.\n\n        Args:\n            timeout_seconds: Maximum time to wait for graceful shutdown before forcing cancellation.\n\n        \"\"\"\n        if not self.is_running():\n            return  # Already shut down\n\n        self.server.should_exit = True\n        try:\n            await asyncio.wait_for(self.task, timeout=timeout_seconds)\n        except TimeoutError:\n            logger.warning(\n                \"Server shutdown timed out after %ss, forcing cancellation\",\n                timeout_seconds,\n            )\n            self.task.cancel()\n            try:\n                await self.task\n            except asyncio.CancelledError:\n                pass\n        except Exception as e:\n            logger.error(\"Error during server shutdown: %s\", e)\n            # Still try to cancel the task to clean up\n            if not self.task.done():\n                self.task.cancel()\n                try:\n                    await self.task\n                except asyncio.CancelledError:\n                    pass\n\n    def is_running(self) -&gt; bool:\n        \"\"\"Check if the server is still running.\n\n        Returns:\n            True if the server task is still running, False otherwise.\n\n        \"\"\"\n        return not self.task.done()\n\n    @property\n    def port(self) -&gt; int:\n        \"\"\"Get the port the server is running on.\n\n        If the server port was specified as 0, the port will be the one assigned by the OS.\n        This helper method is useful to get the actual port that the server is running on.\n\n        Returns:\n            The port number the server is running on.\n\n        \"\"\"\n        port = self.server.servers[0].sockets[0].getsockname()[1]\n        assert port is not None\n        assert isinstance(port, int)\n        return port\n</code></pre>"},{"location":"api/serving/#any_agent.serving.ServerHandle.port","title":"<code>port</code>  <code>property</code>","text":"<p>Get the port the server is running on.</p> <p>If the server port was specified as 0, the port will be the one assigned by the OS. This helper method is useful to get the actual port that the server is running on.</p> <p>Returns:</p> Type Description <code>int</code> <p>The port number the server is running on.</p>"},{"location":"api/serving/#any_agent.serving.ServerHandle.is_running","title":"<code>is_running()</code>","text":"<p>Check if the server is still running.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the server task is still running, False otherwise.</p> Source code in <code>src/any_agent/serving/server_handle.py</code> <pre><code>def is_running(self) -&gt; bool:\n    \"\"\"Check if the server is still running.\n\n    Returns:\n        True if the server task is still running, False otherwise.\n\n    \"\"\"\n    return not self.task.done()\n</code></pre>"},{"location":"api/serving/#any_agent.serving.ServerHandle.shutdown","title":"<code>shutdown(timeout_seconds=10.0)</code>  <code>async</code>","text":"<p>Gracefully shutdown the server with a timeout.</p> <p>Parameters:</p> Name Type Description Default <code>timeout_seconds</code> <code>float</code> <p>Maximum time to wait for graceful shutdown before forcing cancellation.</p> <code>10.0</code> Source code in <code>src/any_agent/serving/server_handle.py</code> <pre><code>async def shutdown(self, timeout_seconds: float = 10.0) -&gt; None:\n    \"\"\"Gracefully shutdown the server with a timeout.\n\n    Args:\n        timeout_seconds: Maximum time to wait for graceful shutdown before forcing cancellation.\n\n    \"\"\"\n    if not self.is_running():\n        return  # Already shut down\n\n    self.server.should_exit = True\n    try:\n        await asyncio.wait_for(self.task, timeout=timeout_seconds)\n    except TimeoutError:\n        logger.warning(\n            \"Server shutdown timed out after %ss, forcing cancellation\",\n            timeout_seconds,\n        )\n        self.task.cancel()\n        try:\n            await self.task\n        except asyncio.CancelledError:\n            pass\n    except Exception as e:\n        logger.error(\"Error during server shutdown: %s\", e)\n        # Still try to cancel the task to clean up\n        if not self.task.done():\n            self.task.cancel()\n            try:\n                await self.task\n            except asyncio.CancelledError:\n                pass\n</code></pre>"},{"location":"api/tools/","title":"Tools","text":""},{"location":"api/tools/#any_agent.tools","title":"<code>any_agent.tools</code>","text":""},{"location":"api/tools/#any_agent.tools.MCPClient","title":"<code>MCPClient</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Unified MCP client that handles all transport types and frameworks.</p> Source code in <code>src/any_agent/tools/mcp/mcp_client.py</code> <pre><code>class MCPClient(BaseModel):\n    \"\"\"Unified MCP client that handles all transport types and frameworks.\"\"\"\n\n    config: MCPParams\n    framework: AgentFramework\n\n    _session: ClientSession | None = PrivateAttr(default=None)\n    _exit_stack: AsyncExitStack = PrivateAttr(default_factory=AsyncExitStack)\n    _client: Any | None = PrivateAttr(default=None)\n    _get_session_id_callback: Callable[[], str | None] | None = PrivateAttr(\n        default=None\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def model_post_init(self, __context: Any, /) -&gt; None:\n        \"\"\"Initialize the MCP client and check dependencies.\"\"\"\n        if missing_mcp_error:\n            msg = \"You need to `pip install 'any-agent[mcp]'` to use MCP.\"\n            raise ImportError(msg) from missing_mcp_error\n\n    async def connect(self) -&gt; None:\n        \"\"\"Connect using the appropriate transport type.\"\"\"\n        if isinstance(self.config, MCPStdio):\n            server_params = StdioServerParameters(\n                command=self.config.command,\n                args=list(self.config.args),\n                env={**os.environ},\n            )\n            self._client = stdio_client(server_params)\n            read, write = await self._exit_stack.enter_async_context(self._client)\n        elif isinstance(self.config, MCPSse):\n            self._client = sse_client(\n                url=self.config.url,\n                headers=dict(self.config.headers or {}),\n            )\n            read, write = await self._exit_stack.enter_async_context(self._client)\n        elif isinstance(self.config, MCPStreamableHttp):\n            self._client = streamablehttp_client(\n                url=self.config.url,\n                headers=dict(self.config.headers or {}),\n            )\n            transport = await self._exit_stack.enter_async_context(self._client)\n            read, write, self._get_session_id_callback = transport\n        else:\n            msg = f\"Unsupported MCP config type: {type(self.config)}\"\n            raise ValueError(msg)\n\n        # Create and initialize session (common for all transports)\n        timeout = (\n            timedelta(seconds=self.config.client_session_timeout_seconds)\n            if self.config.client_session_timeout_seconds\n            else None\n        )\n        client_session = ClientSession(read, write, timeout)\n        self._session = await self._exit_stack.enter_async_context(client_session)\n        await self._session.initialize()\n\n    async def list_raw_tools(self) -&gt; list[MCPTool]:\n        \"\"\"Get raw MCP tools from the server.\"\"\"\n        if not self._session:\n            msg = \"Not connected to MCP server. Call connect() first.\"\n            raise ValueError(msg)\n\n        available_tools = await self._session.list_tools()\n        return self._filter_tools(available_tools.tools)\n\n    async def list_tools(self) -&gt; list[Callable[..., Any]]:\n        \"\"\"Get tools converted to callable functions that work with any framework.\"\"\"\n        raw_tools = await self.list_raw_tools()\n        return self._convert_tools_to_callables(raw_tools)\n\n    def _filter_tools(self, tools: Sequence[MCPTool]) -&gt; list[MCPTool]:\n        \"\"\"Filter tools based on config.\"\"\"\n        requested_tools = list(self.config.tools or [])\n        if not requested_tools:\n            return list(tools)\n\n        name_to_tool = {tool.name: tool for tool in tools}\n        missing_tools = [name for name in requested_tools if name not in name_to_tool]\n        if missing_tools:\n            error_message = dedent(\n                f\"\"\"Could not find all requested tools in the MCP server:\n                Requested ({len(requested_tools)}): {requested_tools}\n                Available ({len(name_to_tool)}): {list(name_to_tool.keys())}\n                Missing: {missing_tools}\n                \"\"\"\n            )\n            raise ValueError(error_message)\n\n        return [name_to_tool[name] for name in requested_tools]\n\n    def _convert_tools_to_callables(\n        self, tools: list[MCPTool]\n    ) -&gt; list[Callable[..., Any]]:\n        \"\"\"Convert MCP tools to callable functions that work with any framework.\"\"\"\n        if not self._session:\n            msg = \"Session not available for tool conversion\"\n            raise ValueError(msg)\n        tool_functions = []\n        for tool in tools:\n            tool_func = self._create_tool_function(tool)\n            tool_functions.append(tool_func)\n        return tool_functions\n\n    def _create_tool_function(self, tool: MCPTool) -&gt; Callable[..., Any]:\n        \"\"\"Create a properly typed function for an MCP tool.\"\"\"\n        name = tool.name\n        description = tool.description or f\"MCP tool: {name}\"\n        input_schema = tool.inputSchema\n\n        # Extract parameters from schema\n        parameters = []\n        annotations = {}\n        if input_schema and isinstance(input_schema, dict):\n            properties = input_schema.get(\"properties\", {})\n            required = input_schema.get(\"required\", [])\n\n            for param_name, param_info in properties.items():\n                # Use the improved schema conversion\n                base_param_type = self._json_schema_to_python_type(param_info)\n\n                if param_name not in required:\n                    # For optional parameters, use Optional[T] for better framework compatibility\n                    # Note: We use Optional instead of X | Y syntax because some frameworks\n                    # (like Google) don't handle the union syntax properly\n                    optional_param_type: Any = Optional[base_param_type]  # noqa: UP045\n                    annotations[param_name] = optional_param_type\n                    param = inspect.Parameter(\n                        param_name,\n                        inspect.Parameter.KEYWORD_ONLY,\n                        default=None,\n                        annotation=optional_param_type,\n                    )\n                else:\n                    required_param_type: Any = base_param_type\n                    annotations[param_name] = required_param_type\n                    param = inspect.Parameter(\n                        param_name,\n                        inspect.Parameter.KEYWORD_ONLY,\n                        annotation=required_param_type,\n                    )\n                parameters.append(param)\n\n        # Create signature and enhanced docstring\n        signature = inspect.Signature(parameters, return_annotation=str)\n        enhanced_description = self._create_enhanced_description(\n            description, input_schema\n        )\n\n        # Create the actual function\n        async def mcp_tool_function(**kwargs: Any) -&gt; str:\n            \"\"\"Dynamically created MCP tool function.\"\"\"\n            try:\n                if not self._session:\n                    return f\"Error: MCP session not available for tool {name}\"\n                result = await self._session.call_tool(name, kwargs)\n                if hasattr(result, \"content\") and result.content:\n                    if hasattr(result.content[0], \"text\"):\n                        return result.content[0].text\n                    return str(result.content[0])\n                return str(result)\n            except Exception as e:\n                return f\"Error calling MCP tool {name}: {e!s}\"\n\n        # Set function metadata\n        mcp_tool_function.__name__ = name\n        mcp_tool_function.__doc__ = enhanced_description\n        mcp_tool_function.__signature__ = signature  # type: ignore[attr-defined]\n        mcp_tool_function.__annotations__ = {**annotations, \"return\": str}\n\n        return mcp_tool_function\n\n    TYPE_MAPPING: ClassVar[dict[str, type]] = {\n        \"string\": str,\n        \"integer\": int,\n        \"number\": float,\n        \"boolean\": bool,\n        \"array\": list,\n        \"object\": dict,\n    }\n\n    def _json_schema_to_python_type(self, schema: dict[str, Any]) -&gt; type:\n        \"\"\"Convert JSON schema to Python type using robust conversion.\"\"\"\n        schema_type = schema.get(\"type\", \"string\")\n        return self.TYPE_MAPPING.get(schema_type, str)\n\n    def _create_enhanced_description(self, description: str, input_schema: Any) -&gt; str:\n        \"\"\"Create enhanced docstring with parameter descriptions.\"\"\"\n        enhanced_description = description\n        if input_schema and isinstance(input_schema, dict):\n            properties = input_schema.get(\"properties\", {})\n            if properties:\n                param_descriptions = []\n                for param_name, param_info in properties.items():\n                    param_desc = param_info.get(\n                        \"description\", f\"Parameter {param_name}\"\n                    )\n                    param_descriptions.append(f\"    {param_name}: {param_desc}\")\n\n                if param_descriptions:\n                    enhanced_description += \"\\n\\nArgs:\\n\" + \"\\n\".join(\n                        param_descriptions\n                    )\n        return enhanced_description\n\n    async def disconnect(self) -&gt; None:\n        \"\"\"Clean up resources.\"\"\"\n        await self._exit_stack.aclose()\n        self._session = None\n        self._client = None\n</code></pre>"},{"location":"api/tools/#any_agent.tools.MCPClient.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Connect using the appropriate transport type.</p> Source code in <code>src/any_agent/tools/mcp/mcp_client.py</code> <pre><code>async def connect(self) -&gt; None:\n    \"\"\"Connect using the appropriate transport type.\"\"\"\n    if isinstance(self.config, MCPStdio):\n        server_params = StdioServerParameters(\n            command=self.config.command,\n            args=list(self.config.args),\n            env={**os.environ},\n        )\n        self._client = stdio_client(server_params)\n        read, write = await self._exit_stack.enter_async_context(self._client)\n    elif isinstance(self.config, MCPSse):\n        self._client = sse_client(\n            url=self.config.url,\n            headers=dict(self.config.headers or {}),\n        )\n        read, write = await self._exit_stack.enter_async_context(self._client)\n    elif isinstance(self.config, MCPStreamableHttp):\n        self._client = streamablehttp_client(\n            url=self.config.url,\n            headers=dict(self.config.headers or {}),\n        )\n        transport = await self._exit_stack.enter_async_context(self._client)\n        read, write, self._get_session_id_callback = transport\n    else:\n        msg = f\"Unsupported MCP config type: {type(self.config)}\"\n        raise ValueError(msg)\n\n    # Create and initialize session (common for all transports)\n    timeout = (\n        timedelta(seconds=self.config.client_session_timeout_seconds)\n        if self.config.client_session_timeout_seconds\n        else None\n    )\n    client_session = ClientSession(read, write, timeout)\n    self._session = await self._exit_stack.enter_async_context(client_session)\n    await self._session.initialize()\n</code></pre>"},{"location":"api/tools/#any_agent.tools.MCPClient.disconnect","title":"<code>disconnect()</code>  <code>async</code>","text":"<p>Clean up resources.</p> Source code in <code>src/any_agent/tools/mcp/mcp_client.py</code> <pre><code>async def disconnect(self) -&gt; None:\n    \"\"\"Clean up resources.\"\"\"\n    await self._exit_stack.aclose()\n    self._session = None\n    self._client = None\n</code></pre>"},{"location":"api/tools/#any_agent.tools.MCPClient.list_raw_tools","title":"<code>list_raw_tools()</code>  <code>async</code>","text":"<p>Get raw MCP tools from the server.</p> Source code in <code>src/any_agent/tools/mcp/mcp_client.py</code> <pre><code>async def list_raw_tools(self) -&gt; list[MCPTool]:\n    \"\"\"Get raw MCP tools from the server.\"\"\"\n    if not self._session:\n        msg = \"Not connected to MCP server. Call connect() first.\"\n        raise ValueError(msg)\n\n    available_tools = await self._session.list_tools()\n    return self._filter_tools(available_tools.tools)\n</code></pre>"},{"location":"api/tools/#any_agent.tools.MCPClient.list_tools","title":"<code>list_tools()</code>  <code>async</code>","text":"<p>Get tools converted to callable functions that work with any framework.</p> Source code in <code>src/any_agent/tools/mcp/mcp_client.py</code> <pre><code>async def list_tools(self) -&gt; list[Callable[..., Any]]:\n    \"\"\"Get tools converted to callable functions that work with any framework.\"\"\"\n    raw_tools = await self.list_raw_tools()\n    return self._convert_tools_to_callables(raw_tools)\n</code></pre>"},{"location":"api/tools/#any_agent.tools.MCPClient.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Initialize the MCP client and check dependencies.</p> Source code in <code>src/any_agent/tools/mcp/mcp_client.py</code> <pre><code>def model_post_init(self, __context: Any, /) -&gt; None:\n    \"\"\"Initialize the MCP client and check dependencies.\"\"\"\n    if missing_mcp_error:\n        msg = \"You need to `pip install 'any-agent[mcp]'` to use MCP.\"\n        raise ImportError(msg) from missing_mcp_error\n</code></pre>"},{"location":"api/tools/#any_agent.tools.a2a_tool","title":"<code>a2a_tool(url, toolname=None, http_kwargs=None)</code>","text":"<p>Perform a query using A2A to another agent (synchronous version).</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url in which the A2A agent is located.</p> required <code>toolname</code> <code>str</code> <p>The name for the created tool. Defaults to <code>call_{agent name in card}</code>. Leading and trailing whitespace are removed. Whitespace in the middle is replaced by <code>_</code>.</p> <code>None</code> <code>http_kwargs</code> <code>dict</code> <p>Additional kwargs to pass to the httpx client.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[str, Optional[str], Optional[str]], str]</code> <p>A sync <code>Callable</code> that takes a query and returns the agent response.</p> Source code in <code>src/any_agent/tools/a2a.py</code> <pre><code>def a2a_tool(\n    url: str, toolname: Optional[str] = None, http_kwargs: dict[str, Any] | None = None\n) -&gt; Callable[[str, Optional[str], Optional[str]], str]:\n    \"\"\"Perform a query using A2A to another agent (synchronous version).\n\n    Args:\n        url (str): The url in which the A2A agent is located.\n        toolname (str): The name for the created tool. Defaults to `call_{agent name in card}`.\n            Leading and trailing whitespace are removed. Whitespace in the middle is replaced by `_`.\n        http_kwargs (dict): Additional kwargs to pass to the httpx client.\n\n    Returns:\n        A sync `Callable` that takes a query and returns the agent response.\n\n    \"\"\"\n    if not a2a_tool_available:\n        msg = \"You need to `pip install 'any-agent[a2a]'` to use this tool\"\n        raise ImportError(msg)\n\n    # Fetch the async tool upfront to get proper name and documentation (otherwise the tool doesn't have the right name and documentation)\n    async_tool = run_async_in_sync(\n        a2a_tool_async(url, toolname, http_kwargs), allow_running_loop=INSIDE_NOTEBOOK\n    )\n\n    def sync_wrapper(\n        query: str, task_id: Optional[str] = None, context_id: Optional[str] = None\n    ) -&gt; Any:\n        \"\"\"Execute the A2A tool query synchronously.\"\"\"\n        return run_async_in_sync(\n            async_tool(query, task_id, context_id), allow_running_loop=INSIDE_NOTEBOOK\n        )\n\n    # Copy essential metadata from the async tool\n    sync_wrapper.__name__ = async_tool.__name__\n    sync_wrapper.__doc__ = async_tool.__doc__\n\n    return sync_wrapper\n</code></pre>"},{"location":"api/tools/#any_agent.tools.a2a_tool_async","title":"<code>a2a_tool_async(url, toolname=None, http_kwargs=None)</code>  <code>async</code>","text":"<p>Perform a query using A2A to another agent.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url in which the A2A agent is located.</p> required <code>toolname</code> <code>str</code> <p>The name for the created tool. Defaults to <code>call_{agent name in card}</code>. Leading and trailing whitespace are removed. Whitespace in the middle is replaced by <code>_</code>.</p> <code>None</code> <code>http_kwargs</code> <code>dict</code> <p>Additional kwargs to pass to the httpx client.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[str, Optional[str], Optional[str]], Coroutine[Any, Any, dict[str, Any]]]</code> <p>An async <code>Callable</code> that takes a query and returns the agent response.</p> Source code in <code>src/any_agent/tools/a2a.py</code> <pre><code>async def a2a_tool_async(\n    url: str, toolname: Optional[str] = None, http_kwargs: dict[str, Any] | None = None\n) -&gt; Callable[[str, Optional[str], Optional[str]], Coroutine[Any, Any, dict[str, Any]]]:\n    \"\"\"Perform a query using A2A to another agent.\n\n    Args:\n        url (str): The url in which the A2A agent is located.\n        toolname (str): The name for the created tool. Defaults to `call_{agent name in card}`.\n            Leading and trailing whitespace are removed. Whitespace in the middle is replaced by `_`.\n        http_kwargs (dict): Additional kwargs to pass to the httpx client.\n\n    Returns:\n        An async `Callable` that takes a query and returns the agent response.\n\n    \"\"\"\n    if not a2a_tool_available:\n        msg = \"You need to `pip install 'any-agent[a2a]'` to use this tool\"\n        raise ImportError(msg)\n\n    if http_kwargs is None:\n        http_kwargs = {}\n\n    # Default timeout in httpx is 5 seconds. For an agent response, the default should be more lenient.\n    if \"timeout\" not in http_kwargs:\n        http_kwargs[\"timeout\"] = 30.0\n\n    async with httpx.AsyncClient(\n        follow_redirects=True, **http_kwargs\n    ) as resolver_client:\n        a2a_agent_card: AgentCard = await (\n            A2ACardResolver(httpx_client=resolver_client, base_url=url)\n        ).get_agent_card()\n\n    # NOTE: Use Optional[T] instead of T | None syntax throughout this module.\n    # Google ADK's _parse_schema_from_parameter function has compatibility\n    # with the traditional Optional[T] syntax for automatic function calling.\n    # Using T | None syntax causes\"Failed to parse the parameter ... for automatic function calling\"\n    async def _send_query(\n        query: str, task_id: Optional[str] = None, context_id: Optional[str] = None\n    ) -&gt; dict[str, Any]:\n        async with httpx.AsyncClient(follow_redirects=True) as query_client:\n            client = A2AClient(httpx_client=query_client, agent_card=a2a_agent_card)\n            send_message_payload = SendMessageRequest(\n                id=str(uuid4()),\n                params=MessageSendParams(\n                    message=Message(\n                        role=Role.user,\n                        parts=[Part(root=TextPart(text=query))],\n                        # the id is not currently tracked\n                        message_id=str(uuid4().hex),\n                        task_id=task_id,\n                        context_id=context_id,\n                    )\n                ),\n            )\n            # TODO check how to capture exceptions and pass them on to the enclosing framework\n            response = await client.send_message(\n                send_message_payload, http_kwargs=http_kwargs\n            )\n\n            if not response.root:\n                msg = (\n                    \"The A2A agent did not return a root. Are you using an A2A agent not managed by any-agent? \"\n                    \"Please file an issue at https://github.com/mozilla-ai/any-agent/issues so we can help.\"\n                )\n                raise ValueError(msg)\n\n            if isinstance(response.root, JSONRPCErrorResponse):\n                response_dict = {\n                    \"error\": response.root.error.message,\n                    \"code\": response.root.error.code,\n                    \"data\": response.root.error.data,\n                }\n            elif isinstance(response.root, SendMessageSuccessResponse):\n                # Task\n                if isinstance(response.root.result, Task):\n                    task = response.root.result\n                    response_dict = {\n                        \"timestamp\": task.status.timestamp,\n                        \"status\": task.status.state,\n                    }\n                    if task.status.message:\n                        response_dict[\"task_id\"] = task.status.message.task_id\n                        response_dict[\"context_id\"] = task.status.message.context_id\n                        response_dict[\"message\"] = {\n                            \" \".join(\n                                [\n                                    part.root.text\n                                    for part in task.status.message.parts\n                                    if isinstance(part.root, TextPart)\n                                ]\n                            )\n                        }\n                # Message\n                else:\n                    response_dict = {\n                        \"message\": {\n                            \" \".join(\n                                [\n                                    part.root.text\n                                    for part in response.root.result.parts\n                                    if isinstance(part.root, TextPart)\n                                ]\n                            )\n                        },\n                        \"task_id\": response.root.result.task_id,\n                    }\n            else:\n                msg = (\n                    \"The A2A agent did not return a error or a result. Are you using an A2A agent not managed by any-agent? \"\n                    \"Please file an issue at https://github.com/mozilla-ai/any-agent/issues so we can help.\"\n                )\n                raise ValueError(msg)\n\n            return response_dict\n\n    new_name = toolname or a2a_agent_card.name\n    new_name = re.sub(r\"\\s+\", \"_\", new_name.strip())\n    _send_query.__name__ = f\"call_{new_name}\"\n    _send_query.__doc__ = f\"\"\"{a2a_agent_card.description}\n        Send a query to the A2A hosted agent named {a2a_agent_card.name}.\n\n        Agent description: {a2a_agent_card.description}\n\n        Args:\n            query (str): The query to send to the agent.\n            task_id (str, optional): Task ID for continuing an incomplete task.\n                If you want to start a new task, you should not provide a task_id (pass `task_id=None`).\n                If you want to resume a task, use the same task_id from a previous response with TaskState.input_required.\n            context_id (str, optional): Context ID for conversation continuity.\n                If you want to start a new conversation, you should not provide a context_id (pass `context_id=None`).\n\n        Returns:\n            dict: Response from the A2A agent containing:\n                - For successful responses: task_id, context_id, timestamp, status, and message\n                - For errors: error message, code, and data\n\n        Note:\n            If TaskState is terminal (completed/failed), do not reuse the same task_id.\n    \"\"\"\n    return _send_query\n</code></pre>"},{"location":"api/tools/#any_agent.tools.ask_user_verification","title":"<code>ask_user_verification(query)</code>","text":"<p>Asks user to verify the given <code>query</code>.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The question that requires verification.</p> required Source code in <code>src/any_agent/tools/user_interaction.py</code> <pre><code>def ask_user_verification(query: str) -&gt; str:\n    \"\"\"Asks user to verify the given `query`.\n\n    Args:\n        query: The question that requires verification.\n\n    \"\"\"\n    return input(f\"{query} =&gt; Type your answer here:\")\n</code></pre>"},{"location":"api/tools/#any_agent.tools.prepare_final_output","title":"<code>prepare_final_output(output_type, instructions=None)</code>","text":"<p>Prepare instructions and tools for structured output, returning the function directly.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>type[BaseModel]</code> <p>The Pydantic model type for structured output</p> required <code>instructions</code> <code>str | None</code> <p>Original instructions to modify</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, Callable[[str], dict[str, str | bool | dict[str, Any] | list[Any]]]]</code> <p>Tuple of (modified_instructions, final_output_function)</p> Source code in <code>src/any_agent/tools/final_output.py</code> <pre><code>def prepare_final_output(\n    output_type: type[BaseModel], instructions: str | None = None\n) -&gt; tuple[str, Callable[[str], dict[str, str | bool | dict[str, Any] | list[Any]]]]:\n    \"\"\"Prepare instructions and tools for structured output, returning the function directly.\n\n    Args:\n        output_type: The Pydantic model type for structured output\n        instructions: Original instructions to modify\n\n    Returns:\n        Tuple of (modified_instructions, final_output_function)\n\n    \"\"\"\n    tool_name = \"final_output\"\n    modified_instructions = instructions or \"\"\n    modified_instructions += (\n        f\"You must call the {tool_name} tool when finished.\"\n        f\"The 'answer' argument passed to the {tool_name} tool must be a JSON string that matches the following schema:\\n\"\n        f\"{output_type.model_json_schema()}\"\n    )\n\n    def final_output_tool(\n        answer: str,\n    ) -&gt; dict[str, str | bool | dict[str, Any] | list[Any]]:\n        # First check if it's valid JSON\n        try:\n            parsed_answer = json.loads(answer)\n        except json.JSONDecodeError as json_err:\n            return {\n                \"success\": False,\n                \"result\": f\"Invalid JSON format: {json_err}. Please fix the 'answer' parameter so that it is a valid JSON string and call this tool again.\",\n            }\n        # Then validate against the Pydantic model\n        try:\n            output_type.model_validate_json(answer)\n        except ValidationError as e:\n            return {\n                \"success\": False,\n                \"result\": f\"Please fix this validation error: {e}. The format must conform to {output_type.model_json_schema()}\",\n            }\n        else:\n            return {\"success\": True, \"result\": parsed_answer}\n\n    # Set the function name and docstring\n    final_output_tool.__name__ = tool_name\n    final_output_tool.__doc__ = f\"\"\"This tool is used to validate the final output. It must be called when the final answer is ready in order to ensure that the output is valid.\n\n    Args:\n        answer: The final output that can be loaded as a Pydantic model. This must be a JSON compatible string that matches the following schema:\n            {output_type.model_json_schema()}\n\n    Returns:\n        A dictionary with the following keys:\n            - success: True if the output is valid, False otherwise.\n            - result: The final output if success is True, otherwise an error message.\n\n    \"\"\"\n\n    return modified_instructions, final_output_tool\n</code></pre>"},{"location":"api/tools/#any_agent.tools.search_tavily","title":"<code>search_tavily(query, include_images=False)</code>","text":"<p>Perform a Tavily web search based on your query and return the top search results.</p> <p>See https://blog.tavily.com/getting-started-with-the-tavily-search-api for more information.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query to perform.</p> required <code>include_images</code> <code>bool</code> <p>Whether to include images in the results.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The top search results as a formatted string.</p> Source code in <code>src/any_agent/tools/web_browsing.py</code> <pre><code>def search_tavily(query: str, include_images: bool = False) -&gt; str:\n    \"\"\"Perform a Tavily web search based on your query and return the top search results.\n\n    See https://blog.tavily.com/getting-started-with-the-tavily-search-api for more information.\n\n    Args:\n        query (str): The search query to perform.\n        include_images (bool): Whether to include images in the results.\n\n    Returns:\n        The top search results as a formatted string.\n\n    \"\"\"\n    try:\n        from tavily.tavily import TavilyClient\n    except ImportError as e:\n        msg = \"You need to `pip install 'tavily-python'` to use this tool\"\n        raise ImportError(msg) from e\n\n    api_key = os.getenv(\"TAVILY_API_KEY\")\n    if not api_key:\n        return \"TAVILY_API_KEY environment variable not set.\"\n    try:\n        client = TavilyClient(api_key)\n        response = client.search(query, include_images=include_images)\n        results = response.get(\"results\", [])\n        output = []\n        for result in results:\n            output.append(\n                f\"[{result.get('title', 'No Title')}]({result.get('url', '#')})\\n{result.get('content', '')}\"\n            )\n        if include_images and \"images\" in response:\n            output.append(\"\\nImages:\")\n            for image in response[\"images\"]:\n                output.append(image)\n        return \"\\n\\n\".join(output) if output else \"No results found.\"\n    except Exception as e:\n        return f\"Error performing Tavily search: {e!s}\"\n</code></pre>"},{"location":"api/tools/#any_agent.tools.search_web","title":"<code>search_web(query)</code>","text":"<p>Perform a duckduckgo web search based on your query (think a Google search) then returns the top search results.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query to perform.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The top search results.</p> Source code in <code>src/any_agent/tools/web_browsing.py</code> <pre><code>def search_web(query: str) -&gt; str:\n    \"\"\"Perform a duckduckgo web search based on your query (think a Google search) then returns the top search results.\n\n    Args:\n        query (str): The search query to perform.\n\n    Returns:\n        The top search results.\n\n    \"\"\"\n    try:\n        from duckduckgo_search import DDGS  # type: ignore[import-not-found]\n    except ImportError as e:\n        msg = \"You need to `pip install 'duckduckgo_search'` to use this tool\"\n        raise ImportError(msg) from e\n\n    ddgs = DDGS()\n    results = ddgs.text(query, max_results=10)\n    return \"\\n\".join(\n        f\"[{result['title']}]({result['href']})\\n{result['body']}\" for result in results\n    )\n</code></pre>"},{"location":"api/tools/#any_agent.tools.send_console_message","title":"<code>send_console_message(user, query)</code>","text":"<p>Send the specified user a message via console and returns their response.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The question to ask the user.</p> required <code>user</code> <code>str</code> <p>The user to ask the question to.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The user's response.</p> Source code in <code>src/any_agent/tools/user_interaction.py</code> <pre><code>def send_console_message(user: str, query: str) -&gt; str:\n    \"\"\"Send the specified user a message via console and returns their response.\n\n    Args:\n        query: The question to ask the user.\n        user: The user to ask the question to.\n\n    Returns:\n        str: The user's response.\n\n    \"\"\"\n    return Prompt.ask(f\"{query}\\n{user}\")\n</code></pre>"},{"location":"api/tools/#any_agent.tools.show_final_output","title":"<code>show_final_output(answer)</code>","text":"<p>Show the final answer to the user.</p> <p>Parameters:</p> Name Type Description Default <code>answer</code> <code>str</code> <p>The final answer.</p> required Source code in <code>src/any_agent/tools/user_interaction.py</code> <pre><code>def show_final_output(answer: str) -&gt; str:\n    \"\"\"Show the final answer to the user.\n\n    Args:\n        answer: The final answer.\n\n    \"\"\"\n    logger.info(f\"Final output: {answer}\")\n    return answer\n</code></pre>"},{"location":"api/tools/#any_agent.tools.show_plan","title":"<code>show_plan(plan)</code>","text":"<p>Show the current plan to the user.</p> <p>Parameters:</p> Name Type Description Default <code>plan</code> <code>str</code> <p>The current plan.</p> required Source code in <code>src/any_agent/tools/user_interaction.py</code> <pre><code>def show_plan(plan: str) -&gt; str:\n    \"\"\"Show the current plan to the user.\n\n    Args:\n        plan: The current plan.\n\n    \"\"\"\n    logger.info(f\"Current plan: {plan}\")\n    return plan\n</code></pre>"},{"location":"api/tools/#any_agent.tools.visit_webpage","title":"<code>visit_webpage(url, timeout=30, max_length=10000)</code>","text":"<p>Visits a webpage at the given url and reads its content as a markdown string. Use this to browse webpages.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url of the webpage to visit.</p> required <code>timeout</code> <code>int</code> <p>The timeout in seconds for the request.</p> <code>30</code> <code>max_length</code> <code>int</code> <p>The maximum number of characters of text that can be returned (default=10000).         If max_length==-1, text is not truncated and the full webpage is returned.</p> <code>10000</code> Source code in <code>src/any_agent/tools/web_browsing.py</code> <pre><code>def visit_webpage(url: str, timeout: int = 30, max_length: int = 10000) -&gt; str:\n    \"\"\"Visits a webpage at the given url and reads its content as a markdown string. Use this to browse webpages.\n\n    Args:\n        url: The url of the webpage to visit.\n        timeout: The timeout in seconds for the request.\n        max_length: The maximum number of characters of text that can be returned (default=10000).\n                    If max_length==-1, text is not truncated and the full webpage is returned.\n\n    \"\"\"\n    try:\n        from markdownify import markdownify  # type: ignore[import-not-found]\n    except ImportError as e:\n        msg = \"You need to `pip install 'markdownify'` to use this tool\"\n        raise ImportError(msg) from e\n\n    try:\n        response = requests.get(url, timeout=timeout)\n        response.raise_for_status()\n\n        markdown_content = markdownify(response.text).strip()\n\n        markdown_content = re.sub(r\"\\n{2,}\", \"\\n\", markdown_content)\n\n        if max_length == -1:\n            return str(markdown_content)\n        return _truncate_content(markdown_content, max_length)\n    except RequestException as e:\n        return f\"Error fetching the webpage: {e!s}\"\n    except Exception as e:\n        return f\"An unexpected error occurred: {e!s}\"\n</code></pre>"},{"location":"api/tracing/","title":"Tracing","text":""},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentTrace","title":"<code>any_agent.tracing.agent_trace.AgentTrace</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A trace that can be exported to JSON or printed to the console.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>class AgentTrace(BaseModel):\n    \"\"\"A trace that can be exported to JSON or printed to the console.\"\"\"\n\n    spans: list[AgentSpan] = Field(default_factory=list)\n    \"\"\"A list of [`AgentSpan`][any_agent.tracing.agent_trace.AgentSpan] that form the trace.\n    \"\"\"\n\n    final_output: str | dict[str, Any] | BaseModel | None = Field(default=None)\n    \"\"\"Contains the final output message returned by the agent.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def _invalidate_tokens_and_cost_cache(self) -&gt; None:\n        \"\"\"Clear the cached tokens_and_cost property if it exists.\"\"\"\n        if \"tokens\" in self.__dict__:\n            del self.tokens\n        if \"cost\" in self.__dict__:\n            del self.cost\n\n    def add_span(self, span: AgentSpan | Span) -&gt; None:\n        \"\"\"Add an AgentSpan to the trace and clear the tokens_and_cost cache if present.\"\"\"\n        if not isinstance(span, AgentSpan):\n            span = AgentSpan.from_otel(span)\n        self.spans.append(span)\n        self._invalidate_tokens_and_cost_cache()\n\n    def add_spans(self, spans: list[AgentSpan]) -&gt; None:\n        \"\"\"Add a list of AgentSpans to the trace and clear the tokens_and_cost cache if present.\"\"\"\n        self.spans.extend(spans)\n        self._invalidate_tokens_and_cost_cache()\n\n    def spans_to_messages(self) -&gt; list[AgentMessage]:\n        \"\"\"Convert spans to standard message format.\n\n        Returns:\n            List of message dicts with 'role' and 'content' keys.\n\n        \"\"\"\n        messages: list[AgentMessage] = []\n\n        # Process spans in chronological order (excluding the final invoke_agent span)\n        # Filter out any agent invocation spans\n        filtered_spans: list[AgentSpan] = []\n        for span in self.spans:\n            if not span.is_agent_invocation():\n                filtered_spans.append(span)\n\n        for span in filtered_spans:\n            if span.is_llm_call():\n                if input_messages := span.get_input_messages():\n                    for msg in input_messages:\n                        if not any(\n                            existing.role == msg.role\n                            and existing.content == msg.content\n                            for existing in messages\n                        ):\n                            messages.append(msg)\n\n                if output_content := span.get_output_content():\n                    # Avoid duplicate assistant messages\n                    if not (\n                        messages\n                        and messages[-1].role == \"assistant\"\n                        and messages[-1].content == output_content\n                    ):\n                        messages.append(\n                            AgentMessage(role=\"assistant\", content=output_content)\n                        )\n\n            elif span.is_tool_execution():\n                # For tool executions, include the result in the conversation\n                output_content = span.get_output_content()\n                if output_content:\n                    tool_name = span.attributes[GenAI.TOOL_NAME]\n                    tool_args = span.attributes.get(GenAI.TOOL_ARGS, \"{}\")\n                    messages.append(\n                        AgentMessage(\n                            role=\"assistant\",\n                            content=f\"[Tool {tool_name} executed: {output_content} with args: {tool_args}]\",\n                        )\n                    )\n\n        return messages\n\n    @property\n    def duration(self) -&gt; timedelta:\n        \"\"\"Duration of the parent `invoke_agent` span as a datetime.timedelta object.\n\n        The duration is computed from the span's start and end time (in nanoseconds).\n\n        Raises ValueError if:\n            - There are no spans.\n            - The invoke_agent span is not the last span.\n            - Any of the start/end times are missing.\n        \"\"\"\n        if not self.spans:\n            msg = \"No spans found in trace\"\n            raise ValueError(msg)\n        span = self.spans[-1]\n        if not span.is_agent_invocation():\n            msg = \"Last span is not `invoke_agent`\"\n            raise ValueError(msg)\n        if span.start_time is not None and span.end_time is not None:\n            duration_ns = span.end_time - span.start_time\n            return timedelta(seconds=duration_ns / 1_000_000_000)\n        msg = \"Start or end time is missing for the `invoke_agent` span\"\n        raise ValueError(msg)\n\n    @cached_property\n    def tokens(self) -&gt; TokenInfo:\n        \"\"\"The [`TokenInfo`][any_agent.tracing.agent_trace.TokenInfo] for this trace. Cached after first computation.\"\"\"\n        sum_input_tokens = 0\n        sum_output_tokens = 0\n        for span in self.spans:\n            if span.is_llm_call():\n                sum_input_tokens += span.attributes.get(GenAI.USAGE_INPUT_TOKENS, 0)\n                sum_output_tokens += span.attributes.get(GenAI.USAGE_OUTPUT_TOKENS, 0)\n        return TokenInfo(input_tokens=sum_input_tokens, output_tokens=sum_output_tokens)\n\n    @cached_property\n    def cost(self) -&gt; CostInfo:\n        \"\"\"The [`CostInfo`][any_agent.tracing.agent_trace.CostInfo] for this trace. Cached after first computation.\"\"\"\n        sum_input_cost = 0.0\n        sum_output_cost = 0.0\n        for span in self.spans:\n            if span.is_llm_call():\n                sum_input_cost += span.attributes.get(GenAI.USAGE_INPUT_COST, 0)\n                sum_output_cost += span.attributes.get(GenAI.USAGE_OUTPUT_COST, 0)\n        return CostInfo(input_cost=sum_input_cost, output_cost=sum_output_cost)\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentTrace.cost","title":"<code>cost</code>  <code>cached</code> <code>property</code>","text":"<p>The <code>CostInfo</code> for this trace. Cached after first computation.</p>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentTrace.duration","title":"<code>duration</code>  <code>property</code>","text":"<p>Duration of the parent <code>invoke_agent</code> span as a datetime.timedelta object.</p> <p>The duration is computed from the span's start and end time (in nanoseconds).</p> Raises ValueError if <ul> <li>There are no spans.</li> <li>The invoke_agent span is not the last span.</li> <li>Any of the start/end times are missing.</li> </ul>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentTrace.final_output","title":"<code>final_output = Field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Contains the final output message returned by the agent.</p>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentTrace.spans","title":"<code>spans = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of <code>AgentSpan</code> that form the trace.</p>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentTrace.tokens","title":"<code>tokens</code>  <code>cached</code> <code>property</code>","text":"<p>The <code>TokenInfo</code> for this trace. Cached after first computation.</p>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentTrace.add_span","title":"<code>add_span(span)</code>","text":"<p>Add an AgentSpan to the trace and clear the tokens_and_cost cache if present.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>def add_span(self, span: AgentSpan | Span) -&gt; None:\n    \"\"\"Add an AgentSpan to the trace and clear the tokens_and_cost cache if present.\"\"\"\n    if not isinstance(span, AgentSpan):\n        span = AgentSpan.from_otel(span)\n    self.spans.append(span)\n    self._invalidate_tokens_and_cost_cache()\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentTrace.add_spans","title":"<code>add_spans(spans)</code>","text":"<p>Add a list of AgentSpans to the trace and clear the tokens_and_cost cache if present.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>def add_spans(self, spans: list[AgentSpan]) -&gt; None:\n    \"\"\"Add a list of AgentSpans to the trace and clear the tokens_and_cost cache if present.\"\"\"\n    self.spans.extend(spans)\n    self._invalidate_tokens_and_cost_cache()\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentTrace.spans_to_messages","title":"<code>spans_to_messages()</code>","text":"<p>Convert spans to standard message format.</p> <p>Returns:</p> Type Description <code>list[AgentMessage]</code> <p>List of message dicts with 'role' and 'content' keys.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>def spans_to_messages(self) -&gt; list[AgentMessage]:\n    \"\"\"Convert spans to standard message format.\n\n    Returns:\n        List of message dicts with 'role' and 'content' keys.\n\n    \"\"\"\n    messages: list[AgentMessage] = []\n\n    # Process spans in chronological order (excluding the final invoke_agent span)\n    # Filter out any agent invocation spans\n    filtered_spans: list[AgentSpan] = []\n    for span in self.spans:\n        if not span.is_agent_invocation():\n            filtered_spans.append(span)\n\n    for span in filtered_spans:\n        if span.is_llm_call():\n            if input_messages := span.get_input_messages():\n                for msg in input_messages:\n                    if not any(\n                        existing.role == msg.role\n                        and existing.content == msg.content\n                        for existing in messages\n                    ):\n                        messages.append(msg)\n\n            if output_content := span.get_output_content():\n                # Avoid duplicate assistant messages\n                if not (\n                    messages\n                    and messages[-1].role == \"assistant\"\n                    and messages[-1].content == output_content\n                ):\n                    messages.append(\n                        AgentMessage(role=\"assistant\", content=output_content)\n                    )\n\n        elif span.is_tool_execution():\n            # For tool executions, include the result in the conversation\n            output_content = span.get_output_content()\n            if output_content:\n                tool_name = span.attributes[GenAI.TOOL_NAME]\n                tool_args = span.attributes.get(GenAI.TOOL_ARGS, \"{}\")\n                messages.append(\n                    AgentMessage(\n                        role=\"assistant\",\n                        content=f\"[Tool {tool_name} executed: {output_content} with args: {tool_args}]\",\n                    )\n                )\n\n    return messages\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentSpan","title":"<code>any_agent.tracing.agent_trace.AgentSpan</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A span that can be exported to JSON or printed to the console.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>class AgentSpan(BaseModel):\n    \"\"\"A span that can be exported to JSON or printed to the console.\"\"\"\n\n    name: str\n    kind: SpanKind\n    parent: SpanContext | None = None\n    start_time: int | None = None\n    end_time: int | None = None\n    status: Status\n    context: SpanContext\n    attributes: dict[str, Any]\n    links: list[Link]\n    events: list[Event]\n    resource: Resource\n\n    model_config = ConfigDict(arbitrary_types_allowed=False)\n\n    @classmethod\n    def from_otel(cls, otel_span: Span) -&gt; AgentSpan:\n        \"\"\"Create an AgentSpan from an OTEL Span.\"\"\"\n        return cls(\n            name=otel_span.name,\n            kind=SpanKind.from_otel(otel_span.kind),\n            parent=SpanContext.from_otel(otel_span.parent),\n            start_time=otel_span.start_time,\n            end_time=otel_span.end_time,\n            status=Status.from_otel(otel_span.status),\n            context=SpanContext.from_otel(otel_span.context),\n            attributes=dict(otel_span.attributes) if otel_span.attributes else {},\n            links=[Link.from_otel(link) for link in otel_span.links],\n            events=[Event.from_otel(event) for event in otel_span.events],\n            resource=Resource.from_otel(otel_span.resource),\n        )\n\n    def to_readable_span(self) -&gt; ReadableSpan:\n        \"\"\"Create an ReadableSpan from the AgentSpan.\"\"\"\n        return ReadableSpan(\n            name=self.name,\n            kind=self.kind,\n            parent=self.parent,\n            start_time=self.start_time,\n            end_time=self.end_time,\n            status=self.status,\n            context=self.context,\n            attributes=self.attributes,\n            links=self.links,\n            events=self.events,\n            resource=self.resource,\n        )\n\n    def set_attributes(self, attributes: Mapping[str, AttributeValue]) -&gt; None:\n        \"\"\"Set attributes for the span.\"\"\"\n        for key, value in attributes.items():\n            if key in self.attributes:\n                logger.warning(\"Overwriting attribute %s with %s\", key, value)\n            self.attributes[key] = value\n\n    def is_agent_invocation(self) -&gt; bool:\n        \"\"\"Check whether this span is an agent invocation (the very first span).\"\"\"\n        return self.attributes.get(GenAI.OPERATION_NAME) == \"invoke_agent\"\n\n    def is_llm_call(self) -&gt; bool:\n        \"\"\"Check whether this span is a call to an LLM.\"\"\"\n        return self.attributes.get(GenAI.OPERATION_NAME) == \"call_llm\"\n\n    def is_tool_execution(self) -&gt; bool:\n        \"\"\"Check whether this span is an execution of a tool.\"\"\"\n        return self.attributes.get(GenAI.OPERATION_NAME) == \"execute_tool\"\n\n    def get_input_messages(self) -&gt; list[AgentMessage] | None:\n        \"\"\"Extract input messages from an LLM call span.\n\n        Returns:\n            List of message dicts with 'role' and 'content' keys, or None if not available.\n\n        \"\"\"\n        if not self.is_llm_call():\n            msg = \"Span is not an LLM call\"\n            raise ValueError(msg)\n\n        messages_json = self.attributes.get(GenAI.INPUT_MESSAGES)\n        if not messages_json:\n            return None\n\n        try:\n            parsed_messages = json.loads(messages_json)\n        except (json.JSONDecodeError, TypeError) as e:\n            msg = \"Failed to parse input messages from span\"\n            logger.error(msg)\n            raise ValueError(msg) from e\n        if not isinstance(parsed_messages, list):\n            msg = \"Input messages are not a list of messages\"\n            raise ValueError(msg)\n        return [AgentMessage.model_validate(msg) for msg in parsed_messages]\n\n    def get_output_content(self) -&gt; str | None:\n        \"\"\"Extract output content from an LLM call or tool execution span.\n\n        Returns:\n            The output content as a string, or None if not available.\n\n        \"\"\"\n        if not self.is_llm_call() and not self.is_tool_execution():\n            msg = \"Span is not an LLM call or tool execution\"\n            raise ValueError(msg)\n\n        output = self.attributes.get(GenAI.OUTPUT)\n        if not output:\n            logger.debug(\"No output found in span\")\n            return None\n        return str(output)\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentSpan.from_otel","title":"<code>from_otel(otel_span)</code>  <code>classmethod</code>","text":"<p>Create an AgentSpan from an OTEL Span.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>@classmethod\ndef from_otel(cls, otel_span: Span) -&gt; AgentSpan:\n    \"\"\"Create an AgentSpan from an OTEL Span.\"\"\"\n    return cls(\n        name=otel_span.name,\n        kind=SpanKind.from_otel(otel_span.kind),\n        parent=SpanContext.from_otel(otel_span.parent),\n        start_time=otel_span.start_time,\n        end_time=otel_span.end_time,\n        status=Status.from_otel(otel_span.status),\n        context=SpanContext.from_otel(otel_span.context),\n        attributes=dict(otel_span.attributes) if otel_span.attributes else {},\n        links=[Link.from_otel(link) for link in otel_span.links],\n        events=[Event.from_otel(event) for event in otel_span.events],\n        resource=Resource.from_otel(otel_span.resource),\n    )\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentSpan.get_input_messages","title":"<code>get_input_messages()</code>","text":"<p>Extract input messages from an LLM call span.</p> <p>Returns:</p> Type Description <code>list[AgentMessage] | None</code> <p>List of message dicts with 'role' and 'content' keys, or None if not available.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>def get_input_messages(self) -&gt; list[AgentMessage] | None:\n    \"\"\"Extract input messages from an LLM call span.\n\n    Returns:\n        List of message dicts with 'role' and 'content' keys, or None if not available.\n\n    \"\"\"\n    if not self.is_llm_call():\n        msg = \"Span is not an LLM call\"\n        raise ValueError(msg)\n\n    messages_json = self.attributes.get(GenAI.INPUT_MESSAGES)\n    if not messages_json:\n        return None\n\n    try:\n        parsed_messages = json.loads(messages_json)\n    except (json.JSONDecodeError, TypeError) as e:\n        msg = \"Failed to parse input messages from span\"\n        logger.error(msg)\n        raise ValueError(msg) from e\n    if not isinstance(parsed_messages, list):\n        msg = \"Input messages are not a list of messages\"\n        raise ValueError(msg)\n    return [AgentMessage.model_validate(msg) for msg in parsed_messages]\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentSpan.get_output_content","title":"<code>get_output_content()</code>","text":"<p>Extract output content from an LLM call or tool execution span.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>The output content as a string, or None if not available.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>def get_output_content(self) -&gt; str | None:\n    \"\"\"Extract output content from an LLM call or tool execution span.\n\n    Returns:\n        The output content as a string, or None if not available.\n\n    \"\"\"\n    if not self.is_llm_call() and not self.is_tool_execution():\n        msg = \"Span is not an LLM call or tool execution\"\n        raise ValueError(msg)\n\n    output = self.attributes.get(GenAI.OUTPUT)\n    if not output:\n        logger.debug(\"No output found in span\")\n        return None\n    return str(output)\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentSpan.is_agent_invocation","title":"<code>is_agent_invocation()</code>","text":"<p>Check whether this span is an agent invocation (the very first span).</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>def is_agent_invocation(self) -&gt; bool:\n    \"\"\"Check whether this span is an agent invocation (the very first span).\"\"\"\n    return self.attributes.get(GenAI.OPERATION_NAME) == \"invoke_agent\"\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentSpan.is_llm_call","title":"<code>is_llm_call()</code>","text":"<p>Check whether this span is a call to an LLM.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>def is_llm_call(self) -&gt; bool:\n    \"\"\"Check whether this span is a call to an LLM.\"\"\"\n    return self.attributes.get(GenAI.OPERATION_NAME) == \"call_llm\"\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentSpan.is_tool_execution","title":"<code>is_tool_execution()</code>","text":"<p>Check whether this span is an execution of a tool.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>def is_tool_execution(self) -&gt; bool:\n    \"\"\"Check whether this span is an execution of a tool.\"\"\"\n    return self.attributes.get(GenAI.OPERATION_NAME) == \"execute_tool\"\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentSpan.set_attributes","title":"<code>set_attributes(attributes)</code>","text":"<p>Set attributes for the span.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>def set_attributes(self, attributes: Mapping[str, AttributeValue]) -&gt; None:\n    \"\"\"Set attributes for the span.\"\"\"\n    for key, value in attributes.items():\n        if key in self.attributes:\n            logger.warning(\"Overwriting attribute %s with %s\", key, value)\n        self.attributes[key] = value\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.AgentSpan.to_readable_span","title":"<code>to_readable_span()</code>","text":"<p>Create an ReadableSpan from the AgentSpan.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>def to_readable_span(self) -&gt; ReadableSpan:\n    \"\"\"Create an ReadableSpan from the AgentSpan.\"\"\"\n    return ReadableSpan(\n        name=self.name,\n        kind=self.kind,\n        parent=self.parent,\n        start_time=self.start_time,\n        end_time=self.end_time,\n        status=self.status,\n        context=self.context,\n        attributes=self.attributes,\n        links=self.links,\n        events=self.events,\n        resource=self.resource,\n    )\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.CostInfo","title":"<code>any_agent.tracing.agent_trace.CostInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Cost information.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>class CostInfo(BaseModel):\n    \"\"\"Cost information.\"\"\"\n\n    input_cost: float\n    \"Cost associated to the input tokens.\"\n\n    output_cost: float\n    \"\"\"Cost associated to the output tokens.\"\"\"\n\n    @property\n    def total_cost(self) -&gt; float:\n        \"\"\"Total cost.\"\"\"\n        return self.input_cost + self.output_cost\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.CostInfo.input_cost","title":"<code>input_cost</code>  <code>instance-attribute</code>","text":"<p>Cost associated to the input tokens.</p>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.CostInfo.output_cost","title":"<code>output_cost</code>  <code>instance-attribute</code>","text":"<p>Cost associated to the output tokens.</p>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.CostInfo.total_cost","title":"<code>total_cost</code>  <code>property</code>","text":"<p>Total cost.</p>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.TokenInfo","title":"<code>any_agent.tracing.agent_trace.TokenInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Token Count information.</p> Source code in <code>src/any_agent/tracing/agent_trace.py</code> <pre><code>class TokenInfo(BaseModel):\n    \"\"\"Token Count information.\"\"\"\n\n    input_tokens: int\n    \"\"\"Number of input tokens.\"\"\"\n\n    output_tokens: int\n    \"\"\"Number of output tokens.\"\"\"\n\n    @property\n    def total_tokens(self) -&gt; int:\n        \"\"\"Total number of tokens.\"\"\"\n        return self.input_tokens + self.output_tokens\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.TokenInfo.input_tokens","title":"<code>input_tokens</code>  <code>instance-attribute</code>","text":"<p>Number of input tokens.</p>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.TokenInfo.output_tokens","title":"<code>output_tokens</code>  <code>instance-attribute</code>","text":"<p>Number of output tokens.</p>"},{"location":"api/tracing/#any_agent.tracing.agent_trace.TokenInfo.total_tokens","title":"<code>total_tokens</code>  <code>property</code>","text":"<p>Total number of tokens.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI","title":"<code>any_agent.tracing.attributes.GenAI</code>","text":"<p>Constants exported for convenience to access span attributes.</p> <p>Trying to follow OpenTelemetry's Semantic Conventions for Generative AI.</p> <p>We import the constants from <code>opentelemetry.semconv._incubating.attributes.gen_ai_attributes</code> whenever is possible.</p> <p>We only expose the keys that we currently use in <code>any-agent</code>.</p> Source code in <code>src/any_agent/tracing/attributes.py</code> <pre><code>class GenAI:\n    \"\"\"Constants exported for convenience to access span attributes.\n\n    Trying to follow OpenTelemetry's [Semantic Conventions for Generative AI](https://opentelemetry.io/docs/specs/semconv/gen-ai/).\n\n    We import the constants from `opentelemetry.semconv._incubating.attributes.gen_ai_attributes`\n    whenever is possible.\n\n    We only expose the keys that we currently use in `any-agent`.\n    \"\"\"\n\n    AGENT_DESCRIPTION = GEN_AI_AGENT_DESCRIPTION\n    \"\"\"Free-form description of the GenAI agent provided by the application.\"\"\"\n\n    AGENT_NAME = GEN_AI_AGENT_NAME\n    \"\"\"Human-readable name of the GenAI agent provided by the application.\"\"\"\n\n    INPUT_MESSAGES = \"gen_ai.input.messages\"\n    \"\"\"System prompt and user input.\"\"\"\n\n    OPERATION_NAME = GEN_AI_OPERATION_NAME\n    \"\"\"The name of the operation being performed.\"\"\"\n\n    OUTPUT = \"gen_ai.output\"\n    \"\"\"Used in both LLM Calls and Tool Executions for holding their respective outputs.\"\"\"\n\n    OUTPUT_TYPE = GEN_AI_OUTPUT_TYPE\n    \"\"\"Represents the content type requested by the client.\"\"\"\n\n    REQUEST_MODEL = GEN_AI_REQUEST_MODEL\n    \"\"\"The name of the GenAI model a request is being made to.\"\"\"\n\n    TOOL_ARGS = \"gen_ai.tool.args\"\n    \"\"\"Arguments passed to the executed tool.\"\"\"\n\n    TOOL_DESCRIPTION = GEN_AI_TOOL_DESCRIPTION\n    \"\"\"The tool description.\"\"\"\n\n    TOOL_NAME = GEN_AI_TOOL_NAME\n    \"\"\"Name of the tool utilized by the agent.\"\"\"\n\n    USAGE_INPUT_COST = \"gen_ai.usage.input_cost\"\n    \"\"\"Dollars spent for the input of the LLM.\"\"\"\n\n    USAGE_INPUT_TOKENS = GEN_AI_USAGE_INPUT_TOKENS\n    \"\"\"The number of tokens used in the GenAI input (prompt).\"\"\"\n\n    USAGE_OUTPUT_COST = \"gen_ai.usage.output_cost\"\n    \"\"\"Dollars spent for the output of the LLM.\"\"\"\n\n    USAGE_OUTPUT_TOKENS = GEN_AI_USAGE_OUTPUT_TOKENS\n    \"\"\"The number of tokens used in the GenAI response (completion).\"\"\"\n</code></pre>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.AGENT_DESCRIPTION","title":"<code>AGENT_DESCRIPTION = GEN_AI_AGENT_DESCRIPTION</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Free-form description of the GenAI agent provided by the application.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.AGENT_NAME","title":"<code>AGENT_NAME = GEN_AI_AGENT_NAME</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Human-readable name of the GenAI agent provided by the application.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.INPUT_MESSAGES","title":"<code>INPUT_MESSAGES = 'gen_ai.input.messages'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>System prompt and user input.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.OPERATION_NAME","title":"<code>OPERATION_NAME = GEN_AI_OPERATION_NAME</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the operation being performed.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.OUTPUT","title":"<code>OUTPUT = 'gen_ai.output'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Used in both LLM Calls and Tool Executions for holding their respective outputs.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.OUTPUT_TYPE","title":"<code>OUTPUT_TYPE = GEN_AI_OUTPUT_TYPE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Represents the content type requested by the client.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.REQUEST_MODEL","title":"<code>REQUEST_MODEL = GEN_AI_REQUEST_MODEL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the GenAI model a request is being made to.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.TOOL_ARGS","title":"<code>TOOL_ARGS = 'gen_ai.tool.args'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Arguments passed to the executed tool.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.TOOL_DESCRIPTION","title":"<code>TOOL_DESCRIPTION = GEN_AI_TOOL_DESCRIPTION</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The tool description.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.TOOL_NAME","title":"<code>TOOL_NAME = GEN_AI_TOOL_NAME</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the tool utilized by the agent.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.USAGE_INPUT_COST","title":"<code>USAGE_INPUT_COST = 'gen_ai.usage.input_cost'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dollars spent for the input of the LLM.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.USAGE_INPUT_TOKENS","title":"<code>USAGE_INPUT_TOKENS = GEN_AI_USAGE_INPUT_TOKENS</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The number of tokens used in the GenAI input (prompt).</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.USAGE_OUTPUT_COST","title":"<code>USAGE_OUTPUT_COST = 'gen_ai.usage.output_cost'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dollars spent for the output of the LLM.</p>"},{"location":"api/tracing/#any_agent.tracing.attributes.GenAI.USAGE_OUTPUT_TOKENS","title":"<code>USAGE_OUTPUT_TOKENS = GEN_AI_USAGE_OUTPUT_TOKENS</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The number of tokens used in the GenAI response (completion).</p>"},{"location":"cookbook/a2a_as_tool/","title":"Use an Agent as a tool for another agent.","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install 'any-agent[a2a]'\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> %pip install 'any-agent[a2a]'  import nest_asyncio  nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre>import os\nfrom getpass import getpass\n\n# This notebook communicates with Mistral models using the Mistral API.\nif \"MISTRAL_API_KEY\" not in os.environ:\n    print(\"MISTRAL_API_KEY not found in environment!\")\n    api_key = getpass(\"Please enter your MISTRAL_API_KEY: \")\n    os.environ[\"MISTRAL_API_KEY\"] = api_key\n    print(\"MISTRAL_API_KEY set for this session!\")\nelse:\n    print(\"MISTRAL_API_KEY found in environment.\")\n</pre> import os from getpass import getpass  # This notebook communicates with Mistral models using the Mistral API. if \"MISTRAL_API_KEY\" not in os.environ:     print(\"MISTRAL_API_KEY not found in environment!\")     api_key = getpass(\"Please enter your MISTRAL_API_KEY: \")     os.environ[\"MISTRAL_API_KEY\"] = api_key     print(\"MISTRAL_API_KEY set for this session!\") else:     print(\"MISTRAL_API_KEY found in environment.\") In\u00a0[\u00a0]: Copied! <pre>import asyncio\n\nimport httpx\n\nfrom any_agent import AgentConfig, AnyAgent\nfrom any_agent.config import MCPStdio\nfrom any_agent.serving import A2AServingConfig\n\n# This MCP Tool relies upon uvx https://docs.astral.sh/uv/getting-started/installation/\ntime_tool = MCPStdio(\n    command=\"uvx\",\n    args=[\"mcp-server-time\", \"--local-timezone=America/New_York\"],\n    tools=[\n        \"get_current_time\",\n    ],\n)\n\ntime = await AnyAgent.create_async(\n    \"tinyagent\",  # See all options in https://mozilla-ai.github.io/any-agent/\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\",\n        name=\"time_agent\",\n        description=\"I'm an agent to help with getting the time\",\n        tools=[time_tool],\n    ),\n)\ntime_handle = await time.serve_async(A2AServingConfig(port=0, endpoint=\"/time\"))\n\nweather_expert = await AnyAgent.create_async(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\",\n        name=\"weather_expert\",\n        instructions=\"You're an expert that is an avid skier, recommend a great location to ski given a time of the year\",\n        description=\"I'm an agent that is an expert in recommending my favorite location given a time of the year\",\n    ),\n)\nweather_handle = await weather_expert.serve_async(\n    A2AServingConfig(port=0, endpoint=\"/location_recommender\")\n)\n\nweather_port = weather_handle.port\ntime_port = time_handle.port\n\nmax_attempts = 20\npoll_interval = 0.5\nattempts = 0\ntime_url = f\"http://localhost:{time_port}\"\nweather_url = f\"http://localhost:{weather_port}\"\nasync with httpx.AsyncClient() as client:\n    while True:\n        try:\n            # Try to make a basic GET request to check if server is responding\n            await client.get(time_url, timeout=1.0)\n            print(f\"Server is ready at {weather_url}\")\n            break\n        except (httpx.RequestError, httpx.TimeoutException):\n            # Server not ready yet, continue polling\n            pass\n\n        await asyncio.sleep(poll_interval)\n        attempts += 1\n        if attempts &gt;= max_attempts:\n            msg = f\"Could not connect to the servers. Tried {max_attempts} times with {poll_interval} second interval.\"\n            raise ConnectionError(msg)\n</pre> import asyncio  import httpx  from any_agent import AgentConfig, AnyAgent from any_agent.config import MCPStdio from any_agent.serving import A2AServingConfig  # This MCP Tool relies upon uvx https://docs.astral.sh/uv/getting-started/installation/ time_tool = MCPStdio(     command=\"uvx\",     args=[\"mcp-server-time\", \"--local-timezone=America/New_York\"],     tools=[         \"get_current_time\",     ], )  time = await AnyAgent.create_async(     \"tinyagent\",  # See all options in https://mozilla-ai.github.io/any-agent/     AgentConfig(         model_id=\"mistral:mistral-small-latest\",         name=\"time_agent\",         description=\"I'm an agent to help with getting the time\",         tools=[time_tool],     ), ) time_handle = await time.serve_async(A2AServingConfig(port=0, endpoint=\"/time\"))  weather_expert = await AnyAgent.create_async(     \"tinyagent\",     AgentConfig(         model_id=\"mistral:mistral-small-latest\",         name=\"weather_expert\",         instructions=\"You're an expert that is an avid skier, recommend a great location to ski given a time of the year\",         description=\"I'm an agent that is an expert in recommending my favorite location given a time of the year\",     ), ) weather_handle = await weather_expert.serve_async(     A2AServingConfig(port=0, endpoint=\"/location_recommender\") )  weather_port = weather_handle.port time_port = time_handle.port  max_attempts = 20 poll_interval = 0.5 attempts = 0 time_url = f\"http://localhost:{time_port}\" weather_url = f\"http://localhost:{weather_port}\" async with httpx.AsyncClient() as client:     while True:         try:             # Try to make a basic GET request to check if server is responding             await client.get(time_url, timeout=1.0)             print(f\"Server is ready at {weather_url}\")             break         except (httpx.RequestError, httpx.TimeoutException):             # Server not ready yet, continue polling             pass          await asyncio.sleep(poll_interval)         attempts += 1         if attempts &gt;= max_attempts:             msg = f\"Could not connect to the servers. Tried {max_attempts} times with {poll_interval} second interval.\"             raise ConnectionError(msg) In\u00a0[\u00a0]: Copied! <pre>from any_agent import AgentConfig, AnyAgent\nfrom any_agent.tools import a2a_tool_async\n\nconfig = AgentConfig(\n    model_id=\"mistral:mistral-small-latest\",\n    instructions=\"Use the available tools to obtain additional information to answer the query.\",\n    description=\"The orchestrator that can use other agents via tools using the A2A protocol.\",\n    tools=[\n        await a2a_tool_async(\n            f\"http://localhost:{time_port}/time\",\n            http_kwargs={\n                \"timeout\": 30\n            },  # This gives the time agent up to 30 seconds to respond to each request\n        ),\n        await a2a_tool_async(\n            f\"http://localhost:{weather_port}/location_recommender\",\n            http_kwargs={\n                \"timeout\": 30\n            },  # This gives the weather agent up to 30 seconds to respond to each request\n        ),\n    ],\n)\n\nagent = await AnyAgent.create_async(\n    agent_framework=\"tinyagent\",\n    agent_config=config,\n)\n\nagent_trace = await agent.run_async(\"Where should I go this weekend to ski?\")\n\nprint(agent_trace.final_output)\n</pre> from any_agent import AgentConfig, AnyAgent from any_agent.tools import a2a_tool_async  config = AgentConfig(     model_id=\"mistral:mistral-small-latest\",     instructions=\"Use the available tools to obtain additional information to answer the query.\",     description=\"The orchestrator that can use other agents via tools using the A2A protocol.\",     tools=[         await a2a_tool_async(             f\"http://localhost:{time_port}/time\",             http_kwargs={                 \"timeout\": 30             },  # This gives the time agent up to 30 seconds to respond to each request         ),         await a2a_tool_async(             f\"http://localhost:{weather_port}/location_recommender\",             http_kwargs={                 \"timeout\": 30             },  # This gives the weather agent up to 30 seconds to respond to each request         ),     ], )  agent = await AnyAgent.create_async(     agent_framework=\"tinyagent\",     agent_config=config, )  agent_trace = await agent.run_async(\"Where should I go this weekend to ski?\")  print(agent_trace.final_output) In\u00a0[\u00a0]: Copied! <pre>await time_handle.shutdown()\nawait weather_handle.shutdown()\n</pre> await time_handle.shutdown() await weather_handle.shutdown()"},{"location":"cookbook/a2a_as_tool/#use-an-agent-as-a-tool-for-another-agent","title":"Use an Agent as a tool for another agent.\u00b6","text":"<p>Multi-Agent systems are complicated! Enter the A2A protocol by Google: this protocol allows for simpler communication between agents, and easily enables the use of an agent as a tool for another agent. In this tutorial we'll show how you can create a few agents with any-agent and have an agent be provided as a tool for the other agent.</p> <p>This tutorial assumes basic familiarity with any-agent: if you haven't used any-agent before you may also find the Creating your first agent cookbook to be useful. You also may find the other A2A related cookbook to be useful: Serve an Agent with A2A</p> <p>Note: because this tutorial relies upon advanced stdio/stderr communication using the MCP Server, it cannot be run on Google Colab.</p>"},{"location":"cookbook/a2a_as_tool/#install-dependencies","title":"Install Dependencies\u00b6","text":"<p>any-agent uses the python asyncio module to support async functionality. When running in Jupyter notebooks, this means we need to enable the use of nested event loops. We'll install any-agent and enable this below using nest_asyncio.</p>"},{"location":"cookbook/a2a_as_tool/#configure-the-first-two-agents-and-serve-them-over-a2a","title":"Configure the first two agents and serve them over A2A\u00b6","text":"<p>Let's give our first two \"helper\" agents some very simple capabilities. For this demo, we'll use the async method <code>agent.serve_async</code> so that we can easily run all of the agents from inside the notebook in a single python process.</p>"},{"location":"cookbook/a2a_as_tool/#configure-and-use-the-third-agent","title":"Configure and use the Third Agent\u00b6","text":"<p>Now that the first two agents are serving over A2A, our main agent can be given access to use it just like a tool! In order to do this you use the <code>a2a_tool_async</code> function which retrieves the info about the agent and allows you to call it.</p>"},{"location":"cookbook/a2a_as_tool/#shut-down-the-servers-and-the-agents","title":"Shut down the servers and the agents\u00b6","text":"<p>The following code can be used to gracefully shut down the agents that are serving via A2A.</p>"},{"location":"cookbook/agent_with_local_llm/","title":"Local Agent","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install 'any-agent' --quiet\n%pip install ipywidgets --quiet\n\nfrom pathlib import Path\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> %pip install 'any-agent' --quiet %pip install ipywidgets --quiet  from pathlib import Path  import nest_asyncio  nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre>def read_file(file_path: str) -&gt; str:\n    \"\"\"\n    Read a file from the file path given and return its content.\n\n    Args:\n        file_path: The path of the file to read.\n    \"\"\"\n    try:\n        with open(file_path) as file:\n            content = file.read()\n    except Exception as e:\n        content = f\"Error reading file: {file_path} \\n\\n {e}\"\n    return content\n</pre> def read_file(file_path: str) -&gt; str:     \"\"\"     Read a file from the file path given and return its content.      Args:         file_path: The path of the file to read.     \"\"\"     try:         with open(file_path) as file:             content = file.read()     except Exception as e:         content = f\"Error reading file: {file_path} \\n\\n {e}\"     return content In\u00a0[\u00a0]: Copied! <pre>def write_file(file_path: str, content: str) -&gt; str:\n    \"\"\"\n    Write content to a file at the specified file path.\n\n    Args:\n        file_path: The path of the file to write.\n        content: The content to write to the file.\n    \"\"\"\n    try:\n        abs_file_path = Path(file_path).resolve()\n        with open(abs_file_path, \"w\") as file:\n            file.write(content)\n        result = f\"Successfully wrote to file: {abs_file_path}\"\n    except Exception as e:\n        result = f\"Error writing file: {abs_file_path} \\n\\n {e}\"\n    return result\n</pre> def write_file(file_path: str, content: str) -&gt; str:     \"\"\"     Write content to a file at the specified file path.      Args:         file_path: The path of the file to write.         content: The content to write to the file.     \"\"\"     try:         abs_file_path = Path(file_path).resolve()         with open(abs_file_path, \"w\") as file:             file.write(content)         result = f\"Successfully wrote to file: {abs_file_path}\"     except Exception as e:         result = f\"Error writing file: {abs_file_path} \\n\\n {e}\"     return result <p>Now that you have downloaded your LLM and you have defined your tools, you need to pick your agent framework to build your agent. Note that the agent you'll build with any-agent can be run across multiple agent frameworks (Smolagent, TinyAgent, OpenAI, etc) and across various LLMs (Llama, DeepSeek, Mistral, etc). For this example, we will use the tinyagent framework.</p> In\u00a0[\u00a0]: Copied! <pre>from any_agent import AgentConfig, AnyAgent\n\nmodel_id = \"ollama/qwen2.5:14b\"\n\nmodel_args = {\n    \"num_ctx\": 32000,\n    \"temperature\": 0.0,\n    # Note: We use a strict 'WAIT' stop token to force the local model to pause and let the Python kernel execute the tool.\n    # Without this, local models often hallucinate the tool output.\n    \"stop\": [\"WAIT\"],\n}\n\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=model_id,\n        instructions=\"\"\"You are a precise tool-calling agent.\n\n        PROTOCOL:\n        1. Output valid JSON for the tool you want to use.\n        2. AFTER the JSON, write the word \"WAIT\".\n        3. The system will then pause and run the tool for you.\n        FORMAT:\n        {\n            \"name\": \"tool_name\",\n            \"arguments\": {\n                \"arg_name\": \"value\"\n            }\n        }\n        WAIT\n        \"\"\",\n        tools=[read_file, write_file],\n        model_args=model_args,\n    ),\n)\n</pre> from any_agent import AgentConfig, AnyAgent  model_id = \"ollama/qwen2.5:14b\"  model_args = {     \"num_ctx\": 32000,     \"temperature\": 0.0,     # Note: We use a strict 'WAIT' stop token to force the local model to pause and let the Python kernel execute the tool.     # Without this, local models often hallucinate the tool output.     \"stop\": [\"WAIT\"], }  agent = AnyAgent.create(     \"tinyagent\",     AgentConfig(         model_id=model_id,         instructions=\"\"\"You are a precise tool-calling agent.          PROTOCOL:         1. Output valid JSON for the tool you want to use.         2. AFTER the JSON, write the word \"WAIT\".         3. The system will then pause and run the tool for you.         FORMAT:         {             \"name\": \"tool_name\",             \"arguments\": {                 \"arg_name\": \"value\"             }         }         WAIT         \"\"\",         tools=[read_file, write_file],         model_args=model_args,     ), ) In\u00a0[\u00a0]: Copied! <pre>abs_path = Path(\"../../demo/app.py\").resolve()\noutput_dir = Path(\"agent_outputs\")\noutput_dir.mkdir(exist_ok=True)\noutput_file = output_dir / \"code_summary.txt\"\n\n# --- STEP 1: FORCE READ ---\nprint(\"\ud83e\udd16 Step 1: Reading file...\")\nread_agent = agent.run(f\"Output the JSON to read the file at: '{abs_path}'\")\n# The framework executes the tool and returns the trace\n# We assume the last message contains the file content (or we grab it manually)\n# Note: In a real app, you'd extract the tool output programmatically.\n# For this cookbook, we can just feed the file content explicitly if the agent fails,\n# but usually, the agent trace contains the history.\n\n# --- STEP 2: FORCE WRITE ---\n# We feed the Previous History + New Instruction\nprint(\"\ud83e\udd16 Step 2: Summarizing and Writing...\")\nwrite_agent = agent.run(\n    f\"\"\"\n    I have read the file.\n    Here is the content: {read_file(str(abs_path))}\n    Task: Write a summary of this content to: '{output_file}'\n    \"\"\"\n)\n</pre> abs_path = Path(\"../../demo/app.py\").resolve() output_dir = Path(\"agent_outputs\") output_dir.mkdir(exist_ok=True) output_file = output_dir / \"code_summary.txt\"  # --- STEP 1: FORCE READ --- print(\"\ud83e\udd16 Step 1: Reading file...\") read_agent = agent.run(f\"Output the JSON to read the file at: '{abs_path}'\") # The framework executes the tool and returns the trace # We assume the last message contains the file content (or we grab it manually) # Note: In a real app, you'd extract the tool output programmatically. # For this cookbook, we can just feed the file content explicitly if the agent fails, # but usually, the agent trace contains the history.  # --- STEP 2: FORCE WRITE --- # We feed the Previous History + New Instruction print(\"\ud83e\udd16 Step 2: Summarizing and Writing...\") write_agent = agent.run(     f\"\"\"     I have read the file.     Here is the content: {read_file(str(abs_path))}     Task: Write a summary of this content to: '{output_file}'     \"\"\" ) <p>The <code>agent.run</code> method returns an AgentTrace object, which has a few convenient attributes for displaying some interesting information about the run.</p> In\u00a0[\u00a0]: Copied! <pre>from any_agent.tracing.attributes import GenAI\n\n\ndef print_step_analysis(step_name, trace):\n    print(f\"\\n=== Analysis: {step_name} ===\")\n    print(f\"Duration: {trace.duration.total_seconds():.2f} seconds\")\n\n    tools_found = False\n    for span in trace.spans:\n        if span.is_tool_execution():\n            tool_name = span.attributes.get(GenAI.TOOL_NAME, \"Unknown\")\n            print(f\"\u2713 Tool Called: {tool_name}\")\n            tools_found = True\n\n    if not tools_found:\n        print(\"\u26a0\ufe0f  No tools were called in this step.\")\n        # Optional: Print raw output to debug why\n        print(f\"Raw Output: {trace.final_output[:100]}...\")\n\n\n# 1. Inspect the Read Step\nprint_step_analysis(\"Step 1 (Force Read)\", read_agent)\n\n# 2. Inspect the Write Step\nprint_step_analysis(\"Step 2 (Summarize &amp; Write)\", write_agent)\n\n# 3. Verify the physical file\nprint(\"\\n=== Final File Verification ===\")\nif output_file.exists():\n    print(f\"\u2713 SUCCESS: File created at: {output_file.name}\")\n    print(\"\\n\ud83d\udcc4 File Contents:\")\n    print(\"-\" * 50)\n    print(output_file.read_text())\n    print(\"-\" * 50)\nelse:\n    print(f\"\u2717 FAILURE: File not found at {output_file}\")\n</pre> from any_agent.tracing.attributes import GenAI   def print_step_analysis(step_name, trace):     print(f\"\\n=== Analysis: {step_name} ===\")     print(f\"Duration: {trace.duration.total_seconds():.2f} seconds\")      tools_found = False     for span in trace.spans:         if span.is_tool_execution():             tool_name = span.attributes.get(GenAI.TOOL_NAME, \"Unknown\")             print(f\"\u2713 Tool Called: {tool_name}\")             tools_found = True      if not tools_found:         print(\"\u26a0\ufe0f  No tools were called in this step.\")         # Optional: Print raw output to debug why         print(f\"Raw Output: {trace.final_output[:100]}...\")   # 1. Inspect the Read Step print_step_analysis(\"Step 1 (Force Read)\", read_agent)  # 2. Inspect the Write Step print_step_analysis(\"Step 2 (Summarize &amp; Write)\", write_agent)  # 3. Verify the physical file print(\"\\n=== Final File Verification ===\") if output_file.exists():     print(f\"\u2713 SUCCESS: File created at: {output_file.name}\")     print(\"\\n\ud83d\udcc4 File Contents:\")     print(\"-\" * 50)     print(output_file.read_text())     print(\"-\" * 50) else:     print(f\"\u2717 FAILURE: File not found at {output_file}\")"},{"location":"cookbook/agent_with_local_llm/#local-agent","title":"Local Agent\u00b6","text":"<p>(no Google Colab for this one since it is meant to be run entirely locally)</p> <p>This tutorial will guide on how to run agent, fully locally / offline i.e., running with a local LLM and a couple of local tools (Callable Python functions), so no data will leave your machine!</p> <p>This can be especially useful for privacy-sensitive applications or when you want to avoid any cloud dependencies.</p> <p>In this example, we will showcase how to let an agent read and write in your local filesystem! Specifically, we will give read-access to the agent to some files in our codebase and ask it to create a short summary of what this code does, and write it to a file.</p>"},{"location":"cookbook/agent_with_local_llm/#install-dependencies","title":"Install Dependencies\u00b6","text":"<p>any-agent uses the python asyncio module to support async functionality. When running in Jupyter notebooks, this means we need to enable the use of nested event loops. We'll install any-agent and enable this below using nest_asyncio.</p>"},{"location":"cookbook/agent_with_local_llm/#set-up-your-own-llm-locally","title":"Set up your own LLM locally\u00b6","text":"<p>Regardless of which agent framework you choose in any-agent, all of them support any-llm, which is a proxy that allows us to use whichever LLM inside the framework, hosted on by any provider. For example, we could use a local model via llama.cpp or llamafile, a google hosted gemini model, or a AWS bedrock hosted Llama model. For this example, we will use Ollama to run our LLM locally!</p>"},{"location":"cookbook/agent_with_local_llm/#ollama-setup","title":"Ollama setup\u00b6","text":"<p>First, install Ollama by following their instructions: https://ollama.com/download</p>"},{"location":"cookbook/agent_with_local_llm/#picking-an-llm","title":"Picking an LLM\u00b6","text":"<p>Pick a model that you can run locally based on your hardware and download it from your terminal. For example:</p> <p>16-24GB RAM -&gt; qwen2.5:14b (Recommended for Agents), granite3.3:8b, or gemma3:12b with ~10\u201320k context length</p> <p>24+GB RAM -&gt; <code>mistral-small3.2:24b</code> or <code>devstral:24b</code> with ~20k+ context length</p> <p>NOTE: Smaller models have shown to be more inconsistent and less capable, especially as task complexity increases. If you have the hardware, we highly recommend using the larger models like <code>mistral-small3.2:24b</code> for better performance.</p>"},{"location":"cookbook/agent_with_local_llm/#serving-the-model-with-the-appropriate-context-length","title":"Serving the model with the appropriate context length\u00b6","text":"<p>By default, Ollama forces a context length of 8192 tokens to all models, which is not enough for our agent to work properly. We can simply pass to the AgentConfig of <code>any-agent</code> as a model argument (<code>num_ctx</code> is ollama-specific) our desired value of context length like so:</p> <pre><code>    AgentConfig(\n        model_id=\"ollama/qwen2.5:14b\",\n        instructions=\"You must use the available tools to solve the task.\",\n        tools=[mcp_filesystem, show_plan],\n        model_args={\"num_ctx\": 32000},\n    )\n</code></pre> <p>References: AgentConfig, num_ctx</p> <p>All four of the models above have a max context length of 128k tokens, but if you have limited RAM if you set it to 128k it might cause you memory issues. For this example, we will set it to 32,000 tokens and provide a relatively small codebase.</p>"},{"location":"cookbook/agent_with_local_llm/#load-the-model","title":"Load the model\u00b6","text":"<p>Firstly, download the Ollama model locally:</p> <pre><code>ollama pull qwen2.5:14b\n</code></pre> <p>Note: You will need Ollama CLI installed</p>"},{"location":"cookbook/agent_with_local_llm/#configure-the-agent-and-the-tools","title":"Configure the Agent and the Tools\u00b6","text":""},{"location":"cookbook/agent_with_local_llm/#pick-which-tools-to-use","title":"Pick which tools to use\u00b6","text":"<p>Since we want our agent to work fully locally/offline, we will not add any tools that require communication with remote servers. In this example we are using python callable functions as tools, but we could also have used MCP servers that run fully locally (e.g. mcp/filesystem)</p>"},{"location":"cookbook/agent_with_local_llm/#run-the-agent","title":"Run the Agent\u00b6","text":""},{"location":"cookbook/agent_with_local_llm/#view-the-results","title":"View the results\u00b6","text":""},{"location":"cookbook/callbacks/","title":"Using Callbacks","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install 'any-agent' --quiet\n%pip install ddgs --quiet\n\nimport warnings\n\nimport nest_asyncio\n\n# Suppress technical warnings to reduce noise for the user\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nnest_asyncio.apply()\n</pre> %pip install 'any-agent' --quiet %pip install ddgs --quiet  import warnings  import nest_asyncio  # Suppress technical warnings to reduce noise for the user warnings.filterwarnings(\"ignore\", category=DeprecationWarning) warnings.filterwarnings(\"ignore\", category=RuntimeWarning)  nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre>import os\nfrom getpass import getpass\n\nif \"MISTRAL_API_KEY\" not in os.environ:\n    os.environ[\"MISTRAL_API_KEY\"] = getpass(\"Enter your Mistral API Key: \")\n</pre> import os from getpass import getpass  if \"MISTRAL_API_KEY\" not in os.environ:     os.environ[\"MISTRAL_API_KEY\"] = getpass(\"Enter your Mistral API Key: \") In\u00a0[\u00a0]: Copied! <pre>from any_agent import AgentConfig, AnyAgent\nfrom any_agent.tools import search_web\n</pre> from any_agent import AgentConfig, AnyAgent from any_agent.tools import search_web <p><code>any-agent</code> comes with a default callback that will always be used unless you pass a value to <code>AgentConfig.callbacks</code>:</p> <ul> <li><code>ConsolePrintSpan</code></li> </ul> In\u00a0[\u00a0]: Copied! <pre>agent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(model_id=\"mistral:mistral-small-latest\", tools=[search_web]),\n)\n\n## Let's run a simple web search\nagent_trace = agent.run(\"What are 5 LLM agent frameworks that are trending in 2025?\")\n</pre> agent = AnyAgent.create(     \"tinyagent\",     AgentConfig(model_id=\"mistral:mistral-small-latest\", tools=[search_web]), )  ## Let's run a simple web search agent_trace = agent.run(\"What are 5 LLM agent frameworks that are trending in 2025?\") In\u00a0[\u00a0]: Copied! <pre>from any_agent.callbacks import Callback, Context\nfrom any_agent.tracing.attributes import GenAI\n\n\nclass ToolUsageCounter(Callback):\n    def before_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:\n        # 1. Initialize our counter if it doesn't exist yet\n        if \"tool_usage_count\" not in context.shared:\n            context.shared[\"tool_usage_count\"] = 0\n\n        # 2. Increment the counter\n        context.shared[\"tool_usage_count\"] += 1\n\n        # 3. Print for visibility (optional)\n        current_count = context.shared[\"tool_usage_count\"]\n        tool_name = context.current_span.attributes.get(GenAI.TOOL_NAME)\n        print(f\"\ud83e\uddee Tracker: Tool '{tool_name}' called. (Count: {current_count})\")\n\n        # 4. MUST return context\n        return context\n</pre> from any_agent.callbacks import Callback, Context from any_agent.tracing.attributes import GenAI   class ToolUsageCounter(Callback):     def before_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:         # 1. Initialize our counter if it doesn't exist yet         if \"tool_usage_count\" not in context.shared:             context.shared[\"tool_usage_count\"] = 0          # 2. Increment the counter         context.shared[\"tool_usage_count\"] += 1          # 3. Print for visibility (optional)         current_count = context.shared[\"tool_usage_count\"]         tool_name = context.current_span.attributes.get(GenAI.TOOL_NAME)         print(f\"\ud83e\uddee Tracker: Tool '{tool_name}' called. (Count: {current_count})\")          # 4. MUST return context         return context In\u00a0[\u00a0]: Copied! <pre>class BudgetLimit(Callback):\n    def __init__(self, max_tools: int):\n        self.max_tools = max_tools\n\n    def before_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:\n        # We can access the data set by the previous callback!\n        current_count = context.shared.get(\"tool_usage_count\", 0)\n\n        if current_count &gt; self.max_tools:\n            msg = f\"Exceeded limit of {self.max_tools} tool calls\"\n            raise RuntimeError(msg)\n\n        return context\n</pre> class BudgetLimit(Callback):     def __init__(self, max_tools: int):         self.max_tools = max_tools      def before_tool_execution(self, context: Context, *args, **kwargs) -&gt; Context:         # We can access the data set by the previous callback!         current_count = context.shared.get(\"tool_usage_count\", 0)          if current_count &gt; self.max_tools:             msg = f\"Exceeded limit of {self.max_tools} tool calls\"             raise RuntimeError(msg)          return context <p>Let's put this all together.</p> <ol> <li>We register our callbacks in <code>AgentConfig</code>.</li> <li>We use <code>get_default_callbacks()</code> to keep the nice console logging.</li> <li>We give the agent a hard task that requires multiple steps (\"Find the weather in 3 different cities\") to intentionally trigger our limit.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>from any_agent.callbacks import get_default_callbacks\n\n# 1. Configure Agent with our custom stack\nconfig = AgentConfig(\n    model_id=\"mistral:mistral-small-latest\",\n    tools=[search_web],\n    callbacks=[\n        ToolUsageCounter(),  # Runs first: Counts the step\n        BudgetLimit(\n            max_tools=2\n        ),  # Runs second: Checks the limit (set low to force a crash)\n        *get_default_callbacks(),  # Runs last: Logs to console\n    ],\n)\n\nagent = AnyAgent.create(\"tinyagent\", config)\n\n# 2. Run with a complex prompt\nprint(\"--- Starting Stress Test ---\")\ntry:\n    agent.run(\"Find the current weather in Tokyo, New York, and London.\")\nexcept RuntimeError as e:\n    print(f\"\\n\u2705 Success! The agent was stopped: {e}\")\n</pre> from any_agent.callbacks import get_default_callbacks  # 1. Configure Agent with our custom stack config = AgentConfig(     model_id=\"mistral:mistral-small-latest\",     tools=[search_web],     callbacks=[         ToolUsageCounter(),  # Runs first: Counts the step         BudgetLimit(             max_tools=2         ),  # Runs second: Checks the limit (set low to force a crash)         *get_default_callbacks(),  # Runs last: Logs to console     ], )  agent = AnyAgent.create(\"tinyagent\", config)  # 2. Run with a complex prompt print(\"--- Starting Stress Test ---\") try:     agent.run(\"Find the current weather in Tokyo, New York, and London.\") except RuntimeError as e:     print(f\"\\n\u2705 Success! The agent was stopped: {e}\") In\u00a0[\u00a0]: Copied! <pre>import json\nfrom pathlib import Path\n\nfrom any_agent.callbacks.base import Callback\nfrom any_agent.callbacks.context import Context\n\n\nclass SensitiveDataOffloader(Callback):\n    def __init__(self, output_dir: str) -&gt; None:\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(exist_ok=True, parents=True)\n\n    def before_llm_call(self, context: Context, *args, **kwargs) -&gt; Context:\n        span = context.current_span\n\n        # 1. Check if we have input messages to scrub\n        if input_messages := span.attributes.get(GenAI.INPUT_MESSAGES):\n            # 2. Generate a secure filename based on the trace id\n            output_file = self.output_dir / f\"{span.get_span_context().trace_id}.txt\"\n\n            # 3. \"Offload\" the data to the secure location\n            output_file.write_text(str(input_messages))\n\n            # 4. Replace the span attribute with a reference\n            span.set_attribute(\n                GenAI.INPUT_MESSAGES, json.dumps({\"ref\": str(output_file)})\n            )\n\n        return context\n</pre> import json from pathlib import Path  from any_agent.callbacks.base import Callback from any_agent.callbacks.context import Context   class SensitiveDataOffloader(Callback):     def __init__(self, output_dir: str) -&gt; None:         self.output_dir = Path(output_dir)         self.output_dir.mkdir(exist_ok=True, parents=True)      def before_llm_call(self, context: Context, *args, **kwargs) -&gt; Context:         span = context.current_span          # 1. Check if we have input messages to scrub         if input_messages := span.attributes.get(GenAI.INPUT_MESSAGES):             # 2. Generate a secure filename based on the trace id             output_file = self.output_dir / f\"{span.get_span_context().trace_id}.txt\"              # 3. \"Offload\" the data to the secure location             output_file.write_text(str(input_messages))              # 4. Replace the span attribute with a reference             span.set_attribute(                 GenAI.INPUT_MESSAGES, json.dumps({\"ref\": str(output_file)})             )          return context <p>We can now provide our callback to the agent.</p> <p>You can find more information in our docs.</p> In\u00a0[\u00a0]: Copied! <pre>from any_agent.callbacks import get_default_callbacks\n\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\",\n        tools=[search_web],\n        callbacks=[SensitiveDataOffloader(\"sensitive-info\"), *get_default_callbacks()],\n    ),\n)\nagent_trace = agent.run(\"What are 5 LLM agent frameworks that are trending in 2025?\")\n</pre> from any_agent.callbacks import get_default_callbacks  agent = AnyAgent.create(     \"tinyagent\",     AgentConfig(         model_id=\"mistral:mistral-small-latest\",         tools=[search_web],         callbacks=[SensitiveDataOffloader(\"sensitive-info\"), *get_default_callbacks()],     ), ) agent_trace = agent.run(\"What are 5 LLM agent frameworks that are trending in 2025?\") In\u00a0[\u00a0]: Copied! <pre># Show that sensitive data was offloaded\nimport os\n\nfiles = os.listdir(\"sensitive-info\")\nprint(f\"Created {len(files)} secure file(s)\")\n\n# Peek at what was saved (first 200 chars)\nwith open(f\"sensitive-info/{files[0]}\") as f:\n    print(f\"Offloaded data preview: {f.read()[:200]}...\")\n</pre> # Show that sensitive data was offloaded import os  files = os.listdir(\"sensitive-info\") print(f\"Created {len(files)} secure file(s)\")  # Peek at what was saved (first 200 chars) with open(f\"sensitive-info/{files[0]}\") as f:     print(f\"Offloaded data preview: {f.read()[:200]}...\") <p>As you can see in the console output, the input messages in the trace have been now replaced by a reference to the external destination.</p>"},{"location":"cookbook/callbacks/#using-callbacks","title":"Using Callbacks\u00b6","text":"<p>This cookbook shows you how to monitor, control, and secure your agents using callbacks. We'll build three callbacks of increasing complexity: counting tool usage, enforcing rate limits, and protecting sensitive data.</p> <p>You can find more information about callbacks in the docs</p>"},{"location":"cookbook/callbacks/#configure-llm-keys","title":"Configure LLM Keys\u00b6","text":"<p>For this tutorial, we'll use Mistral's mistral-small-latest (fast and affordable). You could also use:</p> <ul> <li><code>gpt-4o-mini</code></li> <li><code>claude-3-5-sonnet-latest</code></li> <li>Any other model supported by any-agent</li> </ul>"},{"location":"cookbook/callbacks/#running-an-agent-with-default-callbacks","title":"Running an agent (with default callbacks)\u00b6","text":""},{"location":"cookbook/callbacks/#count-tool-usage-contextshared","title":"Count Tool Usage (<code>Context.shared</code>)\u00b6","text":"<p>To control our agent, we first need to measure it. We will create a <code>StepCounter</code> callback.</p> <p>The Callback Contract:</p> <ol> <li>Callbacks receive a <code>context</code> object.</li> <li>They can store data in <code>context.shared</code>.</li> <li>They must return the <code>context</code> object.</li> </ol>"},{"location":"cookbook/callbacks/#enforce-rate-limits","title":"Enforce Rate Limits\u00b6","text":"<p>Now that we are counting steps, we can act on that data.</p> <p>We will create a <code>BudgetLimit</code> callback. If the tool usage exceeds our limit, we will raise an exception to immediately halt the agent. This prevents run-away costs.</p>"},{"location":"cookbook/callbacks/#bonus-protect-sensitive-data","title":"Bonus : Protect Sensitive Data\u00b6","text":"<p>Beyond stopping the agent, callbacks can also modify data before it gets logged to your traces. This is critical for preventing Sensitive Information (PII) from leaking into your logs.</p> <p>In the example below, we are going to implement a callback that:</p> <ol> <li>Detects <code>INPUT_MESSAGES</code> in <code>Context.current_span</code>.</li> <li>Writes this text to a secure local file.</li> <li>Replaces the content in the current span with a reference link, so the trace remains clean.</li> </ol>"},{"location":"cookbook/mcp_agent/","title":"Creating an agent with MCP","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install 'any-agent' 'mcp-server-time' --quiet\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> %pip install 'any-agent' 'mcp-server-time' --quiet  import nest_asyncio  nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre>import os\nimport shutil\nfrom getpass import getpass\n\nif \"MISTRAL_API_KEY\" not in os.environ:\n    print(\"MISTRAL_API_KEY not found in environment!\")\n    api_key = getpass(\"Please enter your MISTRAL_API_KEY: \")\n    os.environ[\"MISTRAL_API_KEY\"] = api_key\n    print(\"MISTRAL_API_KEY set for this session!\")\nelse:\n    print(\"MISTRAL_API_KEY found in environment.\")\n\n# Quick Environment Check (Airbnb tool requires npx/Node.js)\\n\",\nif not shutil.which(\"npx\"):\n    print(\n        \"\u26a0\ufe0f Warning: 'npx' was not found in your path. The Airbnb tool requires Node.js/npm to run.\"\n    )\n</pre> import os import shutil from getpass import getpass  if \"MISTRAL_API_KEY\" not in os.environ:     print(\"MISTRAL_API_KEY not found in environment!\")     api_key = getpass(\"Please enter your MISTRAL_API_KEY: \")     os.environ[\"MISTRAL_API_KEY\"] = api_key     print(\"MISTRAL_API_KEY set for this session!\") else:     print(\"MISTRAL_API_KEY found in environment.\")  # Quick Environment Check (Airbnb tool requires npx/Node.js)\\n\", if not shutil.which(\"npx\"):     print(         \"\u26a0\ufe0f Warning: 'npx' was not found in your path. The Airbnb tool requires Node.js/npm to run.\"     ) In\u00a0[\u00a0]: Copied! <pre>import sys\n\nfrom any_agent import AgentConfig, AnyAgent\nfrom any_agent.config import MCPStdio\n\ntime_tool = MCPStdio(\n    command=sys.executable,\n    args=[\"-u\", \"-m\", \"mcp_server_time\", \"--local-timezone\", \"America/New_York\"],\n    tools=[\n        \"get_current_time\",\n    ],\n    client_session_timeout_seconds=30,\n)\n\nprint(\"Done init time_tool\")\n# This MCP tool relies upon npx https://docs.npmjs.com/cli/v8/commands/npx which comes standard with npm\nairbnb_tool = MCPStdio(\n    command=\"npx\",\n    args=[\"-y\", \"@openbnb/mcp-server-airbnb\", \"--ignore-robots-txt\"],\n    client_session_timeout_seconds=30,\n)\nprint(\"Done init airbnb_tool\")\n</pre> import sys  from any_agent import AgentConfig, AnyAgent from any_agent.config import MCPStdio  time_tool = MCPStdio(     command=sys.executable,     args=[\"-u\", \"-m\", \"mcp_server_time\", \"--local-timezone\", \"America/New_York\"],     tools=[         \"get_current_time\",     ],     client_session_timeout_seconds=30, )  print(\"Done init time_tool\") # This MCP tool relies upon npx https://docs.npmjs.com/cli/v8/commands/npx which comes standard with npm airbnb_tool = MCPStdio(     command=\"npx\",     args=[\"-y\", \"@openbnb/mcp-server-airbnb\", \"--ignore-robots-txt\"],     client_session_timeout_seconds=30, ) print(\"Done init airbnb_tool\") In\u00a0[\u00a0]: Copied! <pre># This is a custom tool that we will provide to the agent. For the agent to use the tool, we must provide a docstring\n# and also have proper python typing for input and output parameters\ndef send_message(message: str) -&gt; str:\n    \"\"\"Display a message to the user and wait for their response.\n\n    Args:\n        message: str\n            The message to be displayed to the user.\n\n    Returns:\n        str: The response from the user.\n\n    \"\"\"\n    if os.environ.get(\"IN_PYTEST\") == \"1\":\n        return \"2 people, next weekend, low budget. Do not ask for any more information or confirmation.\"\n    return input(message + \" \")\n</pre> # This is a custom tool that we will provide to the agent. For the agent to use the tool, we must provide a docstring # and also have proper python typing for input and output parameters def send_message(message: str) -&gt; str:     \"\"\"Display a message to the user and wait for their response.      Args:         message: str             The message to be displayed to the user.      Returns:         str: The response from the user.      \"\"\"     if os.environ.get(\"IN_PYTEST\") == \"1\":         return \"2 people, next weekend, low budget. Do not ask for any more information or confirmation.\"     return input(message + \" \") In\u00a0[\u00a0]: Copied! <pre>print(\"Start creating agent\")\ntry:\n    agent = await AnyAgent.create_async(\n        \"tinyagent\",  # See all options in https://mozilla-ai.github.io/any-agent/\n        AgentConfig(\n            model_id=\"mistral:mistral-large-latest\",\n            tools=[time_tool, airbnb_tool, send_message],\n        ),\n    )\nexcept Exception as e:\n    print(f\"\u274c Failed to create agent: {e}\")\nprint(\"Done creating agent\")\n</pre> print(\"Start creating agent\") try:     agent = await AnyAgent.create_async(         \"tinyagent\",  # See all options in https://mozilla-ai.github.io/any-agent/         AgentConfig(             model_id=\"mistral:mistral-large-latest\",             tools=[time_tool, airbnb_tool, send_message],         ),     ) except Exception as e:     print(f\"\u274c Failed to create agent: {e}\") print(\"Done creating agent\") In\u00a0[\u00a0]: Copied! <pre>prompt = \"\"\"\nI am planning a trip to New York, NY for next weekend. Please act as my travel planner.\n\nFollow these steps in strict order:\n1. **Time Check:** Use the time tool to identify the specific dates for \"next weekend.\"\n2. **Information Gathering:** Ask me about my budget and number of guests. Do NOT search yet.\n3. **Wait:** Wait for my response.\n4. **Search:** ONLY after I reply, search for listings.\n   **CRITICAL:** You MUST set the `location` parameter explicitly to \"New York, NY\" in the tool call.\n\"\"\"\n\nagent_trace = await agent.run_async(prompt)\n</pre> prompt = \"\"\" I am planning a trip to New York, NY for next weekend. Please act as my travel planner.  Follow these steps in strict order: 1. **Time Check:** Use the time tool to identify the specific dates for \"next weekend.\" 2. **Information Gathering:** Ask me about my budget and number of guests. Do NOT search yet. 3. **Wait:** Wait for my response. 4. **Search:** ONLY after I reply, search for listings.    **CRITICAL:** You MUST set the `location` parameter explicitly to \"New York, NY\" in the tool call. \"\"\"  agent_trace = await agent.run_async(prompt) <p>The <code>agent.run</code> method returns an AgentTrace object, which has a few convenient attributes for displaying some interesting information about the run.</p> In\u00a0[\u00a0]: Copied! <pre>print(agent_trace.final_output)  # Final answer\nprint(f\"Duration: {agent_trace.duration.total_seconds():.2f} seconds\")\nprint(f\"Total Tokens: {agent_trace.tokens.total_tokens:,}\")\nprint(f\"Total Cost (USD): {agent_trace.cost.total_cost:.6f}\")\n</pre> print(agent_trace.final_output)  # Final answer print(f\"Duration: {agent_trace.duration.total_seconds():.2f} seconds\") print(f\"Total Tokens: {agent_trace.tokens.total_tokens:,}\") print(f\"Total Cost (USD): {agent_trace.cost.total_cost:.6f}\")"},{"location":"cookbook/mcp_agent/#creating-an-agent-with-mcp","title":"Creating an agent with MCP\u00b6","text":"<p>The Model Context Protocol (MCP) introduced by Anthropic has proven to be a popular method for providing an AI agent with access to a variety of tools. This Huggingface blog post  has a nice explanation of MCP.  In this tutorial, we'll build an agent that is able to leverage MCP server provided tools.</p> <p>Note: because this tutorial relies upon advanced stdio/stderr communication using the MCP Server, it cannot be run on Google Colab.</p>"},{"location":"cookbook/mcp_agent/#install-dependencies","title":"Install Dependencies\u00b6","text":"<p>any-agent uses the python asyncio module to support async functionality. When running in Jupyter notebooks, this means we need to enable the use of nested event loops. We'll install any-agent and enable this below using nest_asyncio.</p>"},{"location":"cookbook/mcp_agent/#configure-the-agent","title":"Configure the Agent\u00b6","text":"<p>Now it's time to configure the agent! At this stage you have a few choices:</p>"},{"location":"cookbook/mcp_agent/#pick-the-framework","title":"Pick the framework\u00b6","text":"<p>We support a variety of underlying agent frameworks (OpenAI, Smolagents, Langchain, TinyAgent, etc), which all have their own particular agentic AI implementations. For this tutorial's simple use case, any of the frameworks should work just fine, but any-agent makes it easy to try out a different framework later, if we so choose. For this example, we will use the TinyAgent framework.</p>"},{"location":"cookbook/mcp_agent/#pick-an-llm","title":"Pick an LLM\u00b6","text":"<p>Regardless of which agent framework you choose, each framework supports any-llm, which is a proxy that allows us to use whichever LLM inside the framework, hosted by any provider. For example, we could use a local model via llama.cpp or llamafile, a Google-hosted gGemini model, or a AWS bedrock hosted Llama model. For this example, let's use Mistral AI's mistral:mistral-small-latest.</p>"},{"location":"cookbook/mcp_agent/#pick-which-tools-to-use","title":"Pick which tools to use\u00b6","text":"<p>In this example, we'll add a few MCP servers that we host locally, which means we'll use a Stdio MCP server. If an MCP Server is already running and hosted elsewhere, you can use an SSE connection to access it. You can browse some of the officially supported MCP servers here.</p> <p>Let's use two MCP servers:</p> <ul> <li>Time: so the agent can know what time/day it is.</li> <li>Airbnb: so the agent can browse airbnb listings (Needs npx)</li> </ul> <p>We will also add a custom send_message tool, that way it can ask us additional questions before getting its final answer!</p>"},{"location":"cookbook/mcp_agent/#run-the-agent","title":"Run the Agent\u00b6","text":"<p>Now we've configured our agent, so it's time to run it! Since it has access to airbnb listings as well as the current time, it's a perfect fit for helping me find a nice airbnb for the weekend.</p>"},{"location":"cookbook/mcp_agent/#view-the-results","title":"View the results\u00b6","text":""},{"location":"cookbook/serve_a2a/","title":"Serve an Agent with A2A","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install 'any-agent[a2a]' 'mcp-server-time' --quiet\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> %pip install 'any-agent[a2a]' 'mcp-server-time' --quiet  import nest_asyncio  nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre>import os\nfrom getpass import getpass\n\n# This notebook communicates with Mistral models using the Mistral API.\nif \"MISTRAL_API_KEY\" not in os.environ:\n    print(\"MISTRAL_API_KEY not found in environment!\")\n    api_key = getpass(\"Please enter your MISTRAL_API_KEY: \")\n    os.environ[\"MISTRAL_API_KEY\"] = api_key\n    print(\"MISTRAL_API_KEY set for this session!\")\nelse:\n    print(\"MISTRAL_API_KEY found in environment.\")\n</pre> import os from getpass import getpass  # This notebook communicates with Mistral models using the Mistral API. if \"MISTRAL_API_KEY\" not in os.environ:     print(\"MISTRAL_API_KEY not found in environment!\")     api_key = getpass(\"Please enter your MISTRAL_API_KEY: \")     os.environ[\"MISTRAL_API_KEY\"] = api_key     print(\"MISTRAL_API_KEY set for this session!\") else:     print(\"MISTRAL_API_KEY found in environment.\") In\u00a0[\u00a0]: Copied! <pre>import asyncio\nimport sys\n\nimport httpx\n\nfrom any_agent import AgentConfig, AnyAgent\nfrom any_agent.config import MCPStdio\nfrom any_agent.serving import A2AServingConfig\n\ntime_tool = MCPStdio(\n    command=sys.executable,\n    args=[\"-u\", \"-m\", \"mcp_server_time\", \"--local-timezone\", \"America/New_York\"],\n    tools=[\n        \"get_current_time\",\n    ],\n    client_session_timeout_seconds=30,\n)\n\ntime = await AnyAgent.create_async(\n    \"tinyagent\",  # See all options in https://mozilla-ai.github.io/any-agent/\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\",\n        description=\"I'm an agent to help with getting the time\",\n        tools=[time_tool],\n    ),\n)\n\ntime_handle = await time.serve_async(A2AServingConfig(port=0))\n\nserver_port = time_handle.port\n\nmax_attempts = 20\npoll_interval = 0.5\nattempts = 0\nserver_url = f\"http://localhost:{server_port}\"\nasync with httpx.AsyncClient() as client:\n    while True:\n        try:\n            # Try to make a basic GET request to check if server is responding\n            await client.get(server_url, timeout=1.0)\n            print(f\"Server is ready at {server_url}\")\n            break\n        except (httpx.RequestError, httpx.TimeoutException):\n            # Server not ready yet, continue polling\n            pass\n\n        await asyncio.sleep(poll_interval)\n        attempts += 1\n        if attempts &gt;= max_attempts:\n            msg = f\"Could not connect to {server_url}. Tried {max_attempts} times with {poll_interval} second interval.\"\n            raise ConnectionError(msg)\n</pre> import asyncio import sys  import httpx  from any_agent import AgentConfig, AnyAgent from any_agent.config import MCPStdio from any_agent.serving import A2AServingConfig  time_tool = MCPStdio(     command=sys.executable,     args=[\"-u\", \"-m\", \"mcp_server_time\", \"--local-timezone\", \"America/New_York\"],     tools=[         \"get_current_time\",     ],     client_session_timeout_seconds=30, )  time = await AnyAgent.create_async(     \"tinyagent\",  # See all options in https://mozilla-ai.github.io/any-agent/     AgentConfig(         model_id=\"mistral:mistral-small-latest\",         description=\"I'm an agent to help with getting the time\",         tools=[time_tool],     ), )  time_handle = await time.serve_async(A2AServingConfig(port=0))  server_port = time_handle.port  max_attempts = 20 poll_interval = 0.5 attempts = 0 server_url = f\"http://localhost:{server_port}\" async with httpx.AsyncClient() as client:     while True:         try:             # Try to make a basic GET request to check if server is responding             await client.get(server_url, timeout=1.0)             print(f\"Server is ready at {server_url}\")             break         except (httpx.RequestError, httpx.TimeoutException):             # Server not ready yet, continue polling             pass          await asyncio.sleep(poll_interval)         attempts += 1         if attempts &gt;= max_attempts:             msg = f\"Could not connect to {server_url}. Tried {max_attempts} times with {poll_interval} second interval.\"             raise ConnectionError(msg) In\u00a0[\u00a0]: Copied! <pre>import httpx\nfrom a2a.client import A2ACardResolver, A2AClient\nfrom a2a.types import AgentCard\n\n# Create the httpx client\nhttpx_client = httpx.AsyncClient()\n\nagent_card: AgentCard = await A2ACardResolver(\n    httpx_client,\n    base_url=f\"http://localhost:{server_port}\",\n).get_agent_card(http_kwargs=None)\nprint(agent_card.model_dump_json(indent=2))\n\nclient = A2AClient(httpx_client=httpx_client, agent_card=agent_card)\n</pre> import httpx from a2a.client import A2ACardResolver, A2AClient from a2a.types import AgentCard  # Create the httpx client httpx_client = httpx.AsyncClient()  agent_card: AgentCard = await A2ACardResolver(     httpx_client,     base_url=f\"http://localhost:{server_port}\", ).get_agent_card(http_kwargs=None) print(agent_card.model_dump_json(indent=2))  client = A2AClient(httpx_client=httpx_client, agent_card=agent_card) In\u00a0[\u00a0]: Copied! <pre>from uuid import uuid4\n\nfrom a2a.types import MessageSendParams, SendMessageRequest\n\nsend_message_payload = {\n    \"message\": {\n        \"role\": \"user\",\n        \"parts\": [{\"kind\": \"text\", \"text\": \"What time is it?\"}],\n        \"messageId\": uuid4().hex,\n    },\n}\nrequest = SendMessageRequest(\n    id=str(uuid4()), params=MessageSendParams(**send_message_payload)\n)\nresponse = await client.send_message(request, http_kwargs={\"timeout\": 30.0})\n# Close the httpx client when done\nawait httpx_client.aclose()\n</pre> from uuid import uuid4  from a2a.types import MessageSendParams, SendMessageRequest  send_message_payload = {     \"message\": {         \"role\": \"user\",         \"parts\": [{\"kind\": \"text\", \"text\": \"What time is it?\"}],         \"messageId\": uuid4().hex,     }, } request = SendMessageRequest(     id=str(uuid4()), params=MessageSendParams(**send_message_payload) ) response = await client.send_message(request, http_kwargs={\"timeout\": 30.0}) # Close the httpx client when done await httpx_client.aclose() In\u00a0[\u00a0]: Copied! <pre>print(response.model_dump_json(indent=2))\n</pre> print(response.model_dump_json(indent=2)) In\u00a0[\u00a0]: Copied! <pre>await time_handle.shutdown()\nprint(\"Agent server has been shut down!\")\n</pre> await time_handle.shutdown() print(\"Agent server has been shut down!\")"},{"location":"cookbook/serve_a2a/#serve-an-agent-with-a2a","title":"Serve an Agent with A2A\u00b6","text":"<p>Once you've built an agent, a common question might be: \"how do I let other developers/applications access it?\". Enter the A2A protocol by Google! A2A is \"An open protocol enabling communication and interoperability between opaque agentic applications\". Any-Agent provides support for serving an agent over A2A, as simple as calling <code>await agent.serve_async()</code>. In this tutorial, we'll build and serve an agent using any-agent, and show how you can serve and interact with it via the A2A protocol.</p> <p>This tutorial assumes basic familiarity with any-agent: if you haven't used any-agent before you may also find the Creating your first agent cookbook to be useful.</p> <p>Note: because this tutorial relies upon advanced stdio/stderr communication using the MCP Server, it cannot be run on Google Colab.</p>"},{"location":"cookbook/serve_a2a/#install-dependencies","title":"Install Dependencies\u00b6","text":"<p>any-agent uses the python asyncio module to support async functionality. When running in Jupyter notebooks, this means we need to enable the use of nested event loops. We'll install any-agent and enable this below using nest_asyncio.</p>"},{"location":"cookbook/serve_a2a/#configure-and-run-the-server","title":"Configure and run the server\u00b6","text":"<p>Let's give our agent the very simple capability to access the current time through a Model Context Protocol (MCP) server. For this demo, we'll use the async method <code>agent.serve_async</code> so that we can easily run both the server and client from inside the notebook.</p>"},{"location":"cookbook/serve_a2a/#call-the-agent-using-a2aclient","title":"Call the agent using A2AClient\u00b6","text":"<p>Now that the agent is listening on localhost, we can communicate with it from any other application that supports A2A. For this tutorial we'll use the a2a python SDK, but any client that implements the A2A Protocol would also work.</p>"},{"location":"cookbook/serve_a2a/#a2a-agent-card","title":"A2A Agent Card\u00b6","text":"<p>Before giving the agent a task, we first retrieve the agent's model card, which is a description of the agents capabilities. This helps the client understand what the agent can do, and what A2A features are available.</p>"},{"location":"cookbook/serve_a2a/#make-a-request-of-the-agent","title":"Make a request of the agent\u00b6","text":"<p>Now that we've connected to the agent with the A2AClient, we're ready to make a request of the agent!</p>"},{"location":"cookbook/serve_a2a/#cleanup","title":"Cleanup\u00b6","text":"<p>In order to shut down the A2A server, we can set the <code>should_exit</code> property of the server, which will cause the server to shutdown and the asyncio task to complete.</p>"},{"location":"cookbook/your_first_agent/","title":"Creating your first agent","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install 'any-agent' --quiet\n%pip install ddgs --quiet\n\nimport warnings\n\nimport nest_asyncio\n\n# Suppress technical warnings to reduce noise for the user\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nnest_asyncio.apply()\n</pre> %pip install 'any-agent' --quiet %pip install ddgs --quiet  import warnings  import nest_asyncio  # Suppress technical warnings to reduce noise for the user warnings.filterwarnings(\"ignore\", category=DeprecationWarning) warnings.filterwarnings(\"ignore\", category=RuntimeWarning)  nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre>import os\nfrom getpass import getpass\n\nif \"MISTRAL_API_KEY\" not in os.environ:\n    os.environ[\"MISTRAL_API_KEY\"] = getpass(\"Enter your Mistral API Key: \")\n</pre> import os from getpass import getpass  if \"MISTRAL_API_KEY\" not in os.environ:     os.environ[\"MISTRAL_API_KEY\"] = getpass(\"Enter your Mistral API Key: \") In\u00a0[\u00a0]: Copied! <pre>from any_agent import AgentConfig, AnyAgent\nfrom any_agent.tools import search_web\n\n# We use the 'mistral-small-latest' model we promised in the text\nagent = AnyAgent.create(\n    \"tinyagent\",\n    AgentConfig(model_id=\"mistral:mistral-small-latest\", tools=[search_web]),\n)\n</pre> from any_agent import AgentConfig, AnyAgent from any_agent.tools import search_web  # We use the 'mistral-small-latest' model we promised in the text agent = AnyAgent.create(     \"tinyagent\",     AgentConfig(model_id=\"mistral:mistral-small-latest\", tools=[search_web]), ) In\u00a0[\u00a0]: Copied! <pre>agent_trace = agent.run(\n    \"What are 5 tv shows that are trending in 2025? Check a few sites, and provide the name of the show, the exact release date, the genre, and a brief description of the show.\"\n)\n</pre> agent_trace = agent.run(     \"What are 5 tv shows that are trending in 2025? Check a few sites, and provide the name of the show, the exact release date, the genre, and a brief description of the show.\" ) <p>The <code>agent.run</code> method returns an AgentTrace object, which has a few convenient attributes for displaying some interesting information about the run.</p> In\u00a0[\u00a0]: Copied! <pre>print(agent_trace.final_output)  # Final answer\nprint(f\"Duration: {agent_trace.duration.total_seconds():.2f} seconds\")\nprint(f\"Usage: {agent_trace.tokens.total_tokens:,}\")\nprint(f\"Cost (USD): {agent_trace.cost.total_cost:.6f}\")\n</pre> print(agent_trace.final_output)  # Final answer print(f\"Duration: {agent_trace.duration.total_seconds():.2f} seconds\") print(f\"Usage: {agent_trace.tokens.total_tokens:,}\") print(f\"Cost (USD): {agent_trace.cost.total_cost:.6f}\")"},{"location":"cookbook/your_first_agent/#creating-your-first-agent","title":"Creating your first agent\u00b6","text":"<p>If you're looking to build your first agent using a few simple tools, this is a great place to start. In this cookbook example, we will create and run a simple agent that has access to a few web tools. This can be easily expanded to add more advanced tools and features. \ud83d\ude80</p>"},{"location":"cookbook/your_first_agent/#install-dependencies","title":"Install Dependencies\u00b6","text":"<p>any-agent uses the python asyncio module to support async functionality. When running in Jupyter notebooks, this means we need to enable the use of nested event loops. We'll install any-agent and enable this below using nest_asyncio.</p>"},{"location":"cookbook/your_first_agent/#configure-the-agent","title":"Configure the Agent\u00b6","text":""},{"location":"cookbook/your_first_agent/#pick-an-llm","title":"Pick an LLM\u00b6","text":"<p>For this tutorial, we'll use Mistral's mistral-small-latest (fast and affordable). You could also use:</p> <ul> <li><code>gpt-4o-mini</code></li> <li><code>claude-3-5-sonnet-latest</code></li> <li>Any other model supported by any-agent</li> </ul>"},{"location":"cookbook/your_first_agent/#pick-tools","title":"Pick Tools\u00b6","text":"<p>We'll use <code>search_web</code>(DuckDuckGo), which provides Duck Duck Go search for free with no API key required.</p>"},{"location":"cookbook/your_first_agent/#run-the-agent","title":"Run the Agent\u00b6","text":"<p>Now we've configured our agent, so it's time to run it! Let's give it a simple task: find 5 trending new TV shows that were released recently.</p>"},{"location":"cookbook/your_first_agent/#view-the-results","title":"View the results\u00b6","text":""},{"location":"cookbook/your_first_agent_evaluation/","title":"Evaluating your first agent","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install 'any-agent' --quiet\n%pip install ddgs --quiet\n\nimport warnings\n\nimport nest_asyncio\n\n# Suppress technical warnings to reduce noise for the user\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=RuntimeWarning)\nwarnings.simplefilter(\"ignore\", category=UserWarning)\n\nnest_asyncio.apply()\n</pre> %pip install 'any-agent' --quiet %pip install ddgs --quiet  import warnings  import nest_asyncio  # Suppress technical warnings to reduce noise for the user warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=RuntimeWarning) warnings.simplefilter(\"ignore\", category=UserWarning)  nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre>import os\nfrom getpass import getpass\n\nif \"MISTRAL_API_KEY\" not in os.environ:\n    print(\"MISTRAL_API_KEY not found in environment!\")\n    api_key = getpass(\"Please enter your MISTRAL_API_KEY: \")\n    os.environ[\"MISTRAL_API_KEY\"] = api_key\n    print(\"MISTRAL_API_KEY set for this session!\")\nelse:\n    print(\"MISTRAL_API_KEY found in environment.\")\n</pre> import os from getpass import getpass  if \"MISTRAL_API_KEY\" not in os.environ:     print(\"MISTRAL_API_KEY not found in environment!\")     api_key = getpass(\"Please enter your MISTRAL_API_KEY: \")     os.environ[\"MISTRAL_API_KEY\"] = api_key     print(\"MISTRAL_API_KEY set for this session!\") else:     print(\"MISTRAL_API_KEY found in environment.\") In\u00a0[\u00a0]: Copied! <pre>from any_agent import AgentConfig, AnyAgent\nfrom any_agent.tools import search_web, visit_webpage\n\nagent = AnyAgent.create(\n    \"tinyagent\",  # See all options in https://mozilla-ai.github.io/any-agent/\n    AgentConfig(\n        model_id=\"mistral:mistral-small-latest\", tools=[search_web, visit_webpage]\n    ),\n)\n</pre> from any_agent import AgentConfig, AnyAgent from any_agent.tools import search_web, visit_webpage  agent = AnyAgent.create(     \"tinyagent\",  # See all options in https://mozilla-ai.github.io/any-agent/     AgentConfig(         model_id=\"mistral:mistral-small-latest\", tools=[search_web, visit_webpage]     ), ) In\u00a0[\u00a0]: Copied! <pre>prompt = \"\"\"What film won a Goya Award for best film in 2024?\nPlease provide the name of the film, the genre, a very brief\ndescription of the film - and rotten tomatoes popcornmeter\nscore.\"\"\"\n\nagent_trace = agent.run(prompt)\n</pre> prompt = \"\"\"What film won a Goya Award for best film in 2024? Please provide the name of the film, the genre, a very brief description of the film - and rotten tomatoes popcornmeter score.\"\"\"  agent_trace = agent.run(prompt) In\u00a0[\u00a0]: Copied! <pre>print(f\"\u23f1\ufe0f Duration: {agent_trace.duration.total_seconds():.2f}s\")\nprint(f\"\ud83d\udcb0 Cost: ${agent_trace.cost.total_cost:.6f}\")\n\nprint(\"\\n--- Final Answer ---\")\nprint(agent_trace.final_output)\n\nprint(\"\\n--- Tool Execution Path ---\")\n# Show the exact steps the agent took so we can verify if it \"Cheated\" or not\nfor span in agent_trace.spans:\n    if span.is_tool_execution():\n        print(f\"\ud83d\udee0\ufe0f Tool Used: {span.attributes.get('gen_ai.tool.name')}\")\n</pre> print(f\"\u23f1\ufe0f Duration: {agent_trace.duration.total_seconds():.2f}s\") print(f\"\ud83d\udcb0 Cost: ${agent_trace.cost.total_cost:.6f}\")  print(\"\\n--- Final Answer ---\") print(agent_trace.final_output)  print(\"\\n--- Tool Execution Path ---\") # Show the exact steps the agent took so we can verify if it \"Cheated\" or not for span in agent_trace.spans:     if span.is_tool_execution():         print(f\"\ud83d\udee0\ufe0f Tool Used: {span.attributes.get('gen_ai.tool.name')}\") In\u00a0[\u00a0]: Copied! <pre>from any_agent.tracing.agent_trace import AgentTrace\nfrom any_agent.tracing.attributes import GenAI\n\n\ndef check_tool_usage(trace: AgentTrace, required_tool: str) -&gt; bool:\n    \"\"\"Check if a specific tool was used in the trace.\"\"\"\n    return any(\n        span.attributes[GenAI.TOOL_NAME] == required_tool\n        for span in trace.spans\n        if span.is_tool_execution()\n    )\n\n\ndef evaluate_web_search_efficiency(trace: AgentTrace) -&gt; dict:\n    \"\"\"Custom evaluation function for web search agent efficiency criteria.\"\"\"\n    # Direct access to trace properties\n    token_count = trace.tokens.total_tokens\n    step_count = len(trace.spans)\n    final_output = trace.final_output\n    duration = trace.duration.total_seconds()\n    # Check if web search tools were used\n    used_search = check_tool_usage(trace, \"search_web\")\n    used_visit = check_tool_usage(trace, \"visit_webpage\")\n    # Apply quantitative criteria\n    results = {\n        \"token_efficient\": token_count\n        &lt; 20000,  # Magic number alert: adjust to what you consider reasonable for your budget\n        \"step_efficient\": step_count\n        &lt;= 10,  # A high number of steps would point at problems, but this is also a debatable limit\n        \"has_output\": final_output is not None and len(str(final_output)) &gt; 50,\n        \"used_web_search\": used_search,\n        \"used_webpage_visit\": used_visit,\n        \"reasonable_duration\": duration &lt; 60,\n    }\n    # Choose the quantitative criteria you care most about\n    results[\"passed\"] = all(\n        [\n            results[\"token_efficient\"],\n            results[\"step_efficient\"],\n            results[\"has_output\"],\n            results[\"used_web_search\"],\n        ]\n    )\n    return results\n</pre> from any_agent.tracing.agent_trace import AgentTrace from any_agent.tracing.attributes import GenAI   def check_tool_usage(trace: AgentTrace, required_tool: str) -&gt; bool:     \"\"\"Check if a specific tool was used in the trace.\"\"\"     return any(         span.attributes[GenAI.TOOL_NAME] == required_tool         for span in trace.spans         if span.is_tool_execution()     )   def evaluate_web_search_efficiency(trace: AgentTrace) -&gt; dict:     \"\"\"Custom evaluation function for web search agent efficiency criteria.\"\"\"     # Direct access to trace properties     token_count = trace.tokens.total_tokens     step_count = len(trace.spans)     final_output = trace.final_output     duration = trace.duration.total_seconds()     # Check if web search tools were used     used_search = check_tool_usage(trace, \"search_web\")     used_visit = check_tool_usage(trace, \"visit_webpage\")     # Apply quantitative criteria     results = {         \"token_efficient\": token_count         &lt; 20000,  # Magic number alert: adjust to what you consider reasonable for your budget         \"step_efficient\": step_count         &lt;= 10,  # A high number of steps would point at problems, but this is also a debatable limit         \"has_output\": final_output is not None and len(str(final_output)) &gt; 50,         \"used_web_search\": used_search,         \"used_webpage_visit\": used_visit,         \"reasonable_duration\": duration &lt; 60,     }     # Choose the quantitative criteria you care most about     results[\"passed\"] = all(         [             results[\"token_efficient\"],             results[\"step_efficient\"],             results[\"has_output\"],             results[\"used_web_search\"],         ]     )     return results In\u00a0[\u00a0]: Copied! <pre>evaluation = evaluate_web_search_efficiency(agent_trace)\nprint(\"Custom Code Evaluation Results:\")\nfor key, value in evaluation.items():\n    print(f\"  {key}: {value}\")\n</pre> evaluation = evaluate_web_search_efficiency(agent_trace) print(\"Custom Code Evaluation Results:\") for key, value in evaluation.items():     print(f\"  {key}: {value}\") In\u00a0[\u00a0]: Copied! <pre>from any_agent.evaluation import LlmJudge\n\n# Create an LLM judge\njudge = LlmJudge(model_id=\"mistral:mistral-large-latest\")\n\n# Define evaluation questions - notice the last one is not like the others\nevaluation_questions = [\n    \"Did the agent provide a clear and concise answer?\",\n    \"Did the agent correctly identify the genre?\",\n    \"Did the agent include a Rotten Tomatoes score in its response?\",\n]\n\n# Run evaluations\nprint(\"LLM Judge Evaluation Results:\")\nprint(\"=\" * 60)\n\nresults = []\nfor i, question in enumerate(evaluation_questions, 1):\n    result = judge.run(context=str(agent_trace.spans_to_messages()), question=question)\n    results.append(result)\n    print(f\"Question {i}: {question}\")\n    print(f\"  Passed: {result.passed}\")\n    print(f\"  Reasoning: {result.reasoning}\")\n    print(\"-\" * 60)\n\n# Summary\npassed_count = sum(1 for r in results if r.passed)\nprint(f\"\\nOverall: {passed_count}/{len(results)} criteria passed\")\n</pre> from any_agent.evaluation import LlmJudge  # Create an LLM judge judge = LlmJudge(model_id=\"mistral:mistral-large-latest\")  # Define evaluation questions - notice the last one is not like the others evaluation_questions = [     \"Did the agent provide a clear and concise answer?\",     \"Did the agent correctly identify the genre?\",     \"Did the agent include a Rotten Tomatoes score in its response?\", ]  # Run evaluations print(\"LLM Judge Evaluation Results:\") print(\"=\" * 60)  results = [] for i, question in enumerate(evaluation_questions, 1):     result = judge.run(context=str(agent_trace.spans_to_messages()), question=question)     results.append(result)     print(f\"Question {i}: {question}\")     print(f\"  Passed: {result.passed}\")     print(f\"  Reasoning: {result.reasoning}\")     print(\"-\" * 60)  # Summary passed_count = sum(1 for r in results if r.passed) print(f\"\\nOverall: {passed_count}/{len(results)} criteria passed\") In\u00a0[\u00a0]: Copied! <pre>from any_agent.evaluation import AgentJudge\nfrom any_agent.tools import search_web\n\n# Create an agent judge\nagent_judge = AgentJudge(model_id=\"mistral:mistral-large-latest\")\n\n# Define a complex evaluation question that requires trace inspection\ncomplex_question = \"\"\"\nEvaluate the agent's performance on this web search task by verifying\nwhether the agent correctly used web search to find relevant information\nfor the winner film of the Goya Award in 2024 and its Rotten Tomatoes rating?\n\nUse the available tools to inspect the trace and, specially, make sure\nthe agent visited Rotten Tomatoes and checked the audience score, not\nthe critics score.\n\"\"\"\n\n# Run the agent judge evaluation\neval_trace = agent_judge.run(\n    trace=agent_trace,\n    question=complex_question,\n    additional_tools=[\n        search_web\n    ],  # Give the judge access to web search for verification\n)\n\n# Get the evaluation result\nresult = eval_trace.final_output\nprint(\"Agent Judge Evaluation Result:\")\nprint(\"=\" * 60)\nprint(f\"Passed: {result.passed}\")\nprint(f\"Reasoning: {result.reasoning}\")\nprint(\"=\" * 60)\n</pre> from any_agent.evaluation import AgentJudge from any_agent.tools import search_web  # Create an agent judge agent_judge = AgentJudge(model_id=\"mistral:mistral-large-latest\")  # Define a complex evaluation question that requires trace inspection complex_question = \"\"\" Evaluate the agent's performance on this web search task by verifying whether the agent correctly used web search to find relevant information for the winner film of the Goya Award in 2024 and its Rotten Tomatoes rating?  Use the available tools to inspect the trace and, specially, make sure the agent visited Rotten Tomatoes and checked the audience score, not the critics score. \"\"\"  # Run the agent judge evaluation eval_trace = agent_judge.run(     trace=agent_trace,     question=complex_question,     additional_tools=[         search_web     ],  # Give the judge access to web search for verification )  # Get the evaluation result result = eval_trace.final_output print(\"Agent Judge Evaluation Result:\") print(\"=\" * 60) print(f\"Passed: {result.passed}\") print(f\"Reasoning: {result.reasoning}\") print(\"=\" * 60) <p>Notice how giving the judge tools enables it to check independently whether the original agent successfully did its job.</p>"},{"location":"cookbook/your_first_agent_evaluation/#evaluating-your-first-agent","title":"Evaluating your first agent\u00b6","text":"<p>In this tutorial, we'll build upon the web search agent from your_first_agent.ipynb and demonstrate how to evaluate its performance using any-agent's evaluation framework. We'll explore different evaluation methods including custom code evaluation, an LLM-based judge, and an agent-based judge.</p> <p>Note: Since we are building on the previous notebook, we encourage you to run that one first to read through details and choices available while building the agent before evaluating it.</p>"},{"location":"cookbook/your_first_agent_evaluation/#install-dependencies","title":"Install Dependencies\u00b6","text":"<p>any-agent uses the python asyncio module to support async functionality. When running in Jupyter notebooks, this means we need to enable the use of nested event loops. We'll install any-agent and enable this below using nest_asyncio.</p>"},{"location":"cookbook/your_first_agent_evaluation/#set-up-the-web-search-agent","title":"Set Up the Web Search Agent\u00b6","text":"<p>First, let's recreate the web search agent from the previous tutorial so we have something to evaluate.</p>"},{"location":"cookbook/your_first_agent_evaluation/#run-the-agent-to-generate-a-trace","title":"Run the Agent to Generate a Trace\u00b6","text":"<p>Now let's run our agent on a test query to generate a trace that we can evaluate.</p>"},{"location":"cookbook/your_first_agent_evaluation/#view-the-agent-results","title":"View the Agent Results\u00b6","text":"<p>Let's first see what our agent produced:</p>"},{"location":"cookbook/your_first_agent_evaluation/#method-1-custom-code-evaluation","title":"Method 1: Custom Code Evaluation\u00b6","text":"<p>Before using LLM-based evaluation, let's start with deterministic custom code evaluation. This is often more efficient, reliable, and cost-effective for specific criteria.</p> <p>Some criteria are clearly quantitative: a result exists or it doesn't, it has a measurable length, the number of steps can be counted and a tool was either called or wasn't.</p>"},{"location":"cookbook/your_first_agent_evaluation/#method-2-llm-judge-evaluation","title":"Method 2: LLM Judge Evaluation\u00b6","text":"<p>The method above is already useful and can assess quantitative results (how long or how costly answers were, whether a specific tool was present). Programmatic evaluations are less costly, more deterministic, but also less flexible. They can see that a tool was used: but was the result well understood? Was the content actually used to extract an answer? This is a qualitative assessment.</p> <p>For such criteria, you can use the <code>LlmJudge</code>. This is great for evaluating response quality, helpfulness, and other subjective criteria.</p>"},{"location":"cookbook/your_first_agent_evaluation/#good-to-know-different-models","title":"\ud83d\udca1 Good to know: different models\u00b6","text":"<p>Notice we use a different LLM as a judge to the one we used for the original agent, as LLM judges are known to have a bias towards their own results.</p>"},{"location":"cookbook/your_first_agent_evaluation/#good-to-know-fuzzy-criteria","title":"\ud83d\udca1 Good to know: fuzzy criteria\u00b6","text":"<p>Notice Question 3: if you run the evaluation multiple times, it won't pass or fail consistently, since the LLM judge may interpret that only the description should be under 10 words, not necessarily the whole Agent's answer. In the programmatic method, there is nothing to interpret: we check that the final output was under 10 words.</p> <p>This showcases the main downside with using an LLM judge: as with humans, criteria can be misunderstood.</p> <p>On the other hand, using a programmatic approach to assess clarity, for example, would have been rather complex without an LLM judge.</p> <p>A take-home message here is to use custom code when criteria can be counted or measured, and think of using an LLMJudge when your criteria are qualitative.</p>"},{"location":"cookbook/your_first_agent_evaluation/#method-3-agent-judge-evaluation","title":"Method 3: Agent Judge Evaluation\u00b6","text":"<p>For more complex evaluations that require inspecting specific aspects of the trace, we can use the <code>AgentJudge</code>. Notice the AgentJudge can:</p> <ul> <li>call built-in tools to get straight to relevant parts of the traces (e.g. final output),</li> <li>call additional tools that the original agent did not have. For example, you will see below how we give it a second search tool so it can do its own research to check if the original agent's answer was correct.</li> </ul> <p>As with the LLMJudge, we choose a different model to the one enabling the original judge.</p>"}]}